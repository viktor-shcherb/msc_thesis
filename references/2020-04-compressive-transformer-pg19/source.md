# Compressive Transformers for Long-Range Sequence Modelling

**Authors:** Jack W. Rae\*, Anna Potapenko\*, Siddhant M. Jayakumar, Chloe Hillier, Timothy P. Lillicrap
**Affiliations:** Jack W. Rae (DeepMind; CoMPLEX, Computer Science, University College London, UK), Anna Potapenko (DeepMind, London, UK), Siddhant M. Jayakumar (DeepMind, London, UK), Chloe Hillier (DeepMind, London, UK), Timothy P. Lillicrap (DeepMind; CoMPLEX, Computer Science, University College London, UK)

\* Equal contribution

## Publication Status

- **arXiv preprint:** November 2019, arXiv:1911.05507
- **Peer-reviewed:** Yes
- **Conference:** International Conference on Learning Representations (ICLR), April 26 - May 1, 2020, Virtual Conference (originally Addis Ababa, Ethiopia)
- **Status:** Published conference paper

## Preferred Citation

Cite the ICLR 2020 version:

> Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., & Lillicrap, T. P. (2020). Compressive Transformers for Long-Range Sequence Modelling. In International Conference on Learning Representations.

## Links

- arXiv: https://arxiv.org/abs/1911.05507
- OpenReview: https://openreview.net/forum?id=SylKikSYDH
- PDF (OpenReview): https://openreview.net/pdf?id=SylKikSYDH
- Dataset (PG-19): https://github.com/google-deepmind/pg19
