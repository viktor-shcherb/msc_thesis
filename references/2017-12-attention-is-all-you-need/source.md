# Attention Is All You Need

**Authors:** Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin
**Affiliations:** Google Brain (Vaswani, Shazeer, Kaiser), Google Research (Parmar, Uszkoreit, Jones), University of Toronto (Gomez; work performed while at Google Brain), Independent (Polosukhin; work performed while at Google Research)

## Publication Status

- **arXiv preprint:** June 2017, arXiv:1706.03762
- **Peer-reviewed:** Yes
- **Conference:** 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA, December 4--9, 2017
- **Status:** Published conference paper

All authors contributed equally; listing order is random. The paper notes individual contributions: Jakob Uszkoreit proposed replacing RNNs with self-attention; Ashish Vaswani and Illia Polosukhin designed and implemented the first Transformer models; Noam Shazeer proposed scaled dot-product attention, multi-head attention, and the parameter-free positional encoding; Niki Parmar implemented and tuned model variants in tensor2tensor; Llion Jones built the initial codebase and efficient inference; Łukasz Kaiser and Aidan Gomez designed and implemented tensor2tensor.

## Preferred Citation

Cite the NIPS 2017 version:

> Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention Is All You Need. In Advances in Neural Information Processing Systems 30 (NIPS 2017), pp. 5998--6008.

## Links

- arXiv: https://arxiv.org/abs/1706.03762
- NeurIPS proceedings: https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html
- Code: https://github.com/tensorflow/tensor2tensor
