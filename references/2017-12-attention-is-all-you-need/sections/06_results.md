# 6 Results [p. 8-10]

## 6.1 Machine Translation [p. 8-9]

On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even the base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. [p. 8]

On the WMT 2014 English-to-French translation task, the big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate P_drop = 0.1, instead of 0.3. [p. 8]

For the base models, a single model obtained by averaging the last 5 checkpoints was used, which were written at 10-minute intervals. For the big models, the last 20 checkpoints were averaged. Beam search was used with a beam size of 4 and length penalty alpha = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. The maximum output length during inference was set to input length + 50, but terminate early when possible [38]. [p. 8]

Table 2 summarizes the results and compares translation quality and training costs to other model architectures from the literature. The number of floating point operations used to train a model is estimated by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU.^5 [p. 8]

^5 Values used: 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively. [p. 8]

### Tables

**Table 2** (p. 8): `"The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost."`

| Model | BLEU EN-DE | BLEU EN-FR | Training Cost (FLOPs) EN-DE | Training Cost (FLOPs) EN-FR |
|---|---|---|---|---|
| ByteNet [18] | 23.75 | | | |
| Deep-Att + PosUnk [39] | | 39.2 | | 1.0 * 10^20 |
| GNMT + RL [38] | 24.6 | 39.92 | 2.3 * 10^19 | 1.4 * 10^20 |
| ConvS2S [9] | 25.16 | 40.46 | 9.6 * 10^18 | 1.5 * 10^20 |
| MoE [32] | 26.03 | 40.56 | 2.0 * 10^19 | 1.2 * 10^20 |
| Deep-Att + PosUnk Ensemble [39] | | 40.4 | | 8.0 * 10^20 |
| GNMT + RL Ensemble [38] | 26.30 | 41.16 | 1.8 * 10^20 | 1.1 * 10^21 |
| ConvS2S Ensemble [9] | 26.36 | **41.29** | 7.7 * 10^19 | 1.2 * 10^21 |
| Transformer (base model) | 27.3 | 38.1 | **3.3 * 10^18** | |
| Transformer (big) | **28.4** | **41.8** | | 2.3 * 10^19 |

## 6.2 Model Variations [p. 9]

To evaluate the importance of different components of the Transformer, the base model was varied in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. Beam search was used as described in the previous section, but no checkpoint averaging. Results are presented in Table 3. [p. 9]

**Table 3** (p. 9): `"Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities."`

| | N | d_model | d_ff | h | d_k | d_v | P_drop | epsilon_ls | train steps | PPL (dev) | BLEU (dev) | params x10^6 |
|---|---|---|---|---|---|---|---|---|---|---|---|---|
| base | 6 | 512 | 2048 | 8 | 64 | 64 | 0.1 | 0.1 | 100K | 4.92 | 25.8 | 65 |
| (A) | | | | 1 | 512 | 512 | | | | 5.29 | 24.9 | |
| | | | | 4 | 128 | 128 | | | | 5.00 | 25.5 | |
| | | | | 16 | 32 | 32 | | | | 4.91 | 25.8 | |
| | | | | 32 | 16 | 16 | | | | 5.01 | 25.4 | |
| (B) | | | | | 16 | | | | | 5.16 | 25.1 | 58 |
| | | | | | 32 | | | | | 5.01 | 25.4 | 60 |
| (C) | 2 | | | | | | | | | 6.11 | 23.7 | 36 |
| | 4 | | | | | | | | | 5.19 | 25.3 | 50 |
| | 8 | | | | | | | | | 4.88 | 25.5 | 80 |
| | | 256 | | | 32 | 32 | | | | 5.75 | 24.5 | 28 |
| | | 1024 | | | 128 | 128 | | | | 4.66 | 26.0 | 168 |
| | | | 1024 | | | | | | | 5.12 | 25.4 | 53 |
| | | | 4096 | | | | | | | 4.75 | 26.2 | 90 |
| (D) | | | | | | | 0.0 | | | 5.77 | 24.6 | |
| | | | | | | | 0.2 | | | 4.95 | 25.5 | |
| | | | | | | | | 0.0 | | 4.67 | 25.3 | |
| | | | | | | | | 0.2 | | 5.47 | 25.7 | |
| (E) | | | positional embedding instead of sinusoids | | | | | | | 4.92 | 25.7 | |
| big | 6 | 1024 | 4096 | 16 | | | 0.3 | | 300K | **4.33** | **26.4** | 213 |

In Table 3 rows (A), the number of attention heads and the attention key and value dimensions are varied, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. [p. 9]

In Table 3 rows (B), reducing the attention key size d_k hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. [p. 9]

In rows (C) and (D), bigger models are better, and dropout is very helpful in avoiding over-fitting. [p. 9]

In row (E), the sinusoidal positional encoding is replaced with learned positional embeddings [9], and nearly identical results to the base model are observed. [p. 9]

## 6.3 English Constituency Parsing [p. 9-10]

To evaluate if the Transformer can generalize to other tasks, experiments were performed on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. [p. 9]

A 4-layer transformer with d_model = 1024 was trained on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. It was also trained in a semi-supervised setting, using the larger high-confidence and BerkeleyParser corpora from approximately 17M sentences [37]. A vocabulary of 16K tokens was used for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. [p. 9]

Only a small number of experiments were performed to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set; all other parameters remained unchanged from the English-to-German base translation model. During inference, the maximum output length was increased to input length + 300. A beam size of 21 and alpha = 0.3 was used for both WSJ only and the semi-supervised setting. [p. 10]

**Table 4** (p. 10): `"The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)"`

| Parser | Training | WSJ 23 F1 |
|---|---|---|
| Vinyals & Kaiser et al. (2014) [37] | WSJ only, discriminative | 88.3 |
| Petrov et al. (2006) [29] | WSJ only, discriminative | 90.4 |
| Zhu et al. (2013) [40] | WSJ only, discriminative | 90.4 |
| Dyer et al. (2016) [8] | WSJ only, discriminative | 91.7 |
| Transformer (4 layers) | WSJ only, discriminative | 91.3 |
| Zhu et al. (2013) [40] | semi-supervised | 91.3 |
| Huang & Harper (2009) [14] | semi-supervised | 91.3 |
| McClosky et al. (2006) [26] | semi-supervised | 92.1 |
| Vinyals & Kaiser et al. (2014) [37] | semi-supervised | 92.1 |
| Transformer (4 layers) | semi-supervised | 92.7 |
| Luong et al. (2015) [23] | multi-task | 93.0 |
| Dyer et al. (2016) [8] | generative | 93.3 |

Results in Table 4 show that despite the lack of task-specific tuning the model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]. [p. 10]

In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences. [p. 10]
