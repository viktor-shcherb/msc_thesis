# Longformer: The Long-Document Transformer

**Authors:** Iz Beltagy, Matthew E. Peters, Arman Cohan
**Affiliation:** Allen Institute for Artificial Intelligence (AI2)

## Publication Status

- **arXiv preprint:** April 2020, arXiv:2004.05150 (v2: December 2020)
- **Peer-reviewed:** No
- **Status:** Preprint

## Notes

Despite remaining unpublished at a peer-reviewed venue, the Longformer paper has accumulated thousands of citations and has been widely adopted in both research and production systems. The pretrained Longformer and LED models are available on HuggingFace and have been integrated into the Transformers library. The paper's contributions -- sliding window attention, global attention, and the pretrain-finetune paradigm for long-document models -- became standard building blocks for subsequent efficient Transformer work (ETC, BigBird, and others).

## Preferred Citation

Cite the arXiv preprint:

> Beltagy, I., Peters, M. E., & Cohan, A. (2020). Longformer: The Long-Document Transformer. arXiv preprint, arXiv:2004.05150.

## Links

- arXiv: https://arxiv.org/abs/2004.05150
- Code: https://github.com/allenai/longformer
- HuggingFace models: https://huggingface.co/allenai/longformer-base-4096
