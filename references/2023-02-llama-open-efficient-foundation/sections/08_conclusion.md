# 8 Conclusion [p. 11]

[p. 11] A series of language models that are released openly, and competitive with state-of-the-art foundation models is presented. Most notably, LLaMA-13B outperforms GPT-3 while being more than 10x smaller, and LLaMA-65B is competitive with Chinchilla-70B and PaLM-540B. Unlike previous studies, it is shown that it is possible to achieve state-of-the-art performance by training exclusively on publicly available data, without resorting to proprietary datasets. The hope is that releasing these models to the research community will accelerate the development of large language models, and help efforts to improve their robustness and mitigate known issues such as toxicity and bias. Additionally, as observed like Chung et al. (2022), finetuning these models on instructions lead to promising results, and further investigation is planned in future work. Finally, there are plans to release larger models trained on larger pretraining corpora in the future, since a constant improvement in performance as scaling was observed.
