# LLaMA: Open and Efficient Foundation Language Models

**Authors:** Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample
**Affiliation:** Meta AI

## Publication Status

- **arXiv preprint:** February 2023, arXiv:2302.13971
- **Peer-reviewed:** No
- **Conference/Journal:** None
- **Status:** Preprint

LLaMA was released as a technical report alongside the model weights. Despite remaining unpublished at a peer-reviewed venue, it is among the most cited papers in the LLM literature and established the architecture used by the majority of open-weight language models.

## Preferred Citation

> Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., & Lample, G. (2023). LLaMA: Open and Efficient Foundation Language Models. arXiv:2302.13971.

## Links

- arXiv: https://arxiv.org/abs/2302.13971
- Code: https://github.com/facebookresearch/llama
