# GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints

**Authors:** Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, Sumit Sanghai
**Affiliation(s):** Google Research; University of Southern California (de Jong)

## Publication Status

- **arXiv preprint:** May 2023, arXiv:2305.13245
- **Peer-reviewed:** Yes
- **Conference/Journal:** Conference on Empirical Methods in Natural Language Processing (EMNLP), December 2023, Singapore
- **Status:** Published conference paper

## Preferred Citation

> Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebrón, F., & Sanghai, S. (2023). GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)*.

## Links

- arXiv: https://arxiv.org/abs/2305.13245
- Code: https://github.com/google/flaxformer
