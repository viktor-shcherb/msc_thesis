# Large Language Models Can Be Easily Distracted by Irrelevant Context

**Authors:** Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael Schärli, Denny Zhou
**Affiliations:** Google Research

## Publication Status

- **arXiv preprint:** February 2023, arXiv:2302.00093
- **Peer-reviewed:** Yes
- **Conference:** 40th International Conference on Machine Learning (ICML 2023), PMLR 202:31210--31227, July 23--29, 2023, Honolulu, Hawaii
- **Status:** Published conference paper

## Preferred Citation

Cite the ICML 2023 version:

> Shi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E. H., Schärli, N., & Zhou, D. (2023). Large Language Models Can Be Easily Distracted by Irrelevant Context. In Proceedings of the 40th International Conference on Machine Learning (ICML), PMLR 202:31210--31227.

## Links

- arXiv: https://arxiv.org/abs/2302.00093
- PMLR: https://proceedings.mlr.press/v202/shi23a.html
- OpenReview: https://openreview.net/forum?id=JSZmoN03Op
- Dataset: https://github.com/google-research-datasets/GSM-IC
