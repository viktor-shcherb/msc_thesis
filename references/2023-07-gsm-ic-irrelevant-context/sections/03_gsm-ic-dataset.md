# 3. The GSM-IC Dataset [p. 3-4]

This section introduces the creation process of the GSM-IC dataset (Section 3.1) and the evaluation metrics (Section 3.2). [p. 3]

## 3.1 Dataset Creation

[p. 3] 1,000 problems are randomly chosen from the GSM8K training set as a development set. From this development set, 100 problems are chosen that can be correctly solved by at least one of the prompting techniques mentioned in this paper; i.e., the base dataset is an "easy" subset of GSM8K (Table 2). Each base problem requires two to seven reasoning steps to solve. Among the 100 base problems, 60 of them can be solved with two reasoning steps. The full dataset statistics can be found in Appendix A. [p. 3]

The number of reasoning steps of a problem is given by the number of sentences in its standard answer (Cobbe et al., 2021). [p. 3]

**Table 2** (p. 3): Accuracy (x100) on the base 100-example dataset using `code-davinci-002`. See Table 3 for results with `text-davinci-003`.

| | CoT | LtM | PROGRAM | 0-CoT |
|---|---|---|---|---|
| (base) | 95.0 | 94.0 | 83.0 | 44.0 |
| + SC | 96.0 | 99.0 | 91.0 | 76.0 |

[p. 3] Examples of the new dataset are generated by adding to each base problem one sentence containing irrelevant information. A template-based method (Figure 1) is used to generate these sentences, which can be characterized by the following three factors:

- **Topic of the inserted sentence.** Templates are written for both in-topic and off-topic sentences. In-topic sentences are closely related to the topic of the original problem, whereas off-topic sentences are about a different topic. [p. 3]

- **Role name overlap.** Most sentence templates contain some role name blanks, which can be filled with names that may or may not overlap with the role names that occur in the problem. For blank fillers that have overlap with original role names: (1) randomly pick a role name A from the original problem description and (2) create the blank fillers with template such as `A's father` and `A's sister`. [p. 3]

- **Range of numbers.** Since the focus is arithmetic reasoning, most sentence templates also contain a number blank. The number blank can be filled with a number of similar or different magnitude to those in the original problem description. Concretely, for a number *a*, if there exists a number *b* in the original problem description or solution such that 1/10 <= a/b <= 10, it is considered an in-range number, and otherwise an out-of-range number. Since the standard answer to GSM8K problems are all positive integers, only positive integers are considered as the number blank fillers. [p. 4]

[p. 4] The authors manually verify that (1) all the generated sentences are acceptable in English and that (2) adding them does not affect the standard solution of the base problem. Because the above factors are orthogonal, for each base example a set of derived examples with different factor combinations is generated. The full GSM-IC benchmark consists of 58,052 examples. More details about the dataset creation process can be found in Appendix A. [p. 4]

**Figure 1** (p. 3): "Illustration of the considered factors when creating the GSM-IC dataset. Best viewed in color."

The figure shows the template-based method for generating irrelevant sentences. It presents an original problem about Jeanne buying tickets, then shows:
- **Options for the Irrelevant Sentence Topic:** In-Topic (e.g., "[ROLE] rides [NUMBER] kilometers to the bus station every day.") vs. Off-Topic (e.g., "The shoe size of [ROLE] is [NUMBER].")
- **Options for [ROLE]: Lexical Overlap with Original Characters?** Yes (e.g., Jeanne's father, Jeanne's sister, Jeanne's neighbor...) vs. No (e.g., Ada, Jack, Mary, Tom...)
- **Options for [NUMBER]:** In-Range (5, 6, 7, 8...) vs. Out-of-Range (100, 1000, 5000...)

## 3.2 Evaluation Metrics

[p. 4] For a problem *p*, its standard solution is denoted by *s(p)*, and the solution of method M by M(p). Two metrics are defined to evaluate the distractibility of M:

- **Micro accuracy** Acc_micro(M; P) is the average accuracy of method M over all the test problems P.

$$Acc_{micro}(\mathcal{M}; \mathcal{P}) = \frac{\sum_{p \in \mathcal{P}} \mathbb{1}[\mathcal{M}(p) = s(p)]}{|\mathcal{P}|}$$

This means that the micro accuracy weighs all the individual test problems equally. [p. 4]

- **Macro accuracy** Acc_macro(M; B) is the average accuracy of method M over classes of test problems, where each class P(b) consists of the set of test examples derived from the base example b in B. M's prediction for a class P(b) is correct if and only if M's prediction for all problems in this class are correct.

$$Acc_{macro}(\mathcal{M}; \mathcal{B}) = \frac{\sum_{b \in \mathcal{B}} \mathbb{1}\left[\bigwedge_{p \in \mathcal{P}(b)} [\mathcal{M}(p) = s(p)]\right]}{|\mathcal{B}|}$$

This means that the macro accuracy is the fraction of base problems that can be consistently solved no matter what irrelevant sentence is being added. [p. 4]

- **Normalized accuracy** measures how a method is affected by the distractors, considering its accuracy on base problems. For a micro or macro accuracy a_M achieved by method M, its corresponding normalized accuracy is:

$$norm(a_{\mathcal{M}}; \mathcal{M}) = \frac{a_{\mathcal{M}}}{n_{\mathcal{M}}},$$

where n_M denotes the base problem accuracy of method M (Table 2). [p. 4]
