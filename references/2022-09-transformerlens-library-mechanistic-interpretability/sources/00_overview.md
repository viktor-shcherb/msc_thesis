# TransformerLens: A Library for Mechanistic Interpretability of Language Models

**Author(s):** Neel Nanda, Joseph Bloom
**Type:** Open-source software library (GitHub + docs + package distribution)
**Date:** September 2022 (release metadata)
**Primary URL:** https://github.com/TransformerLensOrg/TransformerLens

## Summary

TransformerLens is an open-source toolkit aimed at making mechanistic interpretability workflows practical for GPT-style language models. The project's own materials emphasize ease of access to internal activations, intervention tooling, and reusable abstractions for circuit-style analysis. Since this contribution is software-first (not a peer-reviewed paper), authoritative metadata and claims come from repository and documentation sources.

## Source structure

1. **GitHub Repository (README)** (primary): Declared goals and supported interpretability workflows
2. **`citation.cff`** (primary): Canonical citation metadata, release date, and abstract
3. **Docs â€” Citing TransformerLens** (supplementary): User-facing BibTeX citation guidance
4. **PyPI package page** (supplementary): Distribution metadata and package context
