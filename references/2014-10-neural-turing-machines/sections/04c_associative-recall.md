# 4.3 Associative Recall [p. 14–18]

[p. 14] The previous tasks show that the NTM can apply algorithms to relatively simple, linear data structures. The next order of complexity in organising data arises from "indirection" -- that is, when one data item points to another. The authors test the NTM's capability for learning an instance of this more interesting class by constructing a list of items so that querying with one of the items demands that the network return the subsequent item.

More specifically, an item is defined as a sequence of binary vectors that is bounded on the left and right by delimiter symbols. After several items have been propagated to the network, a query is made by showing a random item, and the network is asked to produce the next item. In the experiments, each item consisted of three six-bit binary vectors (giving a total of 18 bits per item). During training, a minimum of 2 items and a maximum of 6 items were used in a single episode.

[p. 16] **Figure 10** (p. 16): "Associative Recall Learning Curves for NTM and LSTM."
The figure plots cost per sequence (bits) on the y-axis (range 0–20) against sequence number (thousands) on the x-axis (range 0–1000). Three curves are shown: LSTM (blue, cross markers), NTM with LSTM Controller (green, square markers), NTM with Feedforward Controller (red, triangle markers). NTM with a feedforward controller drops to near zero cost within approximately 30,000 episodes. NTM with LSTM controller reaches near zero within approximately 100,000 episodes. LSTM does not reach zero cost after a million episodes, remaining around 1–2 bits.

Figure 10 shows that NTM learns this task significantly faster than LSTM, terminating at near zero cost within approximately 30,000 episodes, whereas LSTM does not reach zero cost after a million episodes. Additionally, NTM with a feedforward controller learns faster than NTM with an LSTM controller. These two results suggest that NTM's external memory is a more effective way of maintaining the data structure than LSTM's internal state.

NTM also generalises much better to longer sequences than LSTM, as can be seen in Figure 11. NTM with a feedforward controller is nearly perfect for sequences of up to 12 items (twice the maximum length used in training), and still has an average cost below 1 bit per sequence for sequences of 15 items.

[p. 17] **Figure 11** (p. 17): "Generalisation Performance on Associative Recall for Longer Item Sequences. The NTM with either a feedforward or LSTM controller generalises to much longer sequences of items than the LSTM alone. In particular, the NTM with a feedforward controller is nearly perfect for item sequences of twice the length of sequences in its training set."
The figure plots cost per sequence (bits) on the y-axis (range 0–40) against number of items per sequence on the x-axis (range 6–20). Three curves: LSTM (blue, cross markers) rises sharply from ~1 at 6 items to ~35 at 12 items and ~38 at 16 items. NTM with LSTM Controller (green, square markers) stays low, rising from ~0 at 6 items to ~7 at 18 items and ~8 at 20 items. NTM with Feedforward Controller (red, triangle markers) stays lowest, rising from ~0 at 6 items to ~5 at 18 items and ~7 at 20 items. Trained on sequences of up to 6 items, demonstrating NTM's superior generalisation.

[p. 17–18] **Figure 12** (p. 18): "NTM Memory Use During the Associative Recall Task. In 'Inputs,' a sequence of items, each composed of three consecutive binary random vectors is propagated to the controller. The distinction between items is designated by delimiter symbols (row 7 in 'Inputs'). After several items have been presented, a delimiter that designates a query is presented (row 8 in 'Inputs'). A single query item is presented (green box), and the network target corresponds to the subsequent item in the sequence (red box). In 'Outputs,' we see that the network correctly produces the target item. The red boxes in the read and write weightings highlight the three locations where the target item was written and then read. The solution the network finds is to form a compressed representation (black box in 'Adds') of each item that it can store in a single location."
The figure has the same layout as Figures 6 and 9: two columns (left: Inputs/Adds/Write Weightings, right: Outputs/Reads/Read Weightings). In "Inputs," items are sequences of three binary vectors separated by delimiter symbols in row 7. Row 8 marks the query delimiter. The green box highlights the query item; the red box highlights the target item. In "Outputs," the network correctly produces the target. In "Write Weightings," memory locations are written along a diagonal, with a red box around the target item's write location. In "Read Weightings," the final three time steps read from contiguous locations (red box) corresponding to where the target item was stored. In "Adds," a compressed representation (black box) of each item is visible.

## Memory access analysis [p. 17]

[p. 17] The delimiter in row 8 prepares the network to receive a query item. The query item corresponds to the second item in the sequence (contained in the green box). In "Outputs," the network crisply outputs item 3 in the sequence (from the red box). In "Read Weightings," on the last three time steps, the controller reads from contiguous locations that each store the time slices of item 3. This is curious because it appears that the network has jumped directly to the correct location storing item 3. However, this behaviour can be explained by looking at "Write Weightings." The memory is written to even when the input presents a delimiter symbol between items. One can confirm in "Adds" that data are indeed written to memory when the delimiters are presented (e.g., the data within the black box); furthermore, each time a delimiter is presented, the vector added to memory is different.

Further analysis of the memory reveals that the network accesses the location it reads after the query by using a content-based lookup that produces a weighting that is shifted by one. Additionally, the key used for content-lookup corresponds to the vector that was added in the black box. This implies the following memory-access algorithm: when each item delimiter is presented, the controller writes a compressed representation of the previous three time slices of the item. After the query arrives, the controller recomputes the same compressed representation of the query item, uses a content-based lookup to find the location where it wrote the first representation, and then shifts by one to produce the subsequent item in the sequence (thereby combining content-based lookup with location-based offsetting).
