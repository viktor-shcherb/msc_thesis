# References cited in section notes

- **Von Neumann, 1945** — John von Neumann, "First Draft of a Report on the EDVAC," 1945.
  Cited in 01_introduction.md as foundational reference for the three mechanisms of computer programs.

- **Siegelmann and Sontag, 1995** — Hava T. Siegelmann and Eduardo D. Sontag, "On the Computational Power of Neural Nets," *Journal of Computer and System Sciences*, 1995.
  Cited in 01_introduction.md as proof that RNNs are Turing-Complete.

- **Baddeley et al., 2009** — Alan Baddeley, working memory model reference, 2009.
  Cited in 01_introduction.md and 02_foundational-research.md as the standard model of working memory (central executive + memory buffer).

- **Hadley, 2009** — Robert F. Hadley, "The Problem of Rapid Variable Creation," *Neural Computation*, 2009.
  Cited in 01_introduction.md for the concept of "rapidly-created variables."

- **Minsky, 1967** — Marvin Minsky, *Computation: Finite and Infinite Machines*, 1967.
  Cited in 01_introduction.md for analogy of binding data to memory slots like registers.

- **Miller, 1956** — George A. Miller, "The Magical Number Seven, Plus or Minus Two," *Psychological Review*, 1956.
  Cited in 02_foundational-research.md for capacity limitations of working memory ("chunks").

- **Barrouillet et al., 2004** — Pierre Barrouillet et al., characterising capacity limitations of working memory, 2004.
  Cited in 02_foundational-research.md as ongoing debate on capacity characterisation.

- **Goldman-Rakic, 1995** — Patricia S. Goldman-Rakic, working memory and prefrontal cortex, 1995.
  Cited in 02_foundational-research.md for neuroscience basis of working memory (prefrontal cortex and basal ganglia).

- **Rigotti et al., 2013** — Mattia Rigotti et al., dimensionality of population code in prefrontal cortex, 2013.
  Cited in 02_foundational-research.md for quantifying delay period activity predicting memory performance.

- **Wang, 1999** — Xiao-Jing Wang, biophysical circuits for persistent neuronal firing, 1999.
  Cited in 02_foundational-research.md as a modeling study of working memory.

- **Hazy et al., 2006** — Todd E. Hazy et al., working memory model analogous to LSTM, 2006.
  Cited in 02_foundational-research.md as the most relevant prior model; gates information into memory slots but lacks sophisticated addressing.

- **Dayan, 2008** — Peter Dayan, working memory modeling, 2008.
  Cited in 02_foundational-research.md as a modeling study of working memory.

- **Eliasmith, 2013** — Chris Eliasmith, working memory modeling, 2013.
  Cited in 02_foundational-research.md as a modeling study of working memory.

- **Gallistel and King, 2009** — C. R. Gallistel and Adam Philip King, *Memory and the Computational Brain*, 2009.
  Cited in 02_foundational-research.md for the argument that addressing is implicated in brain operation.

- **Marcus, 2003** — Gary F. Marcus, *The Algebraic Mind*, 2003.
  Cited in 02_foundational-research.md for the argument that addressing is implicated in brain operation.

- **Chomsky, 1956** — Noam Chomsky, foundational work in linguistics, 1956.
  Cited in 02_foundational-research.md for the influence of computers on cognitive science and linguistics.

- **Miller, 2003** — George A. Miller, cognitive science history, 2003.
  Cited in 02_foundational-research.md for the emergence of cognitive science alongside AI.

- **Rumelhart et al., 1986** — David E. Rumelhart, James L. McClelland, and the PDP Research Group, *Parallel Distributed Processing*, 1986.
  Cited in 02_foundational-research.md for the PDP/connectionist revolution.

- **Fodor and Pylyshyn, 1988** — Jerry A. Fodor and Zenon W. Pylyshyn, "Connectionism and Cognitive Architecture: A Critical Analysis," *Cognition*, 1988.
  Cited in 02_foundational-research.md for two critiques of neural networks: inability to do variable-binding and inability to handle variable-length structures.

- **Hinton, 1986** — Geoffrey E. Hinton, distributed representations, 1986.
  Cited in 02_foundational-research.md as response to Fodor and Pylyshyn's critique.

- **Smolensky, 1990** — Paul Smolensky, tensor product representations, 1990.
  Cited in 02_foundational-research.md as response to Fodor and Pylyshyn's critique.

- **Touretzky, 1990** — David S. Touretzky, connectionist variable-binding mechanisms, 1990.
  Cited in 02_foundational-research.md as response to Fodor and Pylyshyn's critique.

- **Pollack, 1990** — Jordan B. Pollack, recursive auto-associative memory, 1990.
  Cited in 02_foundational-research.md as response to Fodor and Pylyshyn's critique; also cited in 02_foundational-research.md (Section 2.3) for building parse trees greedily.

- **Plate, 2003** — Tony A. Plate, *Holographic Reduced Representations*, 2003.
  Cited in 02_foundational-research.md as response to Fodor and Pylyshyn's critique.

- **Kanerva, 2009** — Pentti Kanerva, hyperdimensional computing, 2009.
  Cited in 02_foundational-research.md as response to Fodor and Pylyshyn's critique.

- **Fitch et al., 2005** — W. Tecumseh Fitch, Marc D. Hauser, and Noam Chomsky, recursion as uniquely human, 2005.
  Cited in 02_foundational-research.md for the view that recursive processing is a uniquely human innovation enabling language.

- **Jackendoff and Pinker, 2005** — Ray Jackendoff and Steven Pinker, multiple adaptations for language evolution, 2005.
  Cited in 02_foundational-research.md for the opposing view that recursive processing predates language.

- **Hochreiter and Schmidhuber, 1997** — Sepp Hochreiter and Jurgen Schmidhuber, "Long Short-Term Memory," *Neural Computation*, 1997.
  Cited in 02_foundational-research.md as the original LSTM paper.

- **Hochreiter et al., 2001a** — Sepp Hochreiter et al., vanishing and exploding gradient problem, 2001.
  Cited in 02_foundational-research.md for identifying the vanishing/exploding gradient problem.

- **Seung, 1998** — H. Sebastian Seung, perfect integrators for memory storage, 1998.
  Cited in 02_foundational-research.md for the concept of perfect integrators embedded in LSTM.

- **Graves et al., 2013** — Alex Graves et al., speech recognition with RNNs, 2013.
  Cited in 02_foundational-research.md as an application of RNNs to speech recognition.

- **Graves and Jaitly, 2014** — Alex Graves and Navdeep Jaitly, speech recognition, 2014.
  Cited in 02_foundational-research.md as an application of RNNs to speech recognition.

- **Sutskever et al., 2011** — Ilya Sutskever et al., text generation with RNNs, 2011.
  Cited in 02_foundational-research.md as an application of RNNs to text generation.

- **Graves, 2013** — Alex Graves, handwriting generation / differentiable attention, 2013.
  Cited in 02_foundational-research.md for handwriting generation and as a precursor for differentiable attention. Also cited in 04f_experimental-details.md for the form of RMSProp used in training.

- **Sutskever et al., 2014** — Ilya Sutskever et al., sequence to sequence learning for machine translation, 2014.
  Cited in 02_foundational-research.md as an application of RNNs to machine translation.

- **Socher et al., 2012** — Richard Socher et al., recursive neural networks for parse trees, 2012.
  Cited in 02_foundational-research.md for building explicit parse trees.

- **Frasconi et al., 1998** — Paolo Frasconi et al., recursive neural networks, 1998.
  Cited in 02_foundational-research.md for building explicit parse trees.

- **Bahdanau et al., 2014** — Dzmitry Bahdanau et al., attention mechanism for neural machine translation, 2014.
  Cited in 02_foundational-research.md as a precursor for differentiable attention.

- **Hochreiter et al., 2001b** — Sepp Hochreiter et al., program search with RNNs, 2001.
  Cited in 02_foundational-research.md as a precursor for program search.

- **Das et al., 1992** — Sreerupa Das et al., program search with RNNs, 1992.
  Cited in 02_foundational-research.md as a precursor for program search.

- **Hopfield, 1982** — John J. Hopfield, "Neural Networks and Physical Systems with Emergent Collective Computational Abilities," *Proceedings of the National Academy of Sciences*, 1982.
  Cited in 03_neural-turing-machines.md (Section 3.3) for the content-addressing mechanism of Hopfield networks as a precursor to content-based addressing in NTMs.

- **Murphy, 2012** — Kevin P. Murphy, *Machine Learning: A Probabilistic Perspective*, 2012.
  Cited in 04d_dynamic-n-grams.md for the Bayesian analysis yielding the optimal estimator for the dynamic N-Gram task (Equation 10).
