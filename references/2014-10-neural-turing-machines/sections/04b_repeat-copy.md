# 4.2 Repeat Copy [p. 12-14]

[p. 12] The repeat copy task extends copy by requiring the network to output the copied sequence a specified number of times and then emit an end-of-sequence marker. The main motivation was to see if the NTM could learn a simple nested function. Ideally, the authors would like it to be able to execute a "for loop" containing any subroutine it has already learned.

The network receives random-length sequences of random binary vectors, followed by a scalar value indicating the desired number of copies, which appears on a separate input channel. To emit the end marker at the correct time the network must be both able to interpret the extra input and keep count of the number of copies it has performed so far. As with the copy task, no inputs are provided to the network after the initial sequence and repeat number. The networks were trained to reproduce sequences of size eight random binary vectors, where both the sequence length and the number of repetitions were chosen randomly from one to ten. The input representing the repeat number was normalised to have mean zero and variance one.

**Figure 7** (p. 14): "Repeat Copy Learning Curves."
The figure plots cost per sequence (bits) on the y-axis (range 0-200) against sequence number (thousands) on the x-axis (range 0-500). Three curves are shown: LSTM (blue, cross markers), NTM with LSTM Controller (green, square markers), NTM with Feedforward Controller (red, triangle markers). All three start around 140-180 bits. Both NTM variants drop rapidly to near 0 within ~100k sequences. LSTM drops more slowly, reaching near 0 by ~300k sequences. NTM learns the task much faster than LSTM, but both were able to solve it perfectly (footnote 3: "It surprised us that LSTM performed better here than on the copy problem. The likely reasons are that the sequences were shorter (up to length 10 instead of up to 20), and the LSTM network was larger and therefore had more memory capacity.").

[p. 14] The difference between the two architectures only becomes clear when they are asked to generalise beyond the training data. In this case the authors were interested in generalisation along two dimensions: sequence length and number of repetitions. Figure 8 illustrates the effect of doubling first one, then the other, for both LSTM and NTM. Whereas LSTM fails both tests, NTM succeeds with longer sequences and is able to perform more than ten repetitions; however it is unable to keep count of how many repeats it has completed, and does not predict the end marker correctly. This is probably a consequence of representing the number of repetitions numerically, which does not easily generalise beyond a fixed range.

**Figure 8** (p. 15): "NTM and LSTM Generalisation for the Repeat Copy Task. NTM generalises almost perfectly to longer sequences than seen during training. When the number of repeats is increased it is able to continue duplicating the input sequence fairly accurately; but it is unable to predict when the sequence will end, emitting the end marker after the end of every repetition beyond the eleventh. LSTM struggles with both increased length and number, rapidly diverging from the input sequence in both cases."
The figure shows two blocks: NTM (top) and LSTM (bottom), each with two test conditions. NTM Length 10 Repeat 20: Targets/Outputs shown â€” NTM reproduces the repeated sequence well. NTM Length 20 Repeat 10: similar success. LSTM Length 10 Repeat 20: output rapidly diverges from target. LSTM Length 20 Repeat 10: output also diverges, with incorrect sequence content.

**Figure 9** (p. 16): "NTM Memory Use During the Repeat Copy Task. As with the copy task the network first writes the input vectors to memory using iterative shifts. It then reads through the sequence to replicate the input as many times as necessary (six in this case). The white dot at the bottom of the read weightings seems to correspond to an intermediate location used to redirect the head to the start of the sequence (The NTM equivalent of a goto statement)."
The figure has the same layout as Figure 6 (two columns: Write Weightings and Read Weightings, each with Inputs/Outputs, Adds/Reads, and Location rows). Write weightings show a single diagonal pass storing the input. Read weightings show a repeating sawtooth pattern as the head cycles through the stored sequence multiple times. A white dot at the bottom of the read weightings marks the "goto" location used to jump back to the start of the sequence for each repetition.

Figure 9 suggests that NTM learns a simple extension of the copy algorithm in the previous section, where the sequential read is repeated as many times as necessary.
