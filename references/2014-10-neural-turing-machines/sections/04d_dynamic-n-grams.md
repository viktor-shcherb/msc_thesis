# 4.4 Dynamic N-Grams [p. 17–20]

[p. 17] The goal of the dynamic N-Grams task was to test whether NTM could rapidly adapt to new predictive distributions. In particular the authors were interested to see if it were able to use its memory as a re-writable table that it could use to keep count of transition statistics, thereby emulating a conventional N-Gram model.

## Task setup [p. 18]

[p. 18] The authors considered the set of all possible 6-Gram distributions over binary sequences. Each 6-Gram distribution can be expressed as a table of 2^5 = 32 numbers, specifying the probability that the next bit will be one, given all possible length five binary histories. For each training example, they first generated random 6-Gram probabilities by independently drawing all 32 probabilities from the Beta(1/2, 1/2) distribution.

A particular training sequence was then generated by drawing 200 successive bits using the current lookup table.^4 The network observes the sequence one bit at a time and is then asked to predict the next bit.

> ^4 "The first 5 bits, for which insufficient context exists to sample from the table, are drawn i.i.d. from a Bernoulli distribution with p = 0.5." [p. 18]

## Optimal estimator [p. 19]

[p. 19] The optimal estimator for the problem can be determined by Bayesian analysis (Murphy, 2012):

$$P(B = 1 | N_1, N_0, \mathbf{c}) = \frac{N_1 + \frac{1}{2}}{N_1 + N_0 + 1} \tag{10}$$

where **c** is the five bit previous context, B is the value of the next bit and N_0 and N_1 are respectively the number of zeros and ones observed after **c** so far in the sequence.

## Results [p. 19]

[p. 19] The authors can therefore compare NTM to the optimal predictor as well as LSTM. To assess performance they used a validation set of 1000 length 200 sequences sampled from the same distribution as the training data. As shown in Figure 13, NTM achieves a small, but significant performance advantage over LSTM, but never quite reaches the optimum cost.

**Figure 13** (p. 19): "Dynamic N-Gram Learning Curves."
The figure plots cost per sequence (bits) on the y-axis (range 130–160) against sequence number (thousands) on the x-axis (range 0–1000). Four curves are shown: LSTM (blue, cross markers), NTM with LSTM Controller (green, square markers), NTM with Feedforward Controller (red, triangle markers), and Optimal Estimator (orange, star markers). All models start around 150–155 bits. LSTM converges to approximately 137 bits. Both NTM variants converge to approximately 133–134 bits. The Optimal Estimator sits at approximately 131–132 bits. NTM achieves a small but visible improvement over LSTM, but neither reaches the optimal cost.

## Inference analysis [p. 19–20]

[p. 19–20] The evolution of the two architecture's predictions as they observe new inputs is shown in Figure 14, along with the optimal predictions. Close analysis of NTM's memory usage (Figure 15) suggests that the controller uses the memory to count how many ones and zeros it has observed in different contexts, allowing it to implement an algorithm similar to the optimal estimator.

**Figure 14** (p. 20): "Dynamic N-Gram Inference. The top row shows a test sequence from the N-Gram task, and the rows below show the corresponding predictive distributions emitted by the optimal estimator, NTM, and LSTM. In most places the NTM predictions are almost indistinguishable from the optimal ones. However at the points indicated by the two arrows it makes clear mistakes, one of which is explained in Figure 15. LSTM follows the optimal predictions closely in some places but appears to diverge further as the sequence progresses; we speculate that this is due to LSTM 'forgetting' the observations at the start of the sequence."
The figure has four rows labelled Input, Optimal, NTM, and LSTM, each showing a horizontal strip representing the binary input sequence and predictive distributions over time. NTM's row closely matches Optimal except at two points marked by red arrows. LSTM's row diverges increasingly from Optimal as the sequence progresses.

**Figure 15** (p. 20): "NTM Memory Use During the Dynamic N-Gram Task. The red and green arrows indicate point where the same context is repeatedly observed during the test sequence ('00010' for the green arrows, '01111' for the red arrows). At each such point the same location is accessed by the read head, and then, on the next time-step, accessed by the write head. We postulate that the network uses the writes to keep count of the fraction of ones and zeros following each context in the sequence so far. This is supported by the add vectors, which are clearly anti-correlated at places where the input is one or zero, suggesting a distributed 'counter.' Note that the write weightings grow fainter as the same context is repeatedly seen; this may be because the memory records a ratio of ones to zeros, rather than absolute counts. The red box in the prediction sequence corresponds to the mistake at the first red arrow in Figure 14; the controller appears to have accessed the wrong memory location, as the previous context was '01101' and not '01111.'"
The figure shows five horizontal strips labelled (top to bottom): Add Vectors, Write Weights, Predictions, Inputs, Read Weights. Time flows left to right. Red and green arrows mark recurring contexts. In Write Weights and Read Weights, the same memory locations are accessed each time the same 5-bit context appears. Add Vectors show anti-correlated patterns at 0/1 input positions. Write weightings grow fainter with repeated access. A red box in the Predictions row marks an error.
