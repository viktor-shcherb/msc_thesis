# Overview

**Title:** Neural Turing Machines
**Authors:** Alex Graves, Greg Wayne, Ivo Danihelka
**Affiliations:** Google DeepMind, London, UK
**Venue:** arXiv preprint (arXiv:1410.5401v2)
**Date:** 10 December 2014

## Abstract

> "We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that *Neural Turing Machines* can infer simple algorithms such as copying, sorting, and associative recall from input and output examples." [p. 1]

## Section headings

1. Introduction
2. Foundational Research
   - 2.1 Psychology and Neuroscience
   - 2.2 Cognitive Science and Linguistics
   - 2.3 Recurrent Neural Networks
3. Neural Turing Machines
   - 3.1 Reading
   - 3.2 Writing
   - 3.3 Addressing Mechanisms
     - 3.3.1 Focusing by Content
     - 3.3.2 Focusing by Location
   - 3.4 Controller Network
4. Experiments
   - 4.1 Copy
   - 4.2 Repeat Copy
   - 4.3 Associative Recall
   - 4.4 Dynamic N-Grams
   - 4.5 Priority Sort
   - 4.6 Experimental Details
5. Conclusion
6. Acknowledgments
