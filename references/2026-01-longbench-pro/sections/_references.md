# References

Only references cited in the section notes are included.

### An et al. (2023)
Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023. *L-Eval: Instituting standardized evaluation for long context language models*. arXiv:2307.11088.
- Cited in `06_related-works.md` as standardized protocol benchmark.

### Anthropic (2025)
Anthropic. 2025. *Introducing Claude 4*. https://www.anthropic.com/news/claude-4.
- Cited in `01_introduction.md` and `03_construction-process-longbench-pro.md`.

### Bai et al. (2023)
Yushi Bai et al. 2023. *LongBench: A bilingual, multitask benchmark for long context understanding*. arXiv:2308.14508.
- Cited in `06_related-works.md` as predecessor benchmark.

### Bai et al. (2025)
Yushi Bai et al. 2025. *LongBench v2: Towards deeper understanding and reasoning on realistic long-context multitasks*. ACL 2025, pp. 3639-3664.
- Cited in `01_introduction.md` and `06_related-works.md`.

### Comanici et al. (2025)
George Comanici et al. 2025. *Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities*. arXiv:2507.06261.
- Cited in `01_introduction.md`, `03_construction-process-longbench-pro.md`, and `05_evaluation.md`.

### DeepSeek-AI et al. (2025)
DeepSeek-AI et al. 2025. *DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning*. arXiv:2501.12948.
- Cited in `05_evaluation.md` (model family context).

### Hsieh et al. (2024)
Chia-Ping Hsieh et al. 2024. *RULER: Whatâ€™s the real context size of your long-context language models?* arXiv:2404.06654.
- Cited in `01_introduction.md` and `06_related-works.md`.

### Liu et al. (2023)
Nelson F. Liu et al. 2023. *Lost in the Middle: How language models use long contexts*. arXiv:2307.03172.
- Cited in `06_related-works.md` as positional/effective-length confound evidence.

### Liu et al. (2024)
Aixin Liu et al. 2024. *DeepSeek-V3 technical report*. arXiv:2412.19437.
- Cited in `05_evaluation.md` model references.

### Liu et al. (2025)
Aixin Liu et al. 2025. *DeepSeek-V3.2: Pushing the frontier of open large language models*. arXiv:2512.02556.
- Cited in `03_construction-process-longbench-pro.md` and `05_evaluation.md`.

### OpenAI (2025)
OpenAI. 2025. *Introducing GPT-5*. https://openai.com/index/introducing-gpt-5/.
- Cited in `01_introduction.md`, `03_construction-process-longbench-pro.md`, and `05_evaluation.md`.

### Qiu et al. (2024)
Zhiyuan Qiu et al. 2024. *CLongEval: A Chinese benchmark for evaluating long-context large language models*. arXiv:2403.03514.
- Cited in `01_introduction.md` and `06_related-works.md`.

### Shaham et al. (2022)
Uri Shaham et al. 2022. *SCROLLS: Standardized comparison over long language sequences*. arXiv:2201.03533.
- Cited in `06_related-works.md`.

### Shaham et al. (2023)
Uri Shaham et al. 2023. *ZeroSCROLLS: A zero-shot benchmark for long text understanding*. arXiv:2305.14196.
- Cited in `06_related-works.md`.

### Shao et al. (2025)
Zezhong Shao et al. 2025. *DeepSeekMath-V2: Towards self-verifiable mathematical reasoning*. arXiv:2511.22570.
- Cited in `07_conclusion-future-work.md`.

### Wang et al. (2024)
Chenxi Wang et al. 2024. *Ada-L-Eval: Evaluating long-context LLMs with length-adaptable benchmarks*. arXiv:2404.06480.
- Cited in `06_related-works.md`.

### Wei et al. (2022)
Jason Wei et al. 2022. *Chain-of-Thought Prompting Elicits Reasoning in Large Language Models*. NeurIPS 35:24824-24837.
- Cited in `03_construction-process-longbench-pro.md`.

### Yang et al. (2025)
An Yang et al. 2025. *Qwen3 technical report*. arXiv:2505.09388.
- Cited in `03_construction-process-longbench-pro.md` and `05_evaluation.md`.

### Yen et al. (2024)
Hung-Yi Yen et al. 2024. *HELMET: How to evaluate long-context language models effectively and thoroughly*. arXiv:2410.02694.
- Cited in `01_introduction.md` and `06_related-works.md`.

### Zhang et al. (2024)
Xiangyu Zhang et al. 2024. *InfinityBench: Extending long context evaluation beyond 100K tokens*. ACL 2024, pp. 15262-15277.
- Cited in `06_related-works.md`.

### Zhou et al. (2025)
Yutong Zhou et al. 2025. *GSM-Infinite: How do your LLMs behave over infinitely increasing context length and reasoning complexity?* arXiv:2502.05252.
- Cited in `06_related-works.md`.
