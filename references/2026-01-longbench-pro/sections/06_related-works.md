# 6 Related Works [p. 13]

[p. 13] The section frames long-context evaluation as requiring robust measurement of retrieval, integration, and reasoning under sparse/distributed evidence, while controlling for confounders such as positional effects and claimed-vs-effective context mismatch.

[p. 13] Benchmarks are grouped by construction philosophy:
- **Controlled synthetic diagnostics:** RULER (Hsieh et al., 2024), MRCR (OpenAI, 2025), GSM-infinite (Zhou et al., 2025).
- **Long-document NLP suites and protocol-oriented evaluation:** SCROLLS/ZeroSCROLLS (Shaham et al., 2022, 2023), L-Eval and Ada-L-Eval (An et al., 2023; Wang et al., 2024).
- **Realistic benchmark datasets:** LongBench and LongBench v2 (Bai et al., 2023, 2025), InfinityBench (Zhang et al., 2024), CLongEval (Qiu et al., 2024), HELMET (Yen et al., 2024).

[p. 13] Positioning claim for LongBench Pro:
- fully natural documents,
- bilingual coverage (EN/ZH),
- diverse tasks/metrics,
- explicit context/length/difficulty categorization,
- scalable human-model collaborative construction.
