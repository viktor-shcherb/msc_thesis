# 1 Introduction [p. 1-3]

[p. 1] Large language models (LLMs) typically come with a pre-defined context window size. For example, inputs to LLaMA models (Touvron et al., 2023) must be fewer than 2048 tokens. This pre-set context window limit is frequently exceeded in applications such as conducting long conversations, summarizing long documents, or executing long-term planning. Training an LLM from scratch with long context windows requires significant investments, motivating the question: can we extend the context window of an existing pre-trained LLM?

One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer context window. However, the authors empirically found that models trained this way adapt to long context windows very slowly. After training for more than 10000 batches, the effective context window saw a minimal increase, moving from 2048 to 2560 (Table 4). This suggests that direct fine-tuning is inefficient for extending to substantially longer context windows. [p. 1]

[p. 1] While certain techniques such as ALiBi (Press et al., 2022) and LeX (Sun et al., 2022) enable length extrapolation of Transformers (i.e., train on short context windows and inference on longer ones), many existing pre-trained LLMs, including LLaMA (Touvron et al., 2023), use positional encodings that have weak extrapolation properties (e.g., RoPE (Su et al., 2021)). Therefore, the applicability of these techniques for extending the context window sizes of such LLMs remains limited.

The authors introduce Position Interpolation to enable context window extensions for certain existing pre-trained LLMs, including LLaMA. The key idea is that, instead of extrapolation, they directly down-scale the position indices so that the maximum position index matches the previous context window limit in the pre-training stage (see Figure 1). In other words, to accommodate more input tokens, they interpolate the position encodings at neighboring integer positions, utilizing the fact that position encodings can be applied on non-integer positions, as opposed to extrapolating outside the trained positions, which may lead to catastrophic values. [p. 1]

[p. 2] They verify their approach theoretically, by showing that the interpolated attention score has a much smaller upper bound (~600x smaller in LLaMA 7B setting) than the extrapolated one, and is thus much more stable. Therefore, interpolated position encodings are easier for the model to adapt.

Empirically, Position Interpolation is highly effective and efficient, requiring only a very short period of fine-tuning for the model to fully adapt to greatly extended context windows. Experimental results presented for extending the context window to up to 32768 from the initial 2048 across 7B to 65B LLaMA models using Position Interpolation show that:

1. Position Interpolation can easily enable very long context windows (e.g. 32768), requiring only fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality. The cost of fine-tuning is negligible compared to the pre-training costs. This confirms their hypothesis that it is relatively easy for the models to adapt to interpolated position encodings. [p. 2]

2. Position Interpolation generates strong models that can effectively make use of much extended context window. Models extended by Position Interpolation enjoy significant perplexity gains from greatly extended context windows for text modeling, and the perplexity reduces gracefully with the enlargement of context windows. They also applied Position Interpolation in a long text summarization task, and demonstrate competitive performances. [p. 2]

3. Position Interpolation preserves model quality relatively well for tasks within its original context window sizes. Compared with original LLaMA models, the extended LLaMA models saw a minor degradation on several standard benchmarks within a 2048 token limit. [p. 2]

The results highlight the innate ability of Transformer models to "extrapolate to sequence lengths longer than the ones encountered during training" as hypothesized in the seminal work of Vaswani et al. (2017). The authors reaffirm this hypothesis and suggest that the previously known weakness of extrapolating to longer sequences for language modeling (Press et al., 2022) may be due to direct extrapolation of positional encodings and it can be largely mitigated by interpolating position encodings instead. [p. 2-3]

**Concurrent work.** Right before their release, the authors were informed with a concurrent blogpost (SuperHOT, kaiokendev (2023)) that also interpolates positional encoding in RoPE to extend the context window from 2K to 8K. Open source community picked it up in Reddit post and Github Issues, which shows that fine-tuning with LoRA (Hu et al., 2021) also seems to work well. The authors' paper shows a full fine-tuning with up to 65B model work well with Position Interpolation, and they also give theoretical explanations why interpolation achieves much more stable results than extrapolation, by showing that the upper bound of interpolated attention score is much lower than that of extrapolated ones. [p. 3]

## Figure 1

**Figure 1** (p. 2): "An illustration of our Position Interpolation method. Consider a Llama model pre-trained with a 2048 context window length. Upper left illustrates the normal usage of an LLM model: input position indices (blue dots) are within the pre-trained range. Upper right illustrates length extrapolation where models are required to operate unseen positions (red dots) up to 4096. Lower left illustrates Position Interpolation where we downscale the position indices (blue and green dots) themselves from [0, 4096] to [0, 2048] to force them to reside in the pretrained range."

The figure has two rows. Upper row has two panels: upper left ("Normal") shows blue dots within [0, 2048] pre-trained range with RoPE values (y-axis, range approximately -1.0 to 1.0) versus position (x-axis, 0 to 2048); upper right ("Extrapolation") shows blue dots in [0, 2048] and red dots in unseen range [2048, 4096] where the model is required to operate on unseen positions. The green shaded region marks the "Unseen Range". Lower row has one panel ("Position Interpolation"): blue and green dots within [0, 2048] pre-trained range, covering positions [0, 4096] that have been downscaled via the transformation f'(x, m) = f(x, m/2). The x-axis label shows "Position" going to 4096 but position indices are mapped to the pre-trained range. The key visual point is that interpolation keeps all position indices within the pretrained range by downscaling, while extrapolation ventures into unseen territory. [p. 2]
