# References

Only references actually cited in the section notes are included below.

## Ainslie et al. (2023)
Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Ontanon, Siddhartha Brahma, Yury Zemlyanskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, Yun-Hsuan Sung, and Sumit Sanghai. CoLT5: Faster long-range transformers with conditional computation, 2023.
- Cited in 03e_long-document-summarization.md as a baseline on GovReport summarization (SCROLLS Leaderboard).

## Azerbayev et al. (2022)
Zhangir Azerbayev, Edward Ayers, and Bartosz Piotrowski. Proof-pile, 2022. URL https://github.com/zhangir-azerbayev/proof-pile.
- Cited in 03b_long-sequence-language-modeling.md as the source of the Arxiv Math Proof-pile evaluation dataset.

## Beltagy et al. (2020)
Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer, 2020.
- Cited in 04_related-work.md as an approximated multi-head attention method.

## Bulatov et al. (2022)
Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Recurrent memory transformer, 2022.
- Cited in 04_related-work.md as a recurrent/memory transformer approach.

## Child et al. (2019)
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers, 2019.
- Cited in 04_related-work.md as an approximated multi-head attention method.

## Choromanski et al. (2021)
Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with performers. In *9th International Conference on Learning Representations, ICLR 2021*. OpenReview.net, May 2021.
- Cited in 04_related-work.md as an approximated multi-head attention method.

## Computer (2023)
Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL https://github.com/togethercomputer/RedPajama-Data.
- Cited in 03a_setup.md as an alternative fine-tuning dataset used in Section 3.4.

## Dai et al. (2019)
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*, pp. 2978-2988, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285.
- Cited in 04_related-work.md as a recurrent/memory transformer approach.

## Dao et al. (2022)
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In *Advances in Neural Information Processing Systems*, 2022.
- Cited in 03a_setup.md as part of the training framework (Flash Attention).

## Dosovitskiy et al. (2021)
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In *International Conference on Learning Representations*, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.
- Cited in 04_related-work.md as the most related technique (linearly interpolating learnt position embeddings in Vision Transformers).

## Gao et al. (2020)
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800gb dataset of diverse text for language modeling. *arXiv preprint arXiv:2101.00027*, 2020.
- Cited in 01_introduction.md, 02c_position-interpolation.md, 03a_setup.md, 03b_long-sequence-language-modeling.md as the primary pre-training/fine-tuning dataset (the Pile).

## Guu et al. (2020)
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-augmented language model pre-training, 2020.
- Cited in 04_related-work.md as a retrieval-augmented LLM approach.

## Haviv et al. (2022)
Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy. Transformer language models without positional encodings still learn positional information, 2022.
- Cited in 04_related-work.md as a length extrapolation method.

## Hu et al. (2021)
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685*, 2021.
- Cited in 01_introduction.md in the concurrent work discussion (fine-tuning with LoRA).

## Huang et al. (2021)
Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. In *Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pp. 1419-1436, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.112.
- Cited in 03e_long-document-summarization.md as the source of the GovReport dataset.

## Izacard et al. (2022)
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models, 2022.
- Cited in 04_related-work.md as a retrieval-augmented LLM approach.

## Jiang et al. (2022)
Zhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding, Zhiruo Wang, Jamie Callan, and Graham Neubig. Retrieval as attention: End-to-end learning of retrieval and reading within a single transformer, 2022.
- Cited in 04_related-work.md as a retrieval-augmented LLM approach.

## kaiokendev (2023)
kaiokendev. Things im learning while training superhot. https://kaiokendev.github.io/til#extending-context-to-8k, 2023.
- Cited in 01_introduction.md as concurrent work that also interpolates positional encoding in RoPE to extend context from 2K to 8K.

## Karpukhin et al. (2020)
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pp. 6769-6781. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.emnlp-main.550.
- Cited in 04_related-work.md as a retrieval-augmented LLM approach.

## Khattab et al. (2021)
Omar Khattab, Christopher Potts, and Matei Zaharia. Relevance-guided supervision for openqa with colbert. *Transactions of the Association for Computational Linguistics*, 9:929-944, 2021. doi: 10.1162/tacl_a_00405.
- Cited in 04_related-work.md as a retrieval-augmented LLM approach.

## Kitaev et al. (2020)
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In *8th International Conference on Learning Representations, ICLR 2020*. OpenReview.net, April 2020.
- Cited in 04_related-work.md as an approximated multi-head attention method.

## Kudo & Richardson (2018)
Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*, pp. 66-71, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012.
- Cited in 03b_long-sequence-language-modeling.md as the tokenizer used for the Proof-pile evaluation.

## Lin (2004)
Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In *Text Summarization Branches Out*, pp. 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013.
- Cited in 03e_long-document-summarization.md as the evaluation metric (ROUGE scores).

## Loshchilov & Hutter (2019)
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In *International Conference on Learning Representations*, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.
- Cited in 03a_setup.md as the optimizer (AdamW).

## Martins et al. (2021)
Pedro Henrique Martins, Zita Marinho, and Andre F. T. Martins. Infinity-former: Infinite memory transformer, 2021.
- Cited in 04_related-work.md as a recurrent/memory transformer approach.

## Mohtashami & Jaggi (2023)
Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers. *arXiv preprint arXiv:2305.16300*, 2023.
- Cited in 04_related-work.md as a landmark attention method, and in 03c_passkey-retrieval.md as the source of the passkey retrieval evaluation task.

## Mu et al. (2023)
Jesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens, 2023.
- Cited in 04_related-work.md as a recurrent/memory transformer that suggested compressed past inputs may prevent remembering specific details.

## Paszke et al. (2019)
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. *PyTorch: An Imperative Style, High-Performance Deep Learning Library*. Curran Associates Inc., Red Hook, NY, USA, 2019.
- Cited in 03a_setup.md as part of the training framework (PyTorch).

## Press et al. (2022)
Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In *International Conference on Learning Representations*, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0.
- Cited in 01_introduction.md as an existing length extrapolation technique (ALiBi), in 03b_long-sequence-language-modeling.md for the sliding window evaluation approach, and in 04_related-work.md as a length extrapolation method.

## Rae et al. (2020)
Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling. In *International Conference on Learning Representations*, 2020. URL https://openreview.net/forum?id=SylKikSYDH.
- Cited in 03b_long-sequence-language-modeling.md as the source of the PG-19 book corpus evaluation dataset.

## Ren et al. (2021)
Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. Combiner: Full attention transformer with sparse computation cost. In *Advances in Neural Information Processing Systems 34, NeurIPS 2021*, pp. 22470-22482. Curran Associates, Inc., 2021.
- Cited in 04_related-work.md as an approximated multi-head attention method.

## Santhanam et al. (2022)
Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. Colbertv2: Effective and efficient retrieval via lightweight late interaction. In *Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pp. 3715-3734, Seattle, United States, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.272.
- Cited in 04_related-work.md as a retrieval-augmented LLM approach.

## Shaham et al. (2022)
Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized CompaRison over long language sequences. In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*, pp. 12007-12021, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.823.
- Cited in 03e_long-document-summarization.md as the source of the SCROLLS Leaderboard.

## Su et al. (2021)
Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2021.
- Cited in 00_overview.md, 01_introduction.md, 02a_background-rope.md, 02b_direct-extrapolation.md, 02c_position-interpolation.md as the original RoPE paper.

## Sun et al. (2022)
Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer, 2022.
- Cited in 01_introduction.md as an existing length extrapolation technique (LeX), and in 04_related-work.md as a length extrapolation method.

## Touvron et al. (2023)
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.
- Cited in 00_overview.md, 01_introduction.md, 02a_background-rope.md, 03a_setup.md, 04_related-work.md as the LLaMA model paper (the primary model extended in this work).

## Vaswani et al. (2017)
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In *Advances in Neural Information Processing Systems*, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
- Cited in 01_introduction.md as the seminal Transformer work whose hypothesis about extrapolation to longer sequences is reaffirmed.

## Wang et al. (2020)
Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity, 2020.
- Cited in 04_related-work.md as an approximated multi-head attention method.

## Wu et al. (2020)
Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, Alborz Geramifard, and Zhou Yu. Memformer: A memory-augmented transformer for sequence modeling, 2020.
- Cited in 04_related-work.md as a recurrent/memory transformer approach.

## Wu et al. (2022)
Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In *The Tenth International Conference on Learning Representations, ICLR 2022*. OpenReview.net, April 2022.
- Cited in 04_related-work.md as a recurrent/memory transformer approach.

## Zaheer et al. (2020)
Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. In *Advances in Neural Information Processing Systems 33, NeurIPS 2020*. Curran Associates, Inc., 2020.
- Cited in 04_related-work.md as an approximated multi-head attention method.

## Zhang et al. (2022)
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022.
- Cited in 04_related-work.md as an LLM with learnable position embeddings where Position Interpolation could potentially be applied.

## Zhao et al. (2023)
Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, and Shen Li. Pytorch fsdp: Experiences on scaling fully sharded data parallel, 2023.
- Cited in 03a_setup.md as part of the training framework (Fully Sharded Data Parallel).
