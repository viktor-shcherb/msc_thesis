# 8.1 Data [p. 63–64]

## 8.1.1 Speech Understanding [p. 63–64]

[p. 63] The training data can be categorized into two types. The pre-training data includes a large amount of unlabeled speech, which is used to initialize the speech encoder in a self-supervised manner. The supervised finetuning data includes speech recognition, speech translation, and spoken dialogue data; this data is used to unlock specific abilities when integrated with the large language model. [p. 63]

**Pre-training data.** To pre-train the speech encoder, a dataset of approximately 15M hours of speech recordings encompassing a large number of languages is curated. Audio data is filtered using a voice activity detection (VAD) model and audio samples with a VAD threshold above 0.7 are selected for pre-training. In speech pre-training data, the focus is also on ensuring the absence of PII. The Presidio Analyzer is used to identify such PII. [p. 63]

**Speech recognition and translation data.** The ASR training data contains 230K hours of manually transcribed speech recordings that span 34 languages. The AST training data contains 90K hours of translations in two directions: from 33 languages to English and from English to 33 languages. This data contains both supervised and synthetic data generated using the NLLB toolkit (NLLB Team et al., 2022). The use of synthetic AST data enables increased model quality for low-resource languages. The speech segments in the data have a maximum length of 60 seconds. [p. 63]

**Spoken dialogue data.** [p. 63–64] To finetune the speech adapter for spoken dialogue, responses are synthetically generated for speech prompts by asking the language model to respond to transcriptions of those prompts (Fathullah et al., 2024). Synthetic data is generated this way using a subset of the ASR dataset with 60K hours of speech. [p. 64] In addition, 25K hours of synthetic data are generated by running the Voicebox TTS system (Le et al., 2024) on subsets of the data used to finetune Llama 3. Several heuristics are used to select a subset of finetuning data that matches the distribution of speech. These heuristics include focusing on relatively short prompts with a simple structure and without non-text symbols. [p. 64]

## 8.1.2 Speech Generation [p. 64]

[p. 64] The speech generation datasets mainly consist of those for training the text normalization (TN) model and the prosody model (PM). Both training data are augmented with an additional input feature of the Llama 3 embeddings to provide contextual information. [p. 64]

**Text normalization data.** The TN training dataset includes 55K samples that cover a wide range of semiotic classes (e.g., number, date, time) that require non-trivial normalization. Each sample is a pair of written-form text and the corresponding normalized spoken-form text, with an inferred sequence of handcrafted TN rules that carry out the normalization. [p. 64]

**Prosody model data.** The PM training data includes linguistic and prosodic features extracted from a 50K-hour TTS dataset, which are paired transcripts and audios recorded by professional voice actors in studio settings. [p. 64]

**Llama 3 embedding.** The Llama 3 embeddings are taken as the output of the 16th decoder layer. The work is done exclusively with the Llama 3 8B model and embeddings are extracted for a given text (i.e. written-form input text for TN or the audio transcript for PM) as if they are generated by the Llama 3 model with an empty user prompt. In a given sample, each chunk in the Llama 3 token sequence is explicitly aligned with the corresponding chunks in native input sequence for TN or PM, i.e., TN-specific text tokens (demarcated by unicode category) or phone-rate features respectively. This allows for streaming input of Llama 3 tokens and embeddings. [p. 64]
