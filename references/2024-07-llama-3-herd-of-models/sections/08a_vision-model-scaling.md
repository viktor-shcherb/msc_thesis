# 7.3 Model Scaling [p. 57â€“58]

[p. 57] After the visual-recognition components are added to Llama 3, the model contains self-attention layers, cross-attention layers, and a ViT image encoder. To train adapters for the smaller 8B and 70B parameter models, a combination of data and tensor parallelization is found to be the most efficient. Model or pipeline parallelism does not increase efficiency at these scales because the gathering of model parameters would dominate the computation. However, pipeline parallelism (in addition to data and tensor parallelism) is used when training the adapter for the 405B parameter model. Training at this scale introduces three new challenges in addition to those outlined in Section 3.3: model heterogeneity, data heterogeneity, and numerical instabilities. [p. 57]

**Model heterogeneity.** The model computation is heterogeneous because more computation is performed on some tokens than on others. In particular, image tokens are processed by the image encoder and the cross-attention layers, whereas text tokens are only processed by the language backbone. This heterogeneity leads to bottlenecks in the scheduling of pipeline parallelism. This problem is addressed by ensuring each pipeline stage contains five layers: namely, four self-attention layers in the language backbone and a cross-attention layer. (Recall that a cross-attention layer is introduced after every fourth self-attention layer.) In addition, the image encoder is replicated on all pipeline stages. Because training is on paired image-text data, this enables load balancing between the image and text parts of the computation. [p. 57]

[p. 58] **Data heterogeneity.** The data is heterogeneous because, on average, images have more tokens than the associated text: an image has 2,308 tokens, whereas the associated text contains an average of only 192 tokens. As a result, the computation of cross-attention layers requires more time and memory than the computation of self-attention layers. This problem is addressed by introducing sequence parallelization in the image encoder, so that each GPU processes roughly the same number of tokens. Because the average text size is relatively short, a substantially larger micro-batch size (8 instead of 1) is also used. [p. 58]

**Numerical instabilities.** After the image encoder is added to the model, performing gradient accumulation in bf16 leads to numerical instabilities. The most likely explanation for this is that image tokens are introduced into the language backbone via *all* cross-attention layers. This implies that numerical deviations in the representation of an image token have an outsized impact on the overall computation because the errors are compounded. This is addressed by performing gradient accumulation in FP32. [p. 58]
