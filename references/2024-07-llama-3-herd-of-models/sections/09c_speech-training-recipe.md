# 8.3 Training Recipe [p. 65–66]

## 8.3.1 Speech Understanding [p. 65–66]

[p. 65] Training of the speech module is done in two stages. The first stage, speech pre-training, leverages unlabeled data to train a speech encoder that exhibits strong generalization capabilities across languages and acoustic conditions. In the second stage, supervised fine-tuning, the adapter and pre-trained encoder are integrated with the language model, and trained jointly with it while the LLM stays frozen. This enables the model to respond to speech input. This stage uses labeled data corresponding to speech understanding abilities. [p. 65]

Multilingual ASR and AST modeling often results in language confusion/interference, which leads to degraded performance. A popular way to mitigate this is to incorporate language identification (LID) information, both on the source and target side. This can lead to improved performance in the predetermined set of directions, but it does come with potential loss of generality. For instance, if a translation system expects LID on both source and target side, then the model will not likely show good zero-shot performance on directions that were not seen in training. The challenge is to design a system that allows LID information to some extent, but keeps the model general enough such that the model can do speech translation in unseen directions. To address this, system prompts are designed which only contain LID for the text to be emitted (target side). There is no LID information for the speech input (source side) in these prompts, which also potentially allows it to work with code-switched speech. For ASR, the following system prompt is used: **Repeat after me in {language}:**, where {language} comes from one of the 34 languages (English, French, etc.). For speech translation, the system prompt is: **Translate the following sentence into {language}:**. This design has been shown to be effective in prompting the language model to respond in the desired language. The same system prompts are used during training and inference. [p. 65]

**Speech pre-training.** [p. 65–66] The self-supervised BEST-RQ algorithm (Chiu et al., 2022) is used to pre-train the speech [p. 66] encoder. A mask of 32-frame length with a probability of 2.5% is applied to the input mel-spectrogram. If the speech utterances are longer than 60 seconds, a random crop of 6K frames is performed, corresponding to 60 seconds of speech. Mel-spectrogram features are quantized by stacking 4 consecutive frames, projecting the 320-dimensional vectors to a 16-dimensional space, and performing a nearest-neighbor search with respect to cosine similarity metric within a codebook of 8,192 vectors. To stabilize pre-training, 16 different codebooks are employed. The projection matrix and codebooks are randomly initialized and are not updated throughout the model training. The multi-softmax loss is used only on masked frames for efficiency reasons. The encoder is trained for 500K steps with a global batch size of 2,048 utterances. [p. 66]

**Supervised finetuning.** [p. 66] Both the pre-trained speech encoder and the randomly initialized adapter are further jointly optimized with Llama 3 in the supervised finetuning stage. The language model remains unchanged during this process. The training data is a mixture of ASR, AST, and spoken dialogue data. The speech model for Llama 3 8B is trained for 650K updates, using a global batch size of 512 utterances and an initial learning rate of 10^{-4}. The speech model for Llama 3 70B is trained for 600K updates, using a global batch size of 768 utterances and an initial learning rate of 4 x 10^{-5}. [p. 66]

## 8.3.2 Speech Generation [p. 66]

[p. 66] To support real-time processing, the prosody model employs a lookahead mechanism that considers a fixed number of future phones and a variable number of future tokens. This ensures consistent lookahead while processing incoming text, which is crucial for low-latency speech synthesis applications. [p. 66]

**Training.** A dynamic alignment strategy is developed utilizing causal masking to facilitate streamability in speech synthesis. This strategy incorporates a lookahead mechanism for a fixed number of future phones and a variable number of future tokens, aligning with the chunking process during text normalization (Section 8.1.2). For each phone, the token lookahead includes the maximum number of tokens defined by the chunk size, resulting in variable lookahead for Llama embeddings but fixed lookahead for phonemes. [p. 66]

The Llama 3 embeddings are sourced from the Llama 3 8B model, which remains frozen during the training of the Prosody Model. The input phone-rate features include both linguistic and speaker/style controllability elements. The model training is conducted with a batch size of 1,024 utterances, each with a maximum length of 500 phones. A learning rate of 9 x 10^{-4} is employed using the AdamW optimizer, training over 1 million updates with a learning rate warmup for the first 3,000 updates, following a cosine schedule. [p. 66]

**Inference.** During inference, the same lookahead mechanism and causal masking strategy are employed to ensure consistency between training and real-time processing. The PM handles incoming text in a streaming manner, updating the input phone by phone for phone-rate features and chunk by chunk for token-rate features. The new chunk input is updated only when the first phone for that chunk is current, maintaining the alignment and lookahead as during training. [p. 66]

For prosody target prediction, a delayed pattern approach (Kharitonov et al., 2021) is employed, which enhances the model's ability to capture and reproduce long-range prosodic dependencies. This approach contributes to the naturalness and expressiveness of the synthesized speech, ensuring low-latency and high-quality output. [p. 66]
