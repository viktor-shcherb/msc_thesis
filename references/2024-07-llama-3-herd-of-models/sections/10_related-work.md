# 9 Related Work [p. 69–70]

[p. 69] The development of Llama 3 builds on a large body of prior work studying foundation models for language, images, videos, and speech. A comprehensive overview of that work is outside the scope of this paper; the reader is referred to Bordes et al. (2024); Madan et al. (2024); Zhao et al. (2023a) for such overviews. Below, seminal works that directly influenced the development of Llama 3 are briefly outlined. [p. 69]

## 9.1 Language [p. 69–70]

**Scale.** Llama 3 follows the enduring trend of applying straightforward methods at ever increasing scales in foundation models. Improvements are driven by increased compute and improved data, with the 405B model using almost fifty times the pre-training compute budget of Llama 2 70B. Despite containing 405B parameters, the largest Llama 3 in fact contains fewer parameters than earlier and much less performant models such as PaLM (Chowdhery et al., 2023), due to better understanding of scaling laws (Kaplan et al., 2020; Hoffmann et al., 2022). Little is publicly known about the size of other frontier models, such as Claude 3 or GPT 4 (OpenAI, 2023a), but overall performance is comparable. [p. 69]

**Small models.** Developments in smaller models have paralleled those in large models. Models with fewer parameters can dramatically improve inference cost and simplify deployment (Mehta et al., 2024; Team et al., 2024). The smaller Llama 3 models achieve this by training far beyond the point of compute optimal training, effectively trading training compute for inference efficiency. An alternative path is to distill larger models into smaller ones, as in Phi (Abdin et al., 2024). [p. 69]

**Architectures.** While Llama 3 makes minimal architectural modifications to compared to Llama 2, other recent foundation models have explored other designs. Most notably, mixture of experts architectures (Shazeer et al., 2017; Lewis et al., 2021; Fedus et al., 2022; Zhou et al., 2022) can be used as an efficient way to increase the capacity of a models, such as in Mixtral (Jiang et al., 2024) and Arctic (Snowflake, 2024). Llama 3 outperforms these models, suggesting that dense architectures are not the limiting factor, but there remain numerous trade offs in terms of training and inference efficiency, and model stability at scale. [p. 69]

**Open source.** Open weights foundation models have rapidly improved over the last year, with Llama3-405B now competitive with the current closed weight state-of-the-art. Numerous model families have recently been developed, including Mistral (Jiang et al., 2023), Falcon (Almazrouei et al., 2023), MPT (Databricks, 2024), Pythia (Biderman et al., 2023), Arctic (Snowflake, 2024), OpenELM (Mehta et al., 2024), OLMo (Groeneveld et al., 2024), StableLM (Bellagente et al., 2024), OpenLLaMA (Geng and Liu, 2023), Qwen (Bai et al., 2023), Gemma (Team et al., 2024), Grok (XAI, 2024), and Phi (Abdin et al., 2024). [p. 69]

**Post-training.** Post-training Llama 3 follows the established strategy of instruction tuning (Chung et al., 2022; Ouyang et al., 2022) followed by alignment with human feedback (Kaufmann et al., 2023). While some studies have shown the surprising effectiveness of lightweight alignment procedures (Zhou et al., 2024), Llama 3 uses millions of human instructions and preference judgments to improve the pre-trained model, including techniques such as rejection sampling (Bai et al., 2022), supervised finetuning (Sanh et al., 2022), and Direct Preference Optimization (Rafailov et al., 2023). In order to curate these instruction and preference examples, earlier versions of Llama 3 are deployed to filter (Liu et al., 2024c), re-write (Pan et al., 2024), or generate prompts and responses (Liu et al., 2024b) and these techniques are applied through multiple rounds of post-training. [p. 70]

## 9.2 Multimodality [p. 70]

The experiments with multimodal capabilities for Llama 3 are part of a long line of work on foundation models that jointly model multiple modalities. [p. 70]

**Images.** A substantial body of work has trained image-recognition models on large amounts of image-text pairs, for example, Mahajan et al. (2018); Xiao et al. (2024a); Team (2024); OpenAI (2023b). Radford et al. (2021) presented one of the first models to jointly embed images and text via contrastive learning. More recently, a series of models has studied approaches similar to the one used in Llama 3, for example, Alayrac et al. (2022); Dai et al. (2023); Liu et al. (2023c,b); Yang et al. (2023b); Ye et al. (2023); Zhu et al. (2023). The approach in Llama 3 combines ideas from many of these papers to achieve results that are comparable with Gemini 1.0 Ultra (Google, 2023) and GPT-4 Vision (OpenAI, 2023b); see Section 7.6. [p. 70]

**Video.** Although video inputs are supported by an increasing number of foundation models (Google, 2023; OpenAI, 2023b), the body of work on joint modeling of videos and language is not that large. Akin to Llama 3, most current studies adopt an adapter approach to align video and language representations and unlock question-answering and reasoning about videos (Lin et al., 2023; Li et al., 2023a; Maaz et al., 2024; Zhang et al., 2023; Zhao et al., 2022). The finding is that such approaches produce results that are competitive with the state-of-the-art; see Section 7.7. [p. 70]

**Speech.** The work also fits in a larger body of work combining language and speech modeling. Earlier joint models of text and speech include AudioPaLM (Rubenstein et al., 2023), VioLA (Wang et al., 2023b), VoxtLM Maiti et al. (2023), SUTLM (Chou et al., 2023), and Spirit-LM (Nguyen et al., 2024). The work builds on prior compositional approaches to combining speech and language like Fathullah et al. (2024). Unlike most prior work, the choice is to not finetune the language model itself for speech tasks as doing so may lead to contention on non-speech tasks. The finding is that at larger model scales, strong performances are attainable even without such finetuning; see Section 8.4. [p. 70]
