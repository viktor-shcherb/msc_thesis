# Acknowledgments [p. 13]

[p. 13] The authors thank Phil Tillet and Daniel Haziza, who have implemented versions of FlashAttention in Triton [17] and the xformers library [10]. FlashAttention-2 was motivated by exchange of ideas between different ways that attention could be implemented.

The authors are grateful to the Nvidia CUTLASS team (especially Vijay Thakkar, Cris Cecka, Haicheng Wu, and Andrew Kerr) for their CUTLASS library, in particular the CUTLASS 3.x release, which provides clean abstractions and powerful building blocks for the implementation of FlashAttention-2.

Driss Guessous is thanked for integrating FlashAttention to PyTorch.

FlashAttention-2 has benefited from helpful discussions with Phil Wang, Markus Rabe, James Bradbury, Young-Jun Ko, Julien Launay, Daniel Hesslow, Michael Benesty, Horace He, Ashish Vaswani, and Erich Elsen.

Thanks for Stanford CRFM and Stanford NLP for the compute support. Dan Fu and Christopher Re are thanked for their collaboration, constructive feedback, and constant encouragement on this line of work of designing hardware-efficient algorithms. Albert Gu and Beidi Chen are thanked for their helpful suggestions on early drafts of this technical report.
