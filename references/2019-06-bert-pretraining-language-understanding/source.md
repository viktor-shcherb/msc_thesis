# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

**Authors:** Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
**Affiliation:** Google AI Language

## Publication Status

- **arXiv preprint:** October 2018, arXiv:1810.04805
- **Peer-reviewed:** Yes
- **Conference:** Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171--4186, Minneapolis, Minnesota, June 2--7, 2019. Association for Computational Linguistics.
- **DOI:** 10.18653/v1/N19-1423
- **Status:** Published conference paper

## Preferred Citation

Cite the NAACL 2019 version:

> Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171--4186, Minneapolis, Minnesota. Association for Computational Linguistics.

## Links

- arXiv: https://arxiv.org/abs/1810.04805
- ACL Anthology: https://aclanthology.org/N19-1423/
- Code: https://github.com/google-research/bert
