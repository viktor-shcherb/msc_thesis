# References

Full bibliographic details from the paper's reference list. Only references actually cited in section notes are included.

- **Akbik et al., 2018** — Alan Akbik, Duncan Blythe, and Roland Vollgraf. "Contextual string embeddings for sequence labeling." In *Proceedings of the 27th International Conference on Computational Linguistics*, pages 1638-1649, 2018. Cited in 05_ablation-studies.md (CSE NER baseline in Table 7).

- **Al-Rfou et al., 2018** — Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. "Character-level language modeling with deeper self-attention." *arXiv preprint arXiv:1808.04444*, 2018. Cited in 05_ablation-studies.md (largest Transformer in literature, 235M parameters).

- **Ando and Zhang, 2005** — Rie Kubota Ando and Tong Zhang. "A framework for learning predictive structures from multiple tasks and unlabeled data." *Journal of Machine Learning Research*, 6(Nov):1817-1853, 2005. Cited in 02_related-work.md (feature-based approaches).

- **Bentivogli et al., 2009** — Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. "The fifth PASCAL recognizing textual entailment challenge." In *TAC*, 2009. Cited in 08_appendix-b.md (RTE dataset description).

- **Blitzer et al., 2006** — John Blitzer, Ryan McDonald, and Fernando Pereira. "Domain adaptation with structural correspondence learning." In *Proceedings of the 2006 conference on empirical methods in natural language processing*, pages 120-128. Association for Computational Linguistics, 2006. Cited in 02_related-work.md (feature-based approaches).

- **Bowman et al., 2015** — Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. "A large annotated corpus for learning natural language inference." In *EMNLP*. Association for Computational Linguistics, 2015. Cited in 01_introduction.md.

- **Brown et al., 1992** — Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. "Class-based n-gram models of natural language." *Computational linguistics*, 18(4):467-479, 1992. Cited in 02_related-work.md (feature-based approaches).

- **Cer et al., 2017** — Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation." In *Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)*, 2017. Cited in 08_appendix-b.md (STS-B dataset description).

- **Chen et al., 2018** — Z. Chen, H. Zhang, X. Zhang, and L. Zhao. "Quora question pairs." 2018. Cited in 08_appendix-b.md (QQP dataset description).

- **Chelba et al., 2013** — Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. "One billion word benchmark for measuring progress in statistical language modeling." *arXiv preprint arXiv:1312.3005*, 2013. Cited in 03_bert.md (pre-training data, contrasted as a shuffled sentence-level corpus).

- **Clark and Gardner, 2018** — Christopher Clark and Matt Gardner. "Simple and effective multi-paragraph reading comprehension." In *ACL*, 2018. Cited in 04_experiments.md (SQuAD v1.1 baselines).

- **Clark et al., 2018** — Kevin Clark, Minh-Thang Luong, Christopher D Manning, and Quoc Le. "Semi-supervised sequence modeling with cross-view training." In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing*, pages 1914-1925, 2018. Cited in 05_ablation-studies.md (CVT NER baseline in Table 7).

- **Collobert and Weston, 2008** — Ronan Collobert and Jason Weston. "A unified architecture for natural language processing: Deep neural networks with multitask learning." In *Proceedings of the 25th international conference on Machine learning*, pages 160-167. ACM, 2008. Cited in 02_related-work.md (fine-tuning approaches).

- **Conneau et al., 2017** — Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. "Supervised learning of universal sentence representations from natural language inference data." In *Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing*, pages 670-680, Copenhagen, Denmark. Association for Computational Linguistics, 2017. Cited in 02_related-work.md (transfer learning from supervised data).

- **Dai and Le, 2015** — Andrew M Dai and Quoc V Le. "Semi-supervised sequence learning." In *Advances in neural information processing systems*, pages 3079-3087, 2015. Cited in 01_introduction.md, 02_related-work.md (fine-tuning approaches), 03_bert.md (fine-tuning approaches).

- **Deng et al., 2009** — J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. "ImageNet: A Large-Scale Hierarchical Image Database." In *CVPR09*, 2009. Cited in 02_related-work.md (transfer learning from supervised data).

- **Dolan and Brockett, 2005** — William B Dolan and Chris Brockett. "Automatically constructing a corpus of sentential paraphrases." In *Proceedings of the Third International Workshop on Paraphrasing (IWP2005)*, 2005. Cited in 01_introduction.md, 08_appendix-b.md (MRPC dataset description).

- **Fedus et al., 2018** — William Fedus, Ian Goodfellow, and Andrew M. Dai. "Maskgan: Better text generation via filling in the_." *arXiv preprint arXiv:1801.07736*, 2018. Cited in 02_related-work.md (feature-based approaches).

- **Hendrycks and Gimpel, 2016** — Dan Hendrycks and Kevin Gimpel. "Gaussian error linear units (GELUs)." *arXiv preprint arXiv:1606.08415*, 2016. Cited in 07_appendix-a.md (gelu activation used instead of relu).

- **Hill et al., 2016** — Felix Hill, Kyunghyun Cho, and Anna Korhonen. "Learning distributed representations of sentences from unlabelled data." In *Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*. Association for Computational Linguistics, 2016. Cited in 02_related-work.md (feature-based approaches).

- **Howard and Ruder, 2018** — Jeremy Howard and Sebastian Ruder. "Universal language model fine-tuning for text classification." In *ACL*. Association for Computational Linguistics, 2018. Cited in 01_introduction.md, 02_related-work.md (fine-tuning approaches), 03_bert.md (fine-tuning approaches).

- **Hu et al., 2018** — Minghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, and Ming Zhou. "Reinforced mnemonic reader for machine reading comprehension." In *IJCAI*, 2018. Cited in 04_experiments.md (SQuAD v1.1 baselines).

- **Jernite et al., 2017** — Yacine Jernite, Samuel R. Bowman, and David Sontag. "Discourse-based objectives for fast unsupervised sentence representation learning." *CoRR*, abs/1705.00557, 2017. Cited in 02_related-work.md, 03_bert.md (related to NSP task).

- **Joshi et al., 2017** — Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. "Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension." In *ACL*, 2017. Cited in 04_experiments.md (SQuAD v1.1, data augmentation).

- **Kiros et al., 2015** — Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. "Skip-thought vectors." In *Advances in neural information processing systems*, pages 3294-3302, 2015. Cited in 02_related-work.md (feature-based approaches).

- **Le and Mikolov, 2014** — Quoc Le and Tomas Mikolov. "Distributed representations of sentences and documents." In *International Conference on Machine Learning*, pages 1188-1196, 2014. Cited in 02_related-work.md (feature-based approaches).

- **Levesque et al., 2011** — Hector J Levesque, Ernest Davis, and Leora Morgenstern. "The winograd schema challenge." In *Aaai spring symposium: Logical formalizations of commonsense reasoning*, volume 46, page 47, 2011. Cited in 08_appendix-b.md (WNLI dataset description).

- **Logeswaran and Lee, 2018** — Lajanugen Logeswaran and Honglak Lee. "An efficient framework for learning sentence representations." In *International Conference on Learning Representations*, 2018. Cited in 02_related-work.md (feature-based approaches), 03_bert.md (related to NSP task).

- **McCann et al., 2017** — Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. "Learned in translation: Contextualized word vectors." In *NIPS*, 2017. Cited in 02_related-work.md (transfer learning from supervised data).

- **Melamud et al., 2016** — Oren Melamud, Jacob Goldberger, and Ido Dagan. "context2vec: Learning generic context embedding with bidirectional LSTM." In *CoNLL*, 2016. Cited in 02_related-work.md, 05_ablation-studies.md (mentioned increasing hidden size from 200 to 600 helped but 1000 did not).

- **Mikolov et al., 2013** — Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. "Distributed representations of words and phrases and their compositionality." In *Advances in Neural Information Processing Systems 26*, pages 3111-3119. Curran Associates, Inc., 2013. Cited in 02_related-work.md (feature-based approaches).

- **Mnih and Hinton, 2009** — Andriy Mnih and Geoffrey E Hinton. "A scalable hierarchical distributed language model." In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, *Advances in Neural Information Processing Systems 21*, pages 1081-1088. Curran Associates, Inc., 2009. Cited in 02_related-work.md (feature-based approaches).

- **Parikh et al., 2016** — Ankur P Parikh, Oscar Tackstrom, Dipanjan Das, and Jakob Uszkoreit. "A decomposable attention model for natural language inference." In *EMNLP*, 2016. Cited in 03_bert.md (fine-tuning BERT, contrasted approach).

- **Pennington et al., 2014** — Jeffrey Pennington, Richard Socher, and Christopher D. Manning. "Glove: Global vectors for word representation." In *Empirical Methods in Natural Language Processing (EMNLP)*, pages 1532-1543, 2014. Cited in 02_related-work.md (feature-based approaches).

- **Peters et al., 2017** — Matthew Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. "Semi-supervised sequence tagging with bidirectional language models." In *ACL*, 2017. Cited in 02_related-work.md.

- **Peters et al., 2018a** — Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. "Deep contextualized word representations." In *NAACL*, 2018. ELMo; feature-based approach using concatenation of left-to-right and right-to-left LM representations. Cited in 01_introduction.md, 02_related-work.md, 03_bert.md, 04_experiments.md, 05_ablation-studies.md.

- **Peters et al., 2018b** — Matthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. "Dissecting contextual word embeddings: Architecture and representation." In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing*, pages 1499-1509, 2018. Cited in 05_ablation-studies.md (mixed results on downstream task impact of increasing bi-LM size).

- **Radford et al., 2018** — Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. "Improving language understanding with unsupervised learning." Technical report, OpenAI, 2018. OpenAI GPT; fine-tuning approach using left-to-right Transformer. Cited in 01_introduction.md, 02_related-work.md, 03_bert.md, 04_experiments.md.

- **Rajpurkar et al., 2016** — Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. "Squad: 100,000+ questions for machine comprehension of text." In *Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing*, pages 2383-2392, 2016. Cited in 01_introduction.md, 02_related-work.md, 04_experiments.md, 08_appendix-b.md (QNLI dataset description).

- **Seo et al., 2017** — Minjoon Seo, Anirudha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. "Bidirectional attention flow for machine comprehension." In *ICLR*, 2017. Cited in 03_bert.md, 04_experiments.md.

- **Socher et al., 2013** — Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. "Recursive deep models for semantic compositionality over a sentiment treebank." In *Proceedings of the 2013 conference on empirical methods in natural language processing*, pages 1631-1642, 2013. Cited in 02_related-work.md, 08_appendix-b.md (SST-2 dataset description).

- **Sun et al., 2018** — Fu Sun, Linyang Li, Xipeng Qiu, and Yang Liu. "U-net: Machine reading comprehension with unanswerable questions." *arXiv preprint arXiv:1810.06638*, 2018. Cited in 04_experiments.md (SQuAD v2.0 published work baseline).

- **Taylor, 1953** — Wilson L. Taylor. "Cloze procedure: A new tool for measuring readability." *Journalism Bulletin*, 30(4):415-433, 1953. Cited in 01_introduction.md, 03_bert.md.

- **Tjong Kim Sang and De Meulder, 2003** — Erik F Tjong Kim Sang and Fien De Meulder. "Introduction to the conll-2003 shared task: Language-independent named entity recognition." In *CoNLL*, 2003. Cited in 01_introduction.md, 02_related-work.md, 05_ablation-studies.md (CoNLL-2003 NER task).

- **Turian et al., 2010** — Joseph Turian, Lev Ratinov, and Yoshua Bengio. "Word representations: A simple and general method for semi-supervised learning." In *Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics*, ACL '10, pages 384-394, 2010. Cited in 02_related-work.md.

- **Vaswani et al., 2017** — Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. "Attention is all you need." In *Advances in Neural Information Processing Systems*, pages 6000-6010, 2017. Cited in 01_introduction.md, 03_bert.md, 05_ablation-studies.md (largest Transformer was L=6, H=1024, A=16 with 100M params).

- **Vincent et al., 2008** — Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. "Extracting and composing robust features with denoising autoencoders." In *Proceedings of the 25th international conference on Machine learning*, pages 1096-1103. ACM, 2008. Cited in 03_bert.md.

- **Wang et al., 2018a** — Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. "Glue: A multi-task benchmark and analysis platform for natural language understanding." In *Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP*, pages 353-355, 2018. Cited in 02_related-work.md, 04_experiments.md, 08_appendix-b.md (GLUE benchmark dataset descriptions).

- **Wang et al., 2018b** — Wei Wang, Ming Yan, and Chen Wu. "Multi-granularity hierarchical attention fusion networks for reading comprehension and question answering." In *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*. Association for Computational Linguistics, 2018. Cited in 04_experiments.md (SQuAD v2.0 published work baseline).

- **Warstadt et al., 2018** — Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. "Neural network acceptability judgments." *arXiv preprint arXiv:1805.12471*, 2018. Cited in 08_appendix-b.md (CoLA dataset description).

- **Williams et al., 2018** — Adina Williams, Nikita Nangia, and Samuel R Bowman. "A broad-coverage challenge corpus for sentence understanding through inference." In *NAACL*, 2018. Cited in 01_introduction.md, 08_appendix-b.md (MNLI dataset description).

- **Wu et al., 2016** — Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. "Google's neural machine translation system: Bridging the gap between human and machine translation." *arXiv preprint arXiv:1609.08144*, 2016. Cited in 03_bert.md (input representation, WordPiece embeddings).

- **Yosinski et al., 2014** — Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. "How transferable are features in deep neural networks?" In *Advances in neural information processing systems*, pages 3320-3328, 2014. Cited in 02_related-work.md (transfer learning importance in computer vision).

- **Yu et al., 2018** — Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. "QANet: Combining local convolution with global self-attention for reading comprehension." In *ICLR*, 2018. Cited in 04_experiments.md (footnote 11, QANet system on SQuAD leaderboard).

- **Zellers et al., 2018** — Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. "Swag: A large-scale adversarial dataset for grounded commonsense inference." In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, 2018. Cited in 04_experiments.md (SWAG task).

- **Zhu et al., 2015** — Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books." In *Proceedings of the IEEE international conference on computer vision*, pages 19-27, 2015. Cited in 03_bert.md (BooksCorpus, 800M words, used for BERT pre-training).
