# Overview

**Title:** Language Models are Unsupervised Multitask Learners

**Authors:** Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever

**Affiliation:** OpenAI, San Francisco, California, United States

**Venue:** OpenAI technical report (preprint)

**Date:** February 2019

## Abstract

> "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations." [p. 1]

## Section headings

- 1. Introduction
- 2. Approach
  - 2.1. Training Dataset
  - 2.2. Input Representation
  - 2.3. Model
- 3. Experiments
  - 3.1. Language Modeling
  - 3.2. Children's Book Test
  - 3.3. LAMBADA
  - 3.4. Winograd Schema Challenge
  - 3.5. Reading Comprehension
  - 3.6. Summarization
  - 3.7. Translation
  - 3.8. Question Answering
- 4. Generalization vs Memorization
- 5. Related Work
- 6. Discussion
- 7. Conclusion
- 8. Appendix A: Samples
  - 8.1. Model capacity
  - 8.2. Text Memorization
  - 8.3. Diversity
  - 8.4. Robustness
