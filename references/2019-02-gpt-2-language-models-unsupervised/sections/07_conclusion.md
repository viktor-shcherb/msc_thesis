# 7. Conclusion [p. 11]

[p. 11] When a large language model is trained on a sufficiently large and diverse dataset it is able to perform well across many domains and datasets. GPT-2 zero-shots to state of the art performance on 7 out of 8 tested language modeling datasets. The diversity of tasks the model is able to perform in a zero-shot setting suggests that high-capacity models trained to maximize the likelihood of a sufficiently varied text corpus begin to learn how to perform a surprising amount of tasks without the need for explicit supervision. [p. 11]

Preliminary code for downloading and using the small model is available at https://github.com/openai/gpt-2 (footnote 5). [p. 11]
