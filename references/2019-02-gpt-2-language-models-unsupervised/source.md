# Language Models are Unsupervised Multitask Learners

**Authors:** Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever
**Affiliation:** OpenAI

## Publication Status

- **Released:** February 14, 2019 (OpenAI technical report)
- **Peer-reviewed:** No
- **Conference/Journal:** N/A (never submitted to a venue)
- **Status:** Technical report (not submitted to peer review)

Note: This paper was released as an OpenAI technical report alongside the GPT-2 model. It has never been posted to arXiv or formally peer-reviewed. The model weights were released in stages: the 124M parameter version in February 2019, with the full 1.5B parameter version following in November 2019.

## Preferred Citation

> Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language Models are Unsupervised Multitask Learners. OpenAI Blog.

## Links

- OpenAI Blog: https://openai.com/index/better-language-models/
- PDF: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
- Code: https://github.com/openai/gpt-2
