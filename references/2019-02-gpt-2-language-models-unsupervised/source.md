# Language Models are Unsupervised Multitask Learners

**Authors:** Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever
**Affiliation:** OpenAI

## Publication Status

- **Preprint:** February 2019
- **Peer-reviewed:** No
- **Status:** Technical report (not submitted to peer review)

Note: This paper was released as an OpenAI technical report alongside the GPT-2 model weights. It has never been formally peer-reviewed but has been highly influential, accumulating over 15,000 citations and establishing the foundation for zero-shot evaluation of language models.

## Preferred Citation

> Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language Models are Unsupervised Multitask Learners. OpenAI Blog.

## Links

- OpenAI Blog: https://openai.com/blog/better-language-models/
- PDF: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
- Code: https://github.com/openai/gpt-2
