# References

Only references that are cited in the section notes are included below.

---

**Ancona et al., 2019**
Marco Ancona, Enea Ceolini, Cengiz Öztireli, and Markus Gross. 2019. *Gradient-Based Attribution Methods*, pages 169-191. Springer International Publishing.
Cited in 05_conclusion.md as a future direction for adjusting attention weights using gradient-based attribution methods.

**Bahdanau et al., 2015**
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In *proceedings of the 2015 International Conference on Learning Representations*.
Cited in 01_introduction.md as foundational attention mechanism work.

**Brunner et al., 2020**
Gino Brunner, Yang Liu, Damian Pascual, Oliver Richter, Massimiliano Ciaramita, and Roger Wattenhofer. 2020. On identifiability in transformers. In *International Conference on Learning Representations*.
Cited in 01_introduction.md for lack of token identifiability in higher layers; cited in 05_conclusion.md as future direction for building attention graph with effective attention weights.

**Chen and Ji, 2019**
Hanjie Chen and Yangfeng Ji. 2019. Improving the interpretability of neural sentiment classifiers via data augmentation. *arXiv preprint arXiv:1909.04225*.
Cited in 01_introduction.md as example of using attention for interpretation.

**Clark et al., 2019**
Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. What does BERT look at? an analysis of BERT's attention. In *Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP*, pages 276-286, Florence, Italy. Association for Computational Linguistics.
Cited in 01_introduction.md as example of using attention for interpretation.

**Coenen et al., 2019**
Andy Coenen, Emily Reif, Ann Yuan, Been Kim, Adam Pearce, Fernanda Viégas, and Martin Wattenberg. 2019. Visualizing and measuring the geometry of bert. *arXiv preprint arXiv:1906.02715*.
Cited in 01_introduction.md as example of using attention for interpretation.

**Cormen et al., 2009**
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2009. *Introduction to Algorithms, Third Edition*, 3rd edition. The MIT Press.
Cited in 03_attention-rollout-and-attention-flow.md for the maximum flow algorithm definition.

**Dehghani et al., 2019**
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. 2019. Universal transformers. In *proceedings of the 2019 International Conference on Learning Representations*.
Cited in 01_introduction.md as example of using attention for interpretation.

**Devlin et al., 2019**
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In *proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*. Association for Computational Linguistics.
Cited in 02_setups-and-problem-statement.md for the CLS token design used in the model.

**Jain and Wallace, 2019**
Sarthak Jain and Byron C. Wallace. 2019. Attention is not Explanation. In *proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pages 3543-3556. Association for Computational Linguistics.
Cited in 01_introduction.md as evidence that equating attention with explanation is wrong.

**Lee et al., 2017**
Jaesong Lee, Joong-Hwi Shin, and Jun-Seok Kim. 2017. Interactive visualization and manipulation of attention-based neural machine translation. In *proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*, pages 121-126.
Cited in 01_introduction.md as example of using attention for interpretation.

**Linzen et al., 2016**
Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing the ability of LSTMs to learn syntax-sensitive dependencies. *Transactions of the Association for Computational Linguistics*, 4:521-535.
Cited in 02_setups-and-problem-statement.md as the source of the subject-verb agreement dataset.

**Pruthi et al., 2019**
Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, and Zachary C Lipton. 2019. Learning to deceive with attention-based explanations. *arXiv preprint arXiv:1909.07913*.
Cited in 01_introduction.md as evidence that equating attention with explanation is wrong.

**Radford et al., 2019**
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. *OpenAI Blog*, 1(8).
Cited in 02_setups-and-problem-statement.md for the GPT-2 Transformer block architecture used in the model.

**Rocktäschel et al., 2016**
Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomas Kocisky, and Phil Blunsom. 2016. Reasoning about entailment with neural attention. In *International Conference on Learning Representations (ICLR)*.
Cited in 01_introduction.md as example of using attention for interpretation.

**Sanh et al., 2019**
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.
Cited in 04_analysis-and-discussion.md as the DistillBERT model used for SST-2 experiments.

**Serrano and Smith, 2019**
Sofia Serrano and Noah A. Smith. 2019. Is attention interpretable? In *proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*. Association for Computational Linguistics.
Cited in 02_setups-and-problem-statement.md for showing that attention weights do not necessarily correspond to relative importance of input tokens.

**Socher et al., 2013**
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In *Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing*, pages 1631-1642, Seattle, Washington, USA. Association for Computational Linguistics.
Cited in 04_analysis-and-discussion.md as the SST-2 sentiment analysis dataset.

**Vaswani et al., 2017**
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In *Advances in neural information processing systems*, pages 5998-6008.
Cited in 01_introduction.md as foundational attention/Transformer work.

**Vashishth et al., 2019**
Shikhar Vashishth, Shyam Upadhyay, Gaurav Singh Tomar, and Manaal Faruqui. 2019. Attention interpretability across nlp tasks. *arXiv preprint arXiv:1909.11218*.
Cited in 01_introduction.md as evidence that attention can offer meaningful interpretations.

**Vig, 2019**
Jesse Vig. 2019. Visualizing attention in transformer-based language models. *arXiv preprint arXiv:1904.02679*.
Cited in 01_introduction.md as evidence that attention can offer meaningful interpretations; cited in 02_setups-and-problem-statement.md for attention visualization approach.

**Wang et al., 2016**
Yequan Wang, Minlie Huang, Li Zhao, et al. 2016. Attention-based lstm for aspect-level sentiment classification. In *proceedings of the 2016 conference on empirical methods in natural language processing*, pages 606-615.
Cited in 01_introduction.md as example of using attention for interpretation.

**Wang et al., 2018**
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In *Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP*, pages 353-355, Brussels, Belgium. Association for Computational Linguistics.
Cited in 04_analysis-and-discussion.md as the GLUE benchmark containing SST-2.

**Wiegreffe and Pinter, 2019**
Sarah Wiegreffe and Yuval Pinter. 2019. Attention is not not explanation. In *proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*. Association for Computational Linguistics.
Cited in 01_introduction.md as evidence that attention can offer meaningful interpretations.

**Wolf et al., 2019**
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delange, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface's transformers: State-of-the-art natural language processing. *ArXiv*, abs/1910.03771.
Cited in 02_setups-and-problem-statement.md for the GPT-2 Transformer block implementation.

**Xu et al., 2015**
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In *proceedings of International Conference on Machine Learning*, pages 2048-2057.
Cited in 01_introduction.md as example of using attention for interpretation.
