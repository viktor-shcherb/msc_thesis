# C Radar Plot and Analysis [p. 17-18]

**Figure 4** (p. 17): "Average scores on 6 major tasks, on English and Chinese datasets, respectively."

Two radar plots (one for English, one for Chinese) show average scores across 6 task categories: Single-Doc QA, Code Completion, Synthetic Task, Few-shot Learning, Summarization, and Multi-Doc QA. Eight models are plotted: GPT-3.5-Turbo-16k, Llama2-7B-chat-4k, LongChat-v1.5-7B-32k, XGen-7B-8k, InternLM-7B-8k, ChatGLM2-6B, ChatGLM2-6B-32k, and Vicuna-v1.5-7B-16k. GPT-3.5-Turbo-16k generally covers the largest area in both plots, especially on Single-Doc QA and Few-shot Learning. The radar shapes differ across English and Chinese, reflecting language-specific strengths.

[p. 18] Among the 6 major tasks, summarization and code completion tend not to be sufficiently discerning. This may be due to the fact that the similarity-based metrics (ROUGE-L, Edit Sim) on these tasks are not sensitive enough to well distinguish between the strong and weak models. Meanwhile, synthetic tasks tend to offer a higher level of discernment, where models either achieve a high score or display a near-zero performance. These findings lead the authors to believe that it may not be a good idea to simply obtain an average over all tasks as the sign of models' long context capability, as used in previous benchmark (Shaham et al., 2023) -- since the performance on the more discerning tasks, such as the synthetic tasks in this benchmark, may dominate the final rank. This necessitates an evaluation strategy like the one used in LongBench that separately assesses each task category, potentially leading to more meaningful benchmarking results.
