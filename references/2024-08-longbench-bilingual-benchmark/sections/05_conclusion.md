# 5 Conclusion [p. 9]

[p. 9] The authors introduce LongBench, a multi-task bilingual benchmark tailored for gauging long context understanding abilities of LLMs. LongBench covers six key categories and a total of 21 tasks, with data lengths extending from thousands of tokens up to tens of thousands of tokens. They also develop LongBench-E which features a more evenly data length distribution. Extensive experiments on LongBench and LongBench-E yield insightful conclusions about the capabilities of current LLMs on long context understanding.

> "Moreover, our analysis suggests that LongBench and LongBench-E serve as ideal testbeds for future research in long context modeling." [p. 9]
