# 1 Introduction [p. 1-3]

[p. 1]

A central problem in sequence modeling is efficiently handling data that contains long-range dependencies (LRDs). Real-world time-series data often requires reasoning over tens of thousands of time steps, while few sequence models address even thousands of time steps. Results from the long-range arena (LRA) benchmark [40] highlight that sequence models today perform poorly on LRD tasks, including one (Path-X) where no model performs better than random guessing.

Since LRDs are the foremost challenge for sequence models, all standard model families -- continuous-time models (CTMs), RNNs, CNNs, and Transformers -- include many specialized variants designed to address them. Modern examples include orthogonal and Lipschitz RNNs [1, 13] to combat vanishing gradients, dilated convolutions to increase context size [3, 28], and an increasingly vast family of efficient Transformers that reduce the quadratic dependence on sequence length [8, 22]. Despite being designed for LRDs, these solutions still perform poorly on challenging benchmarks such as LRA [40] or raw audio classification [18].

An alternative approach to LRDs was recently introduced based on the **state space model (SSM)** (Fig. 1). SSMs are a foundational scientific model used in fields such as control theory, computational neuroscience, and many more, but have not been applicable to deep learning for concrete theoretical reasons. Gu et al. [18] showed that deep SSMs actually struggle even on simple tasks, but can perform exceptionally well when equipped with special state matrices **A** recently derived to solve a problem of continuous-time memorization [16, 45]. Their Linear State Space Layer (LSSL) conceptually unifies the strengths of CTM, RNN and CNN models, and provides a proof of concept that deep SSMs can address LRDs in principle.

[p. 2]

Unfortunately, the LSSL is infeasible to use in practice because of prohibitive computation and memory requirements. For state dimension N and sequence length L, computing the latent state requires O(N^2 L) operations and O(NL) space -- compared to a Omega(L + N) lower bound for both. Thus for reasonably sized models (e.g. N = 256 in Gu et al. [18]), the LSSL uses orders of magnitude more memory than comparably-sized RNNs or CNNs. Although theoretically efficient algorithms for the LSSL were proposed, these are numerically unstable. In particular, the special **A** matrix is highly non-normal in the linear algebraic sense, which prevents the application of conventional algorithmic techniques. Consequently, although the LSSL showed that SSMs have strong performance, they are currently computationally impractical as a general sequence modeling solution.

The authors introduce the **Structured State Space (S4)** sequence model based on the SSM that solves the critical computational bottleneck in previous work. Technically, S4 reparameterizes the structured state matrices **A** appearing in Gu et al. [16], Voelker et al. [45] by decomposing them as the sum of a low-rank and normal term. Additionally, instead of expanding the standard SSM in coefficient space, they compute its truncated generating function in frequency space, which can be simplified into a multipole-like evaluation. Combining these two ideas, the low-rank term can be corrected by the Woodbury identity while the normal term can be diagonalized stably, ultimately reducing to a well-studied and theoretically stable Cauchy kernel [29, 30]. This results in O-tilde(N + L) computation and O(N + L) memory usage, which is essentially tight for sequence models. Compared to the LSSL, S4 is up to 30x faster with 400x less memory usage, while exceeding the LSSL's performance empirically.

Empirically, S4 significantly advances the state-of-the-art for LRD. On the LRA benchmark for efficient sequence models, S4 is as fast as all baselines while outperforming them by 20+ points on average. S4 is the first model to solve the difficult LRA Path-X task (length-16384), achieving **88% accuracy compared to 50% random guessing** for all prior work. On speech classification with length-16000 sequences, S4 halves the test error (1.7%) of specialized Speech CNNs -- by contrast, all RNN and Transformer baselines fail to learn (>= 70% error).

**Towards a general-purpose sequence model.** Beyond LRD, a broad goal of machine learning is to develop a single model that can be used across a wide range of problems. Models today are typically specialized to solve problems from a particular domain (e.g. images, audio, text, time-series), and enable a narrow range of capabilities (e.g. efficient training, fast generation, handling irregularly sampled data). This specialization is typically expressed via domain-specific preprocessing, inductive biases, and architectures. Sequence models provide a general framework for solving many of these problems with reduced specialization -- e.g. Vision Transformers for image classification with less 2D information [12]. However, most models such as Transformers generally still require substantial specialization per task to achieve high performance.

[p. 3]

Deep SSMs in particular have conceptual strengths that suggest they may be promising as a general sequence modeling solution. These strengths include a principled approach to handling LRDs, as well as the ability to move between continuous-time, convolutional, and recurrent model representations, each with distinct capabilities (Fig. 1). The technical contributions enable SSMs to be applied successfully to a varied set of benchmarks with minimal modification:

- **Large-scale generative modeling.** On CIFAR-10 density estimation, S4 is competitive with the best autoregressive models (2.85 bits per dim). On WikiText-103 language modeling, S4 substantially closes the gap to Transformers (within 0.8 perplexity), setting SoTA for attention-free models.
- **Fast autoregressive generation.** Like RNNs, S4 can use its latent state to perform 60x faster pixel/token generation than standard autoregressive models on CIFAR-10 and WikiText-103.
- **Sampling resolution change.** Like specialized CTMs, S4 can adapt to changes in time-series sampling frequency without retraining, e.g. at 0.5x frequency on speech classification.
- **Learning with weaker inductive biases.** With no architectural changes, S4 surpasses Speech CNNs on speech classification, outperforms the specialized Informer model on time-series forecasting problems, and matches a 2-D ResNet on sequential CIFAR with over 90% accuracy.

## Figure

**Figure 1** (p. 2): "(Left) State Space Models (SSM) parameterized by matrices **A**, **B**, **C**, **D** map an input signal u(t) to output y(t) through a latent state x(t). (Center) Recent theory on continuous-time memorization derives special **A** matrices that allow SSMs to capture LRDs mathematically and empirically. (Right) SSMs can be computed either as a recurrence (left) or convolution (right). However, materializing these conceptual views requires utilizing different representations of its parameters (red, blue, green) which are very expensive to compute. S4 introduces a novel parameterization that efficiently swaps between these representations, allowing it to handle a wide range of tasks, be efficient at both training and inference, and excel at long sequences."

The figure shows three panels:
- **Left ("Continuous State Space"):** A block diagram with input u, latent state x, and output y connected through matrix A (self-loop on x). Below: the continuous SSM equations x-dot = **A**x + **B**u, y = **C**x + **D**u.
- **Center ("Long-Range Dependencies"):** A specific HiPPO matrix A shown as a 3x3 example [[1,0,0],[1,2,0],[1,3,3]], with a red dashed curve suggesting special structure.
- **Right ("Fast Discrete Representations"):** Two sub-panels showing the discrete recurrence (x = A-bar x + B-bar u, y = C-bar x + D-bar u) depicted as a sequential chain of states, and the convolution form (y = K-bar * u) depicted as a continuous kernel waveform.
