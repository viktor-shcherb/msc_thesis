# Efficiently Modeling Long Sequences with Structured State Spaces

**Authors:** Albert Gu, Karan Goel, Christopher Ré
**Affiliation:** Stanford University

## Publication Status

- **arXiv preprint:** October 2021, arXiv:2111.00396 (v3 August 2022)
- **Peer-reviewed:** Yes
- **Conference:** International Conference on Learning Representations (ICLR), April 25--29, 2022, Virtual
- **Status:** Published conference paper
- **Recognition:** Outstanding Paper Honorable Mention

## Preferred Citation

Cite the ICLR 2022 version:

> Gu, A., Goel, K., & Ré, C. (2022). Efficiently Modeling Long Sequences with Structured State Spaces. In International Conference on Learning Representations.

## Notes

S4 is the foundational work that made state space models (SSMs) practical for deep learning on long sequences. It directly led to the Mamba architecture (Gu & Dao, 2023), which combined S4's state space formulation with input-dependent selection. The paper has over 2,000 citations as of early 2026 and established SSMs as a viable alternative to Transformers for sequence modeling.

## Links

- arXiv: https://arxiv.org/abs/2111.00396
- OpenReview: https://openreview.net/forum?id=uYLFoz1vlAC
- Code: https://github.com/state-spaces/s4
