# Limitations [p. 9]

This survey summarizes the approaches and evaluation in the area of long context, and gives our views on future development. However, we don't cover efficient transformer on long context, multimodel long context, etc. In addition, due to limitations of space, we are not able to include all related work [p. 9].

Due to the rapidly evolving nature of the field of Transformer context extension, our survey may not capture the latest developments, particularly those that emerged near or after the time of writing [p. 9].
