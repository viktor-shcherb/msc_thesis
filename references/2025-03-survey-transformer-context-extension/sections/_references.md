# References

This file contains bibliographic information for works cited in the section notes.

## A

**Achiam et al., 2023**
- Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.
- Cited in: 09_appendix-b-details-of-evaluation.md (GPT-4 Turbo 128K context)

**Agarwal et al., 2024**
- Rishabh Agarwal, Avi Singh, Lei M Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand, Zaheer Abbas, Azade Nova, John D Co-Reyes, Eric Chu, et al. 2024. Many-shot in-context learning. arXiv preprint arXiv:2404.11018.
- Cited in: 09_appendix-b-details-of-evaluation.md (many-shot learning)

**An et al., 2023**
- Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: Instituting standardized evaluation for long context language models. arXiv preprint arXiv:2307.11088.
- Cited in: 04b_tasks.md (L-Eval benchmark for tasks), 04a_data.md (Table 1)

**An et al., 2024**
- Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, and Lingpeng Kong. 2024. Training-free long context scaling of large language models. arXiv preprint arXiv:2402.17463.
- Cited in: 03a_positional-encoding.md (position index adjustment method)

**Angelidis et al., 2021**
- Stefanos Angelidis, Reinald Kim Amplayo, Yoshihiko Suhara, Xiaolan Wang, and Mirella Lapata. 2021. Extractive opinion summarization in quantized transformer spaces. Transactions of the Association for Computational Linguistics, 9:277–293.
- Cited in: 04b_tasks.md (sentiment aggregation task)

## B

**Bai et al., 2023**
- Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508.
- Cited in: 04a_data.md (Table 1 - LongBench benchmark), 04b_tasks.md (multiple task types, summary source paragraph identification), 04c_metrics.md (algorithmic metrics)

**Bai et al., 2024**
- Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024. Longwriter: Unleashing 10,000-word generation from long context llms. arXiv preprint arXiv:2408.07055.
- Cited in: 04b_tasks.md (open-ended text generation)

**Beltagy et al., 2020**
- Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150.
- Cited in: 04c_metrics.md (perplexity metric)

**Bertsch et al., 2024a**
- Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew Gormley. 2024a. Unlimiformer: Long-range transformers with unlimited length input. Advances in Neural Information Processing Systems, 36.
- Cited in: 03c_retrieval-augmented.md (token-level retrieval, cross attention)

**Bertsch et al., 2024b**
- Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew R Gormley, and Graham Neubig. 2024b. In-context learning with long-context models: An in-depth exploration. arXiv preprint arXiv:2405.00200.
- Cited in: 04b_tasks.md (many-shot learning)

**bloc97, 2023**
- bloc97. 2023. Add NTK-Aware interpolation "by parts" correction. https://github.com/jquesnelle/scaled-rope/pull/1.
- Cited in: 03a_positional-encoding.md (base frequency adjustment)

**Brown, 2020**
- Tom B Brown. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.
- Cited in: 01_introduction.md (Transformer success)

**Bubeck et al., 2023**
- Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.
- Cited in: 04b_tasks.md (code running task)

**Bulatov et al., 2022**
- Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. 2022. Recurrent memory transformer. Advances in Neural Information Processing Systems, 35:11091–11091.
- Cited in: 03b_context-compression.md (soft compression)

## C

**Chen et al., 2021a**
- Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021a. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.
- Cited in: 01_introduction.md (Transformer applications), 04b_tasks.md (code completion)

**Chen et al., 2021b**
- Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2021b. Summscreen: A dataset for abstractive screenplay summarization. arXiv preprint arXiv:2104.07091.
- Cited in: 04b_tasks.md (document summarization, character identification)

**Chen et al., 2023a**
- Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. 2023a. Walking down the memory maze: Beyond context limit through interactive reading. arXiv preprint arXiv:2310.05029.
- Cited in: 03b_context-compression.md (hard compression)

**Chen et al., 2023b**
- Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023b. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595.
- Cited in: 03a_positional-encoding.md (position index scaling)

**Chen et al., 2023c**
- Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023c. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307.
- Cited in: 03d_attention-pattern.md (sparse attention)

**Chen et al., 2024a**
- Qiguang Chen, Libo Qin, Jinhao Wang, Jinxuan Zhou, and Wanxiang Che. 2024a. Unlocking the boundaries of thought: A reasoning granularity framework to quantify and optimize chain-of-thought. Preprint, arXiv:2503.09301.
- Cited in: 04b_tasks.md (long arithmetic calculation)

**Chen et al., 2024b**
- Zhi Chen, Qiguang Chen, Libo Qin, Qipeng Guo, Haijun Lv, Yicheng Zou, Wanxiang Che, Hang Yan, Kai Chen, and Dahua Lin. 2024b. What are the essential factors in crafting effective long context multi-hop instruction datasets? insights and best practices. arXiv preprint arXiv:2409.01893.
- Cited in: 04b_tasks.md (multi-hop question answering)

**Chen et al., 2025**
- Qiguang Chen, Libo Qin, Jinhua Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. 2025. Towards reasoning via a survey on long chain-of-thought for reasoning large language models. Preprint, arXiv:2503.09301.
- Cited in: 01_introduction.md (excluded topic: long chain-of-thought)

**Chevalier et al., 2023**
- Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. 2023. Adapting language models to compress contexts. arXiv preprint arXiv:2305.14788.
- Cited in: 03b_context-compression.md (soft compression)

**Chi et al., 2022a**
- Ta-Chung Chi, Ting-Han Fan, Peter J Ramadge, and Alexander Rudnicky. 2022a. Kerple: Kernelized relative positional embedding for length extrapolation. Advances in Neural Information Processing Systems, 35:8386–8399.
- Cited in: 03a_positional-encoding.md (learnable attention bias)

**Chi et al., 2022b**
- Ta-Chung Chi, Ting-Han Fan, Alexander I Rudnicky, and Peter J Ramadge. 2022b. Dissecting transformer length extrapolation via the lens of receptive field analysis. arXiv preprint arXiv:2212.10356.
- Cited in: 03a_positional-encoding.md (predefined attention bias)

**Chiang et al., 2023**
- Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language models be an alternative to human evaluations? arXiv preprint arXiv:2305.01937.
- Cited in: 04c_metrics.md (LLM-based metrics)

**Cobbe et al., 2021**
- Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.
- Cited in: 01_introduction.md (Transformer success), 04b_tasks.md (long arithmetic calculation)

## D

**Dai et al., 2019**
- Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978–2988.
- Cited in: 03d_attention-pattern.md (sliding window)

**Dey et al., 2022**
- Suvodip Dey, Maunendra Sankar Desarkar, Asif Ekbal, and PS Brijith. 2022. Dialogue generation on intra-sentence topic augmentation and Knowledge Graph enhanced Transformer for multi-domain multi-turn dialogue. arXiv preprint arXiv:2210.06282.
- Cited in: 01_introduction.md (dialogue systems with long contexts)

**Devlin, 2018**
- Jacob Devlin. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
- Cited in: 01_introduction.md (Transformer success), 04c_metrics.md (BERTScore)

**Ding et al., 2023**
- Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. 2023. Longnet: Scaling transformers to 1,000,000,000 tokens. arXiv preprint arXiv:2307.02486.
- Cited in: 03d_attention-pattern.md (sparse attention)

**Dong et al., 2023a**
- Zican Dong, Tianyi Tang, Lunyi Li, and Ji-Rong Wen. 2023a. Bamboo: A comprehensive benchmark for evaluating long text modeling capacities of large language models. arXiv preprint arXiv:2309.13345.
- Cited in: 04a_data.md (Table 1 - BAMBOO benchmark), 04b_tasks.md (code completion, reordering, context consistency, character identification), 04c_metrics.md (algorithmic metrics improvement), 09_appendix-b-details-of-evaluation.md (multiple tasks)

**Dong et al., 2023b**
- Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. 2023b. A survey on long text modeling with transformers. arXiv preprint arXiv:2302.14502.
- Cited in: 01_introduction.md (prior survey, general topics)

## F

**Fabbri et al., 2019**
- Alexander R Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir R Radev. 2019. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. arXiv preprint arXiv:1906.01749.
- Cited in: 04b_tasks.md (multi-document summarization)

**Fan et al., 2024**
- Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. A survey on rag meeting llms: Towards retrieval-augmented large language models. Preprint, arXiv:2405.06211.
- Cited in: 01_introduction.md (excluded topic: RAG)

**Fu et al., 2023**
- Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166.
- Cited in: 09_appendix-b-details-of-evaluation.md (GPTScore for text evaluation)

## G

**Ge et al., 2023**
- Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. 2023. In-context autoencoder for context compression in a large language model. arXiv preprint arXiv:2307.06945.
- Cited in: 03b_context-compression.md (soft compression)

**Golchin and Surdeanu, 2023**
- Shahriar Golchin and Mihai Surdeanu. 2023. Time travel in llms: Tracing data contamination in large language models. arXiv preprint arXiv:2308.08493.
- Cited in: 09_appendix-b-details-of-evaluation.md (knowledge leakage issue)

**Guo et al., 2023**
- Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. 2023. Longcoder: A long-range pre-trained language model for code completion. In International Conference on Machine Learning, pages 12093–12106. PMLR.
- Cited in: 04b_tasks.md (code completion)

## H

**Han et al., 2024**
- Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. 2024. Lminfinite: Zero-shot extreme length generalization for large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technology (Volume 1: Long Papers), pages 3991–4008.
- Cited in: 01_introduction.md (OOD problem), 02_challenges.md (OOD factors), 03d_attention-pattern.md (sliding window)

**Hao et al., 2022**
- Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. 2022. Structured prompting: Scaling in-context learning to 1,000 examples. arXiv preprint arXiv:2212.06713.
- Cited in: 03d_attention-pattern.md (parallel context)

**Ho et al., 2020**
- Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060.
- Cited in: 04b_tasks.md (multi-hop question answering)

**Huang et al., 2021**
- Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021. Efficient attentions for long document summarization. arXiv preprint arXiv:2104.02112.
- Cited in: 04b_tasks.md (document summarization)

**Huang et al., 2023**
- Yunpeng Huang, Jingwei Xu, Junyu Lai, Zixu Jiang, Taolue Chen, Zenan Li, Yuan Yao, Xiaoxing Ma, Lijuan Yang, Hao Chen, et al. 2023. Advancing transformer architecture in long-context large language models: A comprehensive survey. arXiv preprint arXiv:2311.12351.
- Cited in: 01_introduction.md (prior survey with unclear taxonomy)

## J

**Jiang et al., 2023**
- Huiqiang Jiang, Qianhu Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. Llmlingua: Compressing prompts for accelerated inference of large language models. arXiv preprint arXiv:2310.06839.
- Cited in: 03b_context-compression.md (hard compression)

**Jiang et al., 2024b**
- Huiqiang Jiang, Qianhu Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2024b. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1658–1676. Association for Computational Linguistics.
- Cited in: 03b_context-compression.md (hard compression)

**Joshi et al., 2017**
- Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551.
- Cited in: 04b_tasks.md (single-hop question answering)

## K

**Kasai et al., 2021**
- Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Lavinia Dunagan, Jacob Morrison, Alexander R Fabbri, Yejin Choi, and Noah A Smith. 2021. Bidimensional leaderboards: Generate and evaluate language hand in hand. arXiv preprint arXiv:2112.04139.
- Cited in: 04c_metrics.md (algorithmic metrics)

**Kazemnejad et al., 2024**
- Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. 2024. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36.
- Cited in: 03a_positional-encoding.md (positional encoding as major factor)

**Kočiský et al., 2018**
- Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2018. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317–328.
- Cited in: 04b_tasks.md (single-hop question answering)

**Kryściński et al., 2021**
- Wojciech Kryściński, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. 2021. Booksum: A collection of datasets for long-form narrative summarization. arXiv preprint arXiv:2105.08209.
- Cited in: 04b_tasks.md (reordering task)

**Kumar et al., 2024**
- Ishita Kumar, Snigdha Viswanathan, Sushrita Yerra, Alireza Salemi, Ryan A Rossi, Franck Dernoncourt, Hanieh Deilamsalehy, Xiang Chen, Ruiyi Zhang, Shubham Agarwal, et al. 2024. Longpersonalizer: A benchmark for personalized long-form text generation. arXiv preprint arXiv:2407.11016.
- Cited in: 04b_tasks.md (open-ended text generation)

## L

**Lewis, 2019**
- M Lewis. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.
- Cited in: 01_introduction.md (Transformer success), 04c_metrics.md (BART)

**Li and Roth, 2002**
- Xin Li and Dan Roth. 2002. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics.
- Cited in: 04b_tasks.md (long example learning)

**Li et al., 2023a**
- Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023a. Loogle: Can long-context language models understand long contexts? arXiv preprint arXiv:2311.04939.
- Cited in: 04a_data.md (Table 1 - LooGLE benchmark), 04b_tasks.md (numerical information extraction, reordering), 04c_metrics.md (LLM-based metrics)

**Li et al., 2023b**
- Xianming Li, Zongxi Li, Xiaotian Luo, Haoran Xie, Xing Lee, Yingbin Zhao, Xin Wang, and Qing Li. 2023b. Recurrent attention networks for long-text modeling. arXiv preprint arXiv:2306.06843.
- Cited in: 03b_context-compression.md (soft compression)

**Li et al., 2024a**
- Jia-Nan Li, Quan Tu, Cunli Mao, Zhengtao Yu, Ji-Rong Wen, and Rui Yan. 2024a. Streamingdialogue: Prolonged dialogue learning via long context compression with minimal losses. arXiv preprint arXiv:2403.08312.
- Cited in: 01_introduction.md (dialogue systems)

**Li et al., 2024b**
- Wu Li, Shumin Deng, Kaiyan Zhang, Yunxi Liu, and Kai Chen. 2024b. Needlebench: Can llms do retrieval and reasoning in 1 million context window? arXiv preprint arXiv:2407.11963.
- Cited in: 04a_data.md (Table 1 - NeedleBench), 04b_tasks.md (needle-in-a-haystack), 04c_metrics.md (algorithmic metrics improvement)

**Li et al., 2024c**
- Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. 2024c. Long-context llms struggle with long in-context learning. arXiv preprint arXiv:2404.02060.
- Cited in: 01_introduction.md (content-rich in-context learning), 04a_data.md (Table 1 - LongICLBench), 04b_tasks.md (long example learning)

**Li et al., 2025**
- Haoyang Li, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchan Xu, Xuejia Chen, Nicole Hu, Wei Dong, Qing Li, and Lei Chen. 2025. A survey on large language model acceleration based on kv cache management. Preprint, arXiv:2412.19442.
- Cited in: 01_introduction.md (prior survey on KV cache)

**Lin, 2004**
- Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74–81.
- Cited in: 04c_metrics.md (ROUGE metric)

**Liu et al., 2023a**
- Shuaiqi Liu, Jiannong Cao, Zhongfen Deng, Wenting Zhao, Ruosong Yang, Zhiyuan Wen, and S Yu Philip. 2023a. Neural abstractive summarization for long text and multiple tables. IEEE Transactions on Knowledge and Data Engineering.
- Cited in: 01_introduction.md (book/repo-level tasks)

**Liu et al., 2023b**
- Tianyang Liu, Canwen Xu, and Julian McAuley. 2023b. Repobench: Benchmarking repository-level code auto-completion systems. arXiv preprint arXiv:2306.03091.
- Cited in: 01_introduction.md (repo-level tasks)

**Liu et al., 2023c**
- Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023c. G-eval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634.
- Cited in: 04c_metrics.md (LLM-based metrics), 09_appendix-b-details-of-evaluation.md (G-EVAL)

**Liu et al., 2024b**
- Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, and Dahua Lin. 2024b. Scaling laws of rope-based extrapolation. Preprint, arXiv:2310.05209.
- Cited in: 08_appendix-a-details-of-approaches.md (RoPE scaling laws)

**Liu et al., 2024a**
- Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024a. Lost in the middle: How language models use long context. Transactions of the Association for Computational Linguistics, 12:157–173.
- Cited in: 01_introduction.md ("Lost in the Middle" phenomenon), 02_challenges.md ("Lost in the Middle" phenomenon)

**Luo et al., 2023**
- Zhecheng Luo, Qianqian Xie, and Sophia Ananiadou. 2023. Chatgpt as a factual inconsistency evaluator for text summarization. arXiv preprint arXiv:2303.15621.
- Cited in: 04c_metrics.md (LLM-based metrics)

## M

**Mohtashami and Jaggi, 2023**
- Amirkeivan Mohtashami and Martin Jaggi. 2023. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300.
- Cited in: 04b_tasks.md (needle-in-a-haystack)

**Mohtashami and Jaggi, 2024**
- Amirkeivan Mohtashami and Martin Jaggi. 2024. Random-access infinite context length for transformers. Advances in Neural Information Processing Systems, 36.
- Cited in: 03c_retrieval-augmented.md (landmark attention)

**Mu et al., 2024a**
- Honglin Mu, Yang Xu, Yunlong Feng, Xiaofeng Han, Yutong Li, Yuan Hou, and Wanxiang Che. 2024a. Beyond static evaluation: A dynamic approach to assessing ai assistants' api invocation capabilities. Preprint, arXiv:2403.11128.
- Cited in: 04b_tasks.md (API invocation)

**Mu et al., 2024b**
- Jesse Mu, Xiang Li, and Noah Goodman. 2024b. Learning to compress prompts with gist tokens. Advances in Neural Information Processing Systems, 36.
- Cited in: 03b_context-compression.md (soft compression, gisting)

## N

**Ni et al., 2024**
- Xuanfan Ni, Hengyi Cai, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, and Piji Li. 2024. A benchmark for extremely long context understanding with long-range dependencies. arXiv preprint arXiv:2404.05446.
- Cited in: 04a_data.md (Table 1 - benchmark with long-range dependencies)

**NLPCC, 2014**
- NLPCC. 2014. Task definition for large scale text categorization at nlpcc 2014.
- Cited in: 04b_tasks.md (text categorization)

## P

**Papineni et al., 2002**
- Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311–318.
- Cited in: 04c_metrics.md (BLEU metric)

**Pawar et al., 2024**
- Saurav Pawar, SM Touhidul Islam, SM Zaman, Vinija Jain, Aman Chadha, and Amitava Das. 2024. A survey on context length extension: why, and how of context length extension techniques in large language models–a detailed survey. arXiv preprint arXiv:2401.07872.
- Cited in: 01_introduction.md (prior survey on context extension)

**Peng and Quesnelle, 2023**
- Bowen Peng and Jeffrey Quesnelle. 2023. Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation. arXiv preprint arXiv:2306.15421.
- Cited in: 03a_positional-encoding.md (base frequency adjustment, NTK-aware RoPE)

**Peng et al., 2023**
- Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071.
- Cited in: 03a_positional-encoding.md (base frequency adjustment, YaRN)

**Press et al., 2021**
- Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409.
- Cited in: 03a_positional-encoding.md (predefined attention bias), 04c_metrics.md (perplexity)

## Q

**Qin et al., 2024**
- Libo Qin, Qiguang Chen, Xiachong Feng, Yang Wu, Yongheng Zhang, Yinghui Li, Min Li, Wanxiang Che, and Philip S Yu. 2024. Large language models meet nlp: A survey. arXiv preprint arXiv:2305.12819.
- Cited in: 01_introduction.md (survey on LLMs and NLP)

**Qiu et al., 2024**
- Han Qiu, Jiaxing Huang, Peng Gao, Qin Qi, Xixiang Zhang, Ling Shao, and Shijian Lu. 2024. Longhalaqa: Long-context hallucination evaluation for multimodal large language models. Preprint, arXiv:2410.09962.
- Cited in: 04b_tasks.md (hallucination evaluation)

## R

**Radford, 2018**
- Alec Radford. 2018. Improving language understanding by generative pre-training.
- Cited in: 01_introduction.md (Transformer success, GPT)

**Rafailov et al., 2024**
- Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36.
- Cited in: 01_introduction.md (preference optimization)

**Raffel et al., 2020**
- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1–67.
- Cited in: 03a_positional-encoding.md (T5, learnable attention bias)

**Rajpurkar et al., 2016**
- P Rajpurkar. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250.
- Cited in: 04b_tasks.md (question answering dataset)

**Ratner et al., 2022**
- Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2022. Parallel context windows for large language models. arXiv preprint arXiv:2212.10947.
- Cited in: 03d_attention-pattern.md (parallel context windows)

**Reiter and Belz, 2009**
- Ehud Reiter and Anja Belz. 2009. An investigation into the validity of some metrics for automatically evaluating natural language generation systems. Computational Linguistics, 35(4):529–558.
- Cited in: 04c_metrics.md (NLG evaluation metrics)

**Roy et al., 2021**
- Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2021. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53–68.
- Cited in: 03d_attention-pattern.md (sparse attention, routing)

**Roziere et al., 2023**
- Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950.
- Cited in: 03a_positional-encoding.md (base frequency adjustment)

**Rubin and Berant, 2023**
- Ohad Rubin and Jonathan Berant. 2023. Long-range language modeling with self-retrieval. arXiv preprint arXiv:2306.13421.
- Cited in: 03c_retrieval-augmented.md (self-retrieval)

## S

**Senedd Cymru, 2024**
- Senedd Cymru. 2024. Record of proceedings - senedd. https://record.assembly.wales/. Accessed: 2024-10-15.
- Cited in: 04b_tasks.md (character identification), 09_appendix-b-details-of-evaluation.md (character identification)

**Shaham et al., 2022**
- Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, et al. 2022. SCROLLS: Standardized CompaRison Over Long Language Sequences. arXiv preprint arXiv:2201.03533.
- Cited in: 04a_data.md (Table 1 - SCROLLS benchmark)

**Shaham et al., 2023**
- Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. 2023. ZeroSCROLLS: A zero-shot benchmark for long text understanding. arXiv preprint arXiv:2210.16150.
- Cited in: 04a_data.md (Table 1 - ZeroSCROLLS benchmark), 04b_tasks.md (sentiment aggregation, reordering), 04c_metrics.md (algorithmic metrics)

**Sharma et al., 2019**
- Eva Sharma, Chen Li, and Lu Wang. 2019. Bigpatent: A large-scale dataset for abstractive and coherent summarization. arXiv preprint arXiv:1906.03741.
- Cited in: 01_introduction.md (book/repo-level tasks)

**Shen et al., 2023**
- Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, and Lidong Bing. 2023. Large language models are not yet human-level evaluators for abstractive summarization. arXiv preprint arXiv:2305.13091.
- Cited in: 04c_metrics.md (LLM-based metrics limitations)

**Shi et al., 2024**
- Luohe Shi, Hongyi Zhang, Yao Yao, Zuchao Li, and Hai Zhao. 2024. Keep the cost down: A review on methods to optimize llm's kv-cache consumption. Preprint, arXiv:2407.18003.
- Cited in: 01_introduction.md (KV cache optimization)

**Song et al., 2024**
- Dingjie Song, Shunian Chen, Guiming Hardy Chen, Fei Yu, Xiang Wan, and Benyou Wang. 2024. Milebench: Benchmarking mlms in long context. arXiv preprint arXiv:2404.18532.
- Cited in: 04a_data.md (MileBench benchmark)

**Stent et al., 2005**
- Amanda Stent, Matthew Marge, and Mohit Singhai. 2005. Evaluating evaluation methods for generation in the presence of variation. In International conference on intelligent text processing and computational linguistics, pages 341–351. Springer.
- Cited in: 04c_metrics.md (evaluation methodology)

**Su, 2023**
- Jianlin Su. 2023. Rectified rotary position embeddings. https://github.com/bojone/rerope.
- Cited in: 03a_positional-encoding.md (ReRoPE, position index adjustment)

**Su et al., 2024**
- Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, pages 127063.
- Cited in: 03a_positional-encoding.md (RoPE variants)

**Sun et al., 2021**
- Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. 2021. Do long-range language models actually use long-range context? arXiv preprint arXiv:2109.09115.
- Cited in: 02_challenges.md (long-range context usage)

**Sun et al., 2022**
- Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. 2022. A length-extrapolatable transformer. arXiv preprint arXiv:2212.10554.
- Cited in: 03a_positional-encoding.md (XPOS, structure modification)

## T

**Tan et al., 2024**
- Haochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu, Zhili Liu, Xiaoguang Li, Yasheng Wang, Lifeng Shang, Qun Liu, and Linqi Song. 2024. Proxyu: An alternative framework for evaluating long-form text generation with large language models. arXiv preprint arXiv:2401.15042.
- Cited in: 04c_metrics.md (LLM-based metrics for long-form generation)

**Tancik et al., 2020**
- Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. 2020. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in neural information processing systems, 33:7537–7547.
- Cited in: 03a_positional-encoding.md (Fourier features)

**Touvron et al., 2023**
- Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.
- Cited in: 01_introduction.md (Llama models)

**Trivedi et al., 2022**
- Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539–554.
- Cited in: 04b_tasks.md (multi-hop question answering)

**TVMEG, 2024**
- TVMEG. 2024. Tvmeg - tv mega engine. https://tvmeg.com/. Accessed: 2024-10-15.
- Cited in: 04b_tasks.md (character identification), 09_appendix-b-details-of-evaluation.md (character identification)

**Tworkowski et al., 2024**
- Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Miloś. 2024. Focused transformer: Contrastive training for context scaling. Advances in Neural Information Processing Systems, 36.
- Cited in: 03c_retrieval-augmented.md (token-level retrieval, contrastive training)

## V

**Vaswani, 2017**
- A Vaswani. 2017. Attention is all you need. Advances in Neural Information Processing Systems.
- Cited in: 01_introduction.md (original Transformer), 02_challenges.md (quadratic complexity)

**Vaswani et al., 2023**
- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention is all you need. Preprint, arXiv:1706.03762.
- Cited in: 01_introduction.md (Transformer architecture), 02_challenges.md (attention mechanism)

## W

**Wang and Komatsuzaki, 2021**
- Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax.
- Cited in: 01_introduction.md (open-source LLM)

**Wang et al., 2022**
- Alex Wang, Richard Yuanzhe Pang, Angelica Chen, Jason Phang, and Samuel R Bowman. 2022. Squality: Building a long-document summarization dataset the hard way. arXiv preprint arXiv:2205.11465.
- Cited in: 04b_tasks.md (document summarization)

**Wang et al., 2023a**
- Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023a. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926.
- Cited in: 04c_metrics.md (LLM-based metrics agreement with humans)

**Wang et al., 2023b**
- Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, and Zhifang Sui. 2023b. Making language models better reasoners with step-aware verifier. arXiv preprint arXiv:2206.02336.
- Cited in: 04c_metrics.md (interpretable reasoning)

**Wang et al., 2024a**
- Yiran Wang, Wenyue Hua, Rui Hu, and Yongfeng Zhang. 2024a. Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks. arXiv preprint arXiv:2404.06480.
- Cited in: 04a_data.md (Table 1 - Ada-LEval), 04b_tasks.md (reordering)

**Wang et al., 2024b**
- Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. 2024b. Augmenting language models with long-term memory. Advances in Neural Information Processing Systems, 36.
- Cited in: 03b_context-compression.md (long-term memory)

**Wang et al., 2024c**
- Xiao Wang, Shiao Wang, Yuhe Ding, Yuehang Li, Wentao Wu, Yao Rong, Weizhe Kong, Ju Huang, Shihao Li, Haoxiang Yang, Ziwen Wang, Bo Jiang, Chenglong Li, Yaowei Wang, Yonghong Tian, and Jin Tang. 2024c. State space model for new-generation network alternative to transformers: A survey. Preprint, arXiv:2404.09516.
- Cited in: 01_introduction.md (state space models survey)

**Wu et al., 2022**
- Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. 2022. Memorizing transformers. arXiv preprint arXiv:2203.08913.
- Cited in: 03c_retrieval-augmented.md (token-level retrieval, memorizing)

**Wu et al., 2023**
- Ning Wu, Ming Gong, Linjun Shou, Shining Liang, and Daxin Jiang. 2023. Large language models are diverse role-players for summarization evaluation. In CCF International Conference on Natural Language Processing and Chinese Computing, pages 683–702. Springer.
- Cited in: 04c_metrics.md (LLM-based evaluation)

## X

**Xiao et al., 2023**
- Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453.
- Cited in: 03d_attention-pattern.md (sliding window)

**Xiao et al., 2024**
- Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, and Maosong Sun. 2024. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory. arXiv preprint arXiv:2402.04617.
- Cited in: 03d_attention-pattern.md (training-free long context)

**Xu et al., 2024**
- Xin Xu, Tong Xiao, Zitong Chao, Zhenya Huang, Can Yang, and Yang Wang. 2024. Are longer prompts always better? Prompt selection for math word problems better? arXiv preprint arXiv:2405.14804.
- Cited in: 04b_tasks.md (prompt selection, math problems)

## Y

**Yang et al., 2018**
- Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600.
- Cited in: 04b_tasks.md (multi-hop question answering)

**Yu et al., 2020**
- Dian Yu, Kai Sun, Claire Cardie, and Dong Yu. 2020. Dialogue-based relation extraction. arXiv preprint arXiv:2004.08056.
- Cited in: 04b_tasks.md (dialogue-based tasks)

**Yu et al., 2023**
- Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu. 2024. God knows: A survey on retrieval-augmented generation: A survey. Preprint, arXiv:2405.06437.
- Cited in: 01_introduction.md (RAG survey)

**Yu et al., 2024**
- Lili Yu, Dániel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. 2023. Megabyte: Predicting million-byte sequences with multiscale transformers. Advances in Neural Information Processing Systems, 36:78808–78824.
- Cited in: 01_introduction.md (multiscale transformers)

**Yuan et al., 2021**
- Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. Advances in Neural Information Processing Systems, 34:27263–27277.
- Cited in: 04c_metrics.md (BARTScore metric)

**Yuan et al., 2024**
- Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024. LV-Eval: A balanced long-context benchmark with 5 length levels up to 256K. arXiv preprint arXiv:2402.05136.
- Cited in: 04a_data.md (Table 1 - LV-Eval), 04c_metrics.md (algorithmic metrics improvement)

## Z

**Zan et al., 2022**
- Daoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji Wang, and Jian-Guang Lou. 2022. When language model meets private library. arXiv preprint arXiv:2210.17236.
- Cited in: 04b_tasks.md (code generation with private libraries)

**Zhang et al., 2020**
- Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675.
- Cited in: 04c_metrics.md (BERTScore metric)

**Zhang et al., 2023a**
- Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. 2023a. Repocoder: Repository-level code completion through iterative retrieval and generation. arXiv preprint arXiv:2303.12570.
- Cited in: 04b_tasks.md (repository-level code completion)

**Zhang et al., 2024**
- Junfan Zhang, Yue Wu, Shuailin Li, Xingwei Qu, Weilin Zhao, Yuhang Zhou, Yongfeng Huang, and Yuanqing Li. 2024. ∞Bench: Extending long context evaluation beyond 100K tokens. arXiv preprint arXiv:2402.13718.
- Cited in: 04a_data.md (Table 1 - ∞Bench), 04b_tasks.md (multiple task types, character identification), 04c_metrics.md (LLM-based metrics)

**Zhao et al., 2023**
- Liang Zhao, Xiaocheng Feng, Xiachong Feng, Bin Qin, and Ting Liu. 2023. Length extrapolation of transformers: a survey from the perspective of position encoding. arXiv preprint arXiv:2312.17044.
- Cited in: 01_introduction.md (position encoding survey)

**Zhao et al., 2024**
- Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, and Bin Cui. 2024. Retrieval-augmented generation for ai-generated content: A survey. Preprint, arXiv:2402.19473.
- Cited in: 01_introduction.md (RAG for AIGC survey)

**Zheng et al., 2023**
- Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, et al. 2023. Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 5673–5684.
- Cited in: 04b_tasks.md (code generation)

**Zheng et al., 2024**
- Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36.
- Cited in: 04c_metrics.md (LLM-as-judge evaluation)

**Zhong et al., 2021**
- Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, et al. 2021. Qmsum: A new benchmark for query-based multi-domain meeting summarization. arXiv preprint arXiv:2104.05938.
- Cited in: 04b_tasks.md (query-based summarization)

**Zhu et al., 2024**
- Dawei Zhu, Liang Wang, Nan Yang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. 2024. Longembed: Extending embedding models for long context retrieval. arXiv preprint arXiv:2404.12096.
- Cited in: 04b_tasks.md (needle-in-a-haystack)

**Zhou et al., 2024**
- Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, Shengen Yan, Guohao Dai, Xiao-Ping Zhang, Yuhan Dong, and Yu Wang. 2024. A survey on efficient inference for large language models. arXiv preprint arXiv:2404.14294.
- Cited in: 01_introduction.md (efficient inference survey)

**Zhuang et al., 2023**
- Ziyu Zhuang, Qiguang Chen, Longxuan Ma, Mingda Li, Yi Han, Yushan Qian, Haopeng Bai, Zixian Feng, Weinan Zhang, and Ting Liu. 2023. Through the lens of core competency: Survey on evaluation of large language models. ArXiv, abs/2308.07902.
- Cited in: 01_introduction.md (LLM evaluation survey), 04b_tasks.md (multi-hop question answering)
