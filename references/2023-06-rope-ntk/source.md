# NTK-Aware Scaled RoPE

**Author:** bloc97 (Bowen Peng)

## Publication Status

- **Published:** June 2023, Reddit post on r/LocalLLaMA
- **Peer-reviewed:** No
- **Conference/Journal:** None. This is a community contribution (Reddit post), not a formal paper. There is no arXiv PDF.
- **Status:** Informal community contribution

The method was later formalized in the YaRN paper (Peng et al., ICLR 2024) where bloc97 is the first author. For formal citation purposes, citing YaRN (which includes NTK-aware as Definition 1) is preferable, though many papers cite the Reddit post directly.

## Preferred Citation

If citing the original contribution directly:

> bloc97 (2023). NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation. Reddit post, r/LocalLLaMA.

If citing the formalized version, cite YaRN (Peng et al., ICLR 2024).

## Related Contributions by Same Author

- **NTK-by-parts interpolation** (July 2023): https://github.com/jquesnelle/scaled-rope/pull/1
- **Dynamic NTK** by emozilla/Jeffrey Quesnelle (June 2023): Reddit post, r/LocalLLaMA

## Links

- Reddit post: https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/
