@misc{2023-06-rope-ntk,
  author = {bloc97},
  title = {{NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation}},
  year = {2023},
  month = jun,
  howpublished = {Reddit post, r/LocalLLaMA},
  url = {https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/},
  keywords = {positional-encoding, context-extension, rope, informal},
}
