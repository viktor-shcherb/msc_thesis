# 1 Introduction [p. 1–2]

## Context Expansion in LLMs [p. 1]

The increase in context length for large language models (LLMs; OpenAI 2023; Anthropic 2023; Bai et al., 2023; Xiong et al., 2023; Llama Team 2024) has facilitated the development of a wide range of applications (Fang et al., 2022; Bain et al., 2023), substantially expanding the capabilities of AI systems. Recent advancements in efficient training and attention calculation (Li et al., 2024a; Dao, 2023; Liu et al., 2023) have made it feasible to train LLMs with exceptionally long context windows. For instance, Llama3.1 (Llama Team, 2024) features a context length of 128K tokens, which is 64× longer than that of its initial release (Touvron et al., 2023a).

## The Long-Context Challenge [p. 1]

This trend towards longer context lengths in LLMs promises enhanced capabilities. Previous work has primarily focused on extending the context of LLMs, with significant efforts devoted to improving data engineering techniques (Fu et al., 2024b; Hu et al., 2024; Bai et al., 2024; Zhao et al., 2024). High-quality natural long-context data are scarce in real-world settings, limiting the availability of such data for training purposes. To address this challenge, recent methods aim to generate synthetic training data that better capture the nuances of naturally occurring long-context information, despite inherent challenges such as time consumption in continual training and potential biases (Zhao et al., 2024; An et al., 2024b; Lv et al., 2024). Researchers have also focused on addressing specific architectural limitations. Efforts have been made to address the incorrect extrapolation of the base frequency in Rotary Position Embedding (RoPE) (Su et al., 2022; Peng et al., 2023; Chen et al., 2023; Lin et al., 2024b; Chen et al., 2024).

## The Effective Length Gap [p. 1–2]

However, recent studies (An et al., 2023; Zhang et al., 2024d; Li et al., 2024b; Wang et al., 2024a) reveal a notable discrepancy between these theoretical improvements and observed performance. In practice, the effective context utilization of these models often falls substantially below their claimed or training context lengths. For example, on the widely used RULER benchmark (Hsieh et al., 2024), the effective context length of the latest Llama 3.1 70B model is only 64K, despite employing scaled RoPE base frequency (Peng et al., 2023) and having sufficient training data (Llama Team, 2024). In fact, most open-source models demonstrate effective context lengths less than 50% of their training length (Hsieh et al., 2024). A key research question emerges from these observations: *Why does the effective context length of LLMs fall short of their training context lengths?*

## Core Insight: Left-Skewed Position Frequency [p. 2]

In this study, instead of further extending the context window size of current LLMs, we take a fresh perspective to understand and address this gap. Our core insight revolves around a phenomenon we term the *left-skewed position frequency distribution* — a pattern of severe undertraining of long-distance position indices during pretraining and post-training stages. This skewed distribution significantly contributes to the model's limited long-context modeling tasks. In SlimPajama-627B (Cerebras, 2023), a widely used pretraining corpus (Geng & Liu, 2023; Zhang et al., 2024b), we clearly observe this left-skewed phenomenon. As illustrated in Figure 1a, even with presumably adequate long-sequence data, the frequency of position indices decreases dramatically as distances increase. For instance, when training with a context length of 2048 tokens on SlimPajama, the frequency of position indices used to model relationships between distant tokens (distances > 1024) is less than 20%, and for even longer distances (> 1536), it drops below 5%.

Probing experiments conducted during pretraining reveal that the frequency of exposure to specific position indices has a crucial impact on the training context utilization. Capturing long-range dependencies is inherently more challenging (Zhu et al., 2024; Wu et al., 2024), and this challenge is exacerbated when the frequency of position indices allocated to gather distant information is exceedingly low, as observed in Figure 1. In other words, the difficulty in modeling long-range dependencies, coupled with the undertraining of the positions responsible for them, provides a compelling explanation for the discrepancy between the theoretical and practical context lengths of LLMs.

## Proposed Solution: STRING [p. 2]

Building on these findings, we investigate whether well-trained positions can be leveraged to capture information from distant inputs during inference. To address this, we propose a training-free approach called ShifTed Rotray position embeddING (STRING). This method eschews the use of positions at the tail of the frequency distribution during inference. Specifically, STRING shifts position indices from the main diagonal of the position matrix to its bottom-left corner. This adjustment enables the model to represent long-range dependencies using frequently encountered position indices, effectively approximating the undertrained regions using Flash Attention (Dao, 2023) by combining two key components: (1) sliding window attention (Beltagy et al., 2020; Ding et al., 2023; Xiao et al., 2023) to capture the diagonal, and (2) self-attention at the bottom-left corner using shifted position indices (Algorithm 1). This implementation incurs no additional computational costs and causes no obvious slowdowns during inference.

## Main Results [p. 2]

By strategically overwriting position indices at the upper range of the training length, we achieve substantial performance enhancements across seven open-source LLMs with context lengths ranging from 2K to 128K on the Needle-in-a-Haystack task, resulting in an average score increase of 18 points. STRING requires no additional training, enabling seamless scaling up with powerful large-scale models such as Llama3.1 70B (Llama Team, 2024) and Qwen2 72B (Bai et al., 2023). This integration not only establishes new state-of-the-art performance for open-source LLMs on long-context benchmarks RULER (Hsieh et al., 2024) and InfiniteBench (Zhang et al., 2024d) but also enables Llama3.1 to outperform leading commercial models, including GPT-4-128K (OpenAI, 2023), Claude-2 (Anthropic, 2023), and Kimi-chat (Moonshot AI, 2023), across a wide range of synthetic and practical tasks. The substantial improvements achieved by STRING provide strong evidence for our hypothesis: underrepresented position indices at the tail of the position frequency distribution, strongly constrain the long-context performance of current LLMs. We hope our findings will inspire new approaches to overcome these limitations and lead to more effective long-context processing in future LLM designs.
