# Appendix J: Human Evaluation [p. 32-34]

[p. 32-34] Human evaluation protocol details:

- 15 volunteers,
- 50 counterfactual scenarios,
- pairwise ranking across model outputs for consistency and fluency,
- 150 consistency judgments and 150 fluency judgments.

[p. 32] Participants compared unedited GPT-2 XL, FT+L-edited, and ROME-edited outputs.

[p. 32-34] Figures include aggregate results and full prompt/instruction materials:

- **Figure 26**: aggregate ranking summary.
- **Figures 27-29**: random example rating sheets.
- **Figure 30**: full study instructions and worked example.

[p. 34] Appendix reiterates main qualitative finding: ROME improves consistency with inserted counterfactual facts, with some fluency degradation relative to FT+L in parts of the sample.
