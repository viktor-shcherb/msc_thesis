# Appendix F: Extended Quantitative Results [p. 26-27]

[p. 26-27] Appendix extends results to GPT-2 Medium (345M) and GPT-2 Large (774M).

**Table 5** (p. 26): COUNTERFACT metrics for smaller GPT-2 variants.

Key rows (score `S`):

- GPT-2 Medium baseline: `S = 33.4`.
- GPT-2 Medium FT+L: `S = 68.0`.
- GPT-2 Medium ROME: `S = 87.4`.
- GPT-2 Large baseline: `S = 32.8`.
- GPT-2 Large FT+L: `S = 71.2`.
- GPT-2 Large ROME: `S = 88.2`.

**Table 6** (p. 27): extended zsRE editing results for GPT-2 medium/large variants.

[p. 27] Reported trend: ROME remains strongest overall on these smaller autoregressive models in the reported setup.
