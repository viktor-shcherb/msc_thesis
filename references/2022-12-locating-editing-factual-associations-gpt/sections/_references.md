# References Cited in Notes

**Pearl (2001)**
Judea Pearl. 2001. Direct and indirect effects. *Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI 2001)*.

Cited in: `02_interventions-on-activations-for-tracing-information-flow.md` (causal mediation framing and indirect-effect interpretation).

**Pearl (2009)**
Judea Pearl. 2009. *Causality: Models, Reasoning, and Inference* (2nd ed.). Cambridge University Press.

Cited in: `02_interventions-on-activations-for-tracing-information-flow.md` (causal graph interpretation of hidden-state dependencies).

**Vig et al. (2020b)**
Jesse Vig, Stella Biderman, Yonatan Belinkov, et al. 2020. Investigating gender bias in language models using causal mediation analysis. *NeurIPS 2020*.

Cited in: `02_interventions-on-activations-for-tracing-information-flow.md` and `04_related-work.md` (causal mediation precedent).

**Geva et al. (2021)**
Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021. Transformer feed-forward layers are key-value memories.

Cited in: `03_interventions-on-weights-for-understanding-factual-association-storage.md` and `04_related-work.md` (MLP memory framing).

**Bau et al. (2020)**
David Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu, and Antonio Torralba. 2020. Rewriting a deep generative model. *ECCV 2020*.

Cited in: `03_interventions-on-weights-for-understanding-factual-association-storage.md` and `08_appendix-a-solving-for-lambda-algebraically.md` (rank-one constrained update basis).

**Zhu et al. (2020)**
Chen Zhu, Anshumali Shrivastava Rawat, et al. 2020. Modifying memories in transformer models. *arXiv:2012.00363*.

Cited in: `03_interventions-on-weights-for-understanding-factual-association-storage.md` and `12_appendix-e-method-implementation-details.md` (constrained fine-tuning baseline).

**De Cao et al. (2021)**
Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Editing factual knowledge in language models. *EMNLP 2021*.

Cited in: `03_interventions-on-weights-for-understanding-factual-association-storage.md`, `04_related-work.md`, and `10_appendix-c-details-on-zsre-evaluation-task.md` (KE baseline and zsRE setup lineage).

**Mitchell et al. (2021)**
Eric Mitchell, Charles Lin, et al. 2021. Fast model editing at scale. *ICLR 2022* (arXiv preprint 2021).

Cited in: `03_interventions-on-weights-for-understanding-factual-association-storage.md`, `04_related-work.md`, and `10_appendix-c-details-on-zsre-evaluation-task.md` (MEND baseline and data split conventions).

**Dai et al. (2022)**
Damai Dai, Li Dong, et al. 2022. Knowledge neurons in pretrained transformers. *ACL 2022*.

Cited in: `03_interventions-on-weights-for-understanding-factual-association-storage.md`, `04_related-work.md`, and `12_appendix-e-method-implementation-details.md` (KN baseline).

**Elazar et al. (2021a)**
Yanai Elazar, Nora Kassner, et al. 2021. Measuring and Improving Consistency in Pretrained Language Models. *TACL*.

Cited in: `03_interventions-on-weights-for-understanding-factual-association-storage.md`, `04_related-work.md`, and `11_appendix-d-details-on-counterfact-dataset.md` (ParaRel source for COUNTERFACT).

**Hase et al. (2021)**
Peter Hase, Mona Diab, et al. 2021. Do language models have belief state? *ACL 2021*.

Cited in: `03_interventions-on-weights-for-understanding-factual-association-storage.md` (difficulty of standard editing benchmarks).

**Sundararajan et al. (2017)**
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. *ICML 2017*.

Cited in: `09_appendix-b-causal-tracing.md` (Integrated Gradients comparison).
