# References [p. 11-12]

Note: This file contains only references cited in the extracted notes.

## References cited in notes

**[1]** Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. *Advances in neural information processing systems*, 35:23716–23736, 2022.
- Cited in 01_introduction.md as an example of multi-modal understanding capability of Transformers

**[2]** Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In *International Conference on Learning Representations*, 2021.
- Cited in 01_introduction.md, 02a_background-related-work.md as related work on over-squashing in GNNs

**[3]** Federico Barbero, Ameya Velingker, Amin Saberi, Michael M. Bronstein, and Francesco Di Giovanni. Locality-aware graph rewiring in GNNs. In *The Twelfth International Conference on Learning Representations*, 2024.
- Cited in 01_introduction.md, 02a_background-related-work.md as related work on over-squashing in GNNs

**[4]** Federico Barbero, Alex Vitvitskyi, Christos Perivolaropoulos, Razvan Pascanu, and Petar Veličković. Round and round we go! what makes rotary positional encodings useful? *arXiv preprint arXiv:2410.06205*, 2024.
- Cited in 11_appendix-c-experiments.md regarding RoPE positional encoding decay claims

**[5]** Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. *IEEE Transactions on Neural Networks*, 5(2):157–166, 1994.
- Cited in 02a_background-related-work.md as related work on vanishing gradients for recurrent models

**[6]** Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers, 2019.
- Cited in 02_background.md regarding Turing-completeness of Transformers under certain assumptions

**[7]** Grégoire Delétang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, and Pedro A. Ortega. Neural networks and the chomsky hierarchy, 2023.
- Cited in 02a_background-related-work.md placing Transformers within the Chomsky Hierarchy

**[8]** Francesco Di Giovanni, Lorenzo Giusti, Federico Barbero, Giulia Luise, Pietro Lio, and Michael M Bronstein. On over-squashing in message passing neural networks: The impact of width, depth, and topology. In *International Conference on Machine Learning*, pages 7865–7885. PMLR, 2023.
- Cited in 01_introduction.md, 02a_background-related-work.md, 05_over-squashing.md as related work on over-squashing in GNNs

**[9]** Andrew Joseph Dudzik, Tamara von Glehn, Razvan Pascanu, and Petar Veličković. Asynchronous algorithmic alignment with cocycles. In *Learning on Graphs Conference*, pages 3–1. PMLR, 2024.
- Cited in 06_counting.md regarding permutation invariance and task alignment

**[10]** Team Gemini. Gemini: a family of highly capable multimodal models. *arXiv preprint arXiv:2312.11805*, 2023.
- Cited in 01_introduction.md as the frontier LLM evaluated in this work (Gemini 1.5)

**[11]** Francesco Di Giovanni, T. Konstantin Rusch, Michael Bronstein, Andreea Deac, Marc Lackenby, Siddhartha Mishra, and Petar Veličković. How does over-squashing affect the power of GNNs? *Transactions on Machine Learning Research*, 2024.
- Cited in 01_introduction.md, 02a_background-related-work.md, 05_over-squashing.md regarding commute time and over-squashing

**[12]** Michael Hahn. Theoretical limitations of self-attention in neural sequence models. *Transactions of the Association for Computational Linguistics*, 8:156–171, 2020.
- Cited in 02a_background-related-work.md regarding attention concentration on non-informative tokens

**[13]** Luca Herranz-Celotti and Jean Rouat. Stabilizing rnn gradients through pre-training, 2024.
- Cited in 05_over-squashing.md regarding path counting in deep RNNs

**[14]** Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. *Neural Computation*, 9(8):1735–1780, 1997.
- Cited in 02a_background-related-work.md as related work on vanishing gradients for recurrent models

**[15]** Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. *Advances in Neural Information Processing Systems*, 36, 2024.
- Cited in 11_appendix-c-experiments.md regarding NoPE (No Positional Encodings)

**[16]** Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. *Science*, 378(6624):1092–1097, 2022.
- Cited in 01_introduction.md as an example of code completion capability of Transformers

**[17]** Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. *Transactions of the Association for Computational Linguistics*, 12:157–173, 2024.
- Cited in 01_introduction.md regarding copying failures in LLMs

**[18]** William Merrill and Ashish Sabharwal. The expresssive power of transformers with chain of thought. *arXiv preprint arXiv:2310.07923*, 2023.
- Cited in 02a_background-related-work.md regarding Chain-of-Thought improving Transformer performance

**[19]** R OpenAI. Gpt-4 technical report. *arxiv 2303.08774. View in Article*, 2(5), 2023.
- Cited in 01_introduction.md as an example of conversational agent capability of Transformers

**[20]** Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In Sanjoy Dasgupta and David McAllester, editors, *Proceedings of the 30th International Conference on Machine Learning*, volume 28 of *Proceedings of Machine Learning Research*, pages 1310–1318, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR.
- Cited in 02a_background-related-work.md as related work on vanishing gradients for recurrent models

**[21]** Binghui Peng, Srini Narayanan, and Christos Papadimitriou. On limitations of the transformer architecture, 2024.
- Cited in 02a_background-related-work.md regarding compositional function limitations

**[22]** Jorge Pérez, Pablo Barceló, and Javier Marinkovic. Attention is turing-complete. *Journal of Machine Learning Research*, 22(75):1–35, 2021.
- Cited in 01_introduction.md, 02_background.md regarding infinite precision and Turing-completeness, and 06_counting.md regarding counting limitations

**[23]** S Unnikrishna Pillai, Torsten Suel, and Seunghun Cha. The perron-frobenius theorem: some of its applications. *IEEE Signal Processing Magazine*, 22(2):62–75, 2005.
- Cited in 10_appendix-b-proofs.md regarding the Perron-Frobenius theorem for eigenvalue analysis

**[24]** Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. *arXiv preprint arXiv:2108.12409*, 2021.
- Cited in 11_appendix-c-experiments.md regarding ALiBi positional encodings

**[25]** Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. *Advances in Neural Information Processing Systems*, 36, 2024.
- Cited in 01_introduction.md regarding tool use for LLMs

**[26]** Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. *Neurocomputing*, 568:127063, 2024.
- Cited in 11_appendix-c-experiments.md regarding RoPE positional encoding decay claims

**[27]** Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. *arXiv preprint arXiv:2403.08295*, 2024.
- Cited in 03a_motivating-examples-copying.md, 04_representational-collapse.md, 11_appendix-c-experiments.md as the Gemma 7B model used for experiments

**[28]** Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at a practical size. *arXiv preprint arXiv:2408.00118*, 2024.
- Cited in 11_appendix-c-experiments.md regarding Gemma 2 and sliding window attention

**[29]** Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M Bronstein. Understanding over-squashing and bottlenecks on graphs via curvature. 2022.
- Cited in 05_over-squashing.md as related work on over-squashing in GNNs

**[30]** Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. *Advances in neural information processing systems*, 30, 2017.
- Cited in 01_introduction.md regarding the introduction of Transformer architectures

**[31]** Petar Veličković, Christos Perivolaropoulos, Federico Barbero, and Razvan Pascanu. softmax is not enough (for sharp out-of-distribution). *arXiv preprint arXiv:2410.01104*, 2024.
- Cited in 11_appendix-c-experiments.md regarding limitations of softmax for sharp attention

**[32]** Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. *arXiv preprint arXiv:2206.07682*, 2022.
- Cited in 01_introduction.md regarding counting failures in LLMs

**[33]** Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers, 2021.
- Cited in 02a_background-related-work.md regarding computational model of Transformers and formal languages

**[34]** Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In *International Conference on Machine Learning*, pages 10524–10533. PMLR, 2020.
- Cited in 02_background.md regarding Pre-LN Transformer architecture

**[35]** Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, Yizhe Zhang, Jiatao Gu, and Joshua M Susskind. Stabilizing transformer training by preventing attention entropy collapse. In *International Conference on Machine Learning*, pages 40770–40803. PMLR, 2023.
- Cited in 02a_background-related-work.md regarding attention entropy limitations
