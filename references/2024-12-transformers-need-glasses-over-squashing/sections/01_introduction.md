# 1 Introduction [p. 1–2]

In recent years the field of Natural Language Processing (NLP) has been revolutionised through the introduction of Transformer-based architectures [30]. Large Transformers trained on some version of next-token prediction, known as Large Language Models (LLMs), have demonstrated impressive performance across diverse tasks, including conversational agents [10, 19], understanding multi-modal inputs [1], and code completion [16]. Most contemporary LLMs specifically focus on the decoder part of the original Transformer architecture, and are commonly referred to as decoder-only Transformers. Consequently, the paper focuses primarily on such models.

However, despite the impressive performance of Transformers, recent works have uncovered surprising failures that may point to fundamental issues in their architecture. For instance, Transformer-based LLMs seem to be particularly challenged by seemingly simple tasks requiring counting [32] or copying elements along input sequences [17]. The authors find it important to study such failure cases, as these operations are fundamental building blocks of computation, and they are often necessary for solving reasoning tasks. A common strategy to assist LLMs in solving such tasks is to supply them with "tools" [25]. The authors argue that, while tool use will certainly help, it is still important to improve base model capabilities in this regard, because oftentimes, even producing accurate inputs to a tool may require complex reasoning operations. Specifically, often we need to copy some part of the Transformer's input into a tool—if the base model struggles with robust copying, even this operation can be in peril.

Accordingly, the authors find it important to explain why decoder-only Transformers do not perform well when it comes to such problems—not just as an intellectual endeavour, but also to help guide further practical improvements. While many works have studied the computational capabilities of Transformers [22, 18], they often make assumptions which do not correspond to present practical limitations, such as infinite floating-point precision or 'hard attention', making their conclusions less directly practically applicable.

In this work, the authors take a different approach and study what information can be contained in the representation of the last token at the last layer, as this is ultimately the information that will be used for next-token prediction—the fundamental mechanism through which modern Transformer LLMs perform training and inference. In particular, the authors show that for certain distinct sequences, their last-token representations can become arbitrarily close to each other. This leads to a representational collapse, exacerbated by the lower-precision floating-point types typically used by modern LLM stacks. As a result, Transformers incorrectly produce the same tokens on these sequence pairs—see Figure 1 (a).

Furthermore, the authors reveal that the computation graph employed by decoder-only Transformers, with its unidirectional causal mask, contributes to the observed representational collapse. This unidirectional flow of information, converging at the final token, is in fact likely to lead to a loss of information due to over-squashing. This effect is well studied in graph neural networks (GNNs) [2, 29, 8, 3, 11], and related to vanishing gradients [14, 5, 13]. The authors hope that this result will be of independent interest to the GNN community, as a practical application of over-squashing results at scale. Finally, the authors provide supporting empirical evidence that these issues are likely of practical interest, and propose simple solutions—directly stemming from the theoretical study—to help alleviate them.

**Figure 1** (p. 2): "(a) Representational Collapse (Theorem 4.2). From top to bottom, we have a series of sequences given to Transformer architectures, each comprising repeated 1 tokens with a single 0 token at the end. The color and proximity of the curved lines illustrate how these representations converge as sequence length increases. (b) Over-squashing (Theorem 5.1). Due to the architecture of decoder-only Transformers, tokens that are earlier in their input sequence will have significantly more paths through which their data can reach the representation used for next-token prediction, leading to 'over-squashing'. This effect is depicted here for an early token (blue) and later token (red) in a five-token sequence."

Description: Two-part diagram showing (a) Representational Collapse with sequences of varying lengths (1, 11, 111, 1111, and "1111...1" and "1111...11") converging to similar representations $\mathbf{v}_i^{(L)}$, with curved lines showing convergence measured by $\|\mathbf{v}_n^{(L)} - \mathbf{v}_{n+1}^{(L)}\|_\infty < \epsilon$; (b) Over-squashing showing a five-layer Transformer architecture with tokens $\mathbf{v}^0$ through $\mathbf{v}^4$ and their connections across layers, illustrating many blue paths from early tokens and fewer red paths from later tokens converging to final representation $\mathbf{y}_5$.
- Key elements: Part (a) shows vertical axis with sequences of growing length (0, 01, 11, 111, 1111, long sequences), horizontal wavy lines representing representations; Part (b) shows grid of nodes labeled $\mathbf{v}_i^{(j)}$ with connections between layers
- Notable patterns: In (a), longer sequences show closer convergence; In (b), blue (early token) has more paths than red (later token) through the network
- Supports claim: Demonstrates both the representational collapse phenomenon where distinct sequences yield arbitrarily close final representations, and the over-squashing phenomenon where early tokens have more paths to the final representation than later tokens

In summary, the paper provides the following contributions:

- Theoretical analysis of decoder-only Transformer limitations: the authors formalise the concepts of 'representational collapse' (Section 4) and 'over-squashing' (Section 5) in the context of Transformer-based architectures.
