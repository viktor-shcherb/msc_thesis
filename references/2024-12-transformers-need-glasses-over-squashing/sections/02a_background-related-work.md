# 2 Background (continued) [p. 4]

**Figure 2** (p. 4): "Results on simple copying tasks. (a). Gemini was prompted to predict the last token (diamond) of a sequences '1...10' or the first token (square) of a sequence '01...1'. (b). Same as (a) but with hints (see 3.2 for details) (c). Same as (a) but the sequences have interleaved 0s and 1s. See C.1 for extra details"

Description: Three scatter plots (A, B, C) showing performance on copying tasks with sequence length on x-axis (100-500) and accuracy (represented by checkmarks and x-marks) on y-axis
- Key elements: Plot A shows "Copying first and last tokens" with "First Token" (squares) and "Last Token" (diamonds); Plot B shows "Copying last token with hints" with "Last Token" (diamonds) and "Last Token w/o Hint" (diamonds); Plot C shows "Copying last in interleaved sequence" with "[0,1]*seq length + [0]" and "[1,0]*seq length + [0]"
- Notable patterns: In all three plots, performance degrades as sequence length increases from 100 to 500; first tokens generally perform better than last tokens in plot A; hints improve performance in plot B; interleaved sequences show different patterns in plot C
- Supports claim: Demonstrates that Transformers struggle with simple copying tasks, particularly for last tokens in longer sequences, which motivates the theoretical analysis of representational collapse

Works have also tried to study transformers capabilities through the lens of formal languages, such as Weiss et al. [33], which develops a computational model of what transformers can represent in an analogous way to how Recurrent Neural Networks are associated with finite automata, and then derive an implementable programming language that represents that model. Following that, Delétang et al. [7] place transformers within the Chomsky Hierarchy, showing that they are quite limited and cannot learn the decision problem for simple languages, which prompted authors to show that Transformer LLMs can perform substantially better if they generate a number of decoding tokens linear in the problem input size, through scratch-pad, Chain-of-Thought (CoT) or similar [18]. Finally, Peng et al. [21] show that the Transformer block with finite precision is fundamentally limited in its ability to represent compositional functions and solve simple problems that require it. The current work similarly analyses the Transformer's inability to solve simple computational tasks, and proves that even with techniques like Chain-of-Thought that inability persists as it is inherent to the combination of architecture, next-token prediction, and limited floating point precision.

## Decay in attention mechanisms

Works have also studied the limitations of self-attention by showing that it can reach pathological states that limit what transformers are able to learn. For instance, it has been show how a great reduction in the attention entropy can lead to unstable training if occurring early, but even when occurring later in training it can still lead to significantly lower performance [35]. Further, it has been shown that specific tokens can strongly concentrate attention, leading to transformers being unable to learn to process simple languages, like PARITY and DYCK [12]. The current work will similarly focus on showing how Transformers end-up effectively ignoring many tokens in their input which leads them to fail to solve simple computational problems, studying such a phenomenon by directly analysing the representational capacity.

## Over-squashing

Graph neural networks (GNNs) are neural networks designed to operate over graph structures. Importantly, Transformers, may be seen as types of attention-based GNNs operating over specific types of graphs. The difficulties of propagating information over a graph have been thoroughly analysed, with a notable phenomenon being that of over-squashing [2, 29, 8, 3]. Over-squashing refers to the fact that propagating information over certain graphs that exhibit 'bottlenecks' is likely to induce a 'squashing' of information. This can be made more precise by studying this effect via the notion of a commute time [11]—the expected number of steps that a random walk takes to travel from a node to another node and back. Information travelling between nodes with higher commute time will be squashed more.

A common way to measure over-squashing is by looking at how sensitive the representation $\mathbf{x}_v^{(L)}$ of a node $v$ after $L$ GNN layers is to the initial representation $\mathbf{x}_u^{(0)}$ of another node $u$. In particular, the partial derivative $\partial \mathbf{x}_v^{(L)} / \partial \mathbf{x}_u^{(0)}$ may be shown to decay, especially for nodes with high commute times between them. The current work may be seen as acting as a bridge between the well-studied phenomenon of over-squashing and the loss of information the authors analyse in decoder-only Transformers specifically for language tasks. Note that this type of derivation is typical in the study of vanishing gradients for recurrent models as well [14, 5, 20, 13].
