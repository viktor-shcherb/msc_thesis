# DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models

**Authors:** Damai Dai, Chengqi Deng, Chenggang Zhao, R.X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y.K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, Wenfeng Liang
**Affiliations:** DeepSeek-AI, Peking University

## Publication Status

- **arXiv preprint:** January 2024, arXiv:2401.06066
- **Peer-reviewed:** Yes
- **Conference:** Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024), Volume 1: Long Papers, pages 1280-1297, Bangkok, Thailand, August 2024
- **Status:** Published conference paper

## Preferred Citation

Cite the ACL 2024 version:

> Dai, D., Deng, C., Zhao, C., Xu, R.X., Gao, H., Chen, D., Li, J., Zeng, W., Yu, X., Wu, Y., Xie, Z., Li, Y.K., Huang, P., Luo, F., Ruan, C., Sui, Z., & Liang, W. (2024). DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1280-1297.

## Links

- arXiv: https://arxiv.org/abs/2401.06066
- ACL Anthology: https://aclanthology.org/2024.acl-long.70/
- DOI: https://doi.org/10.18653/v1/2024.acl-long.70
- Code: https://github.com/deepseek-ai/DeepSeek-MoE
- Model weights: https://huggingface.co/deepseek-ai/deepseek-moe-16b-base
