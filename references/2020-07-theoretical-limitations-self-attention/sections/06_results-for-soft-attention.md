# 6 Results for Soft Attention [p. 10--11]

[p. 10] In the previous section, it was shown that transformers using hard attention are not able to recognize a range of core formal languages. This section studies soft attention. Proving limitations as strong as what was found in the hard attention setting would settle a major open problem in computational complexity, and may therefore be extremely hard to attain with currently available mathematical methods.^3 This barrier prevents proving bounds on the *accuracy* that soft attention transformers can achieve; nevertheless, it is possible to prove limitations on the achievable *cross-entropy* in modeling distributions over the formal languages. The smoothness of the operations used in transformers is used to show that any transformer, as inputs get longer, will not be able to robustly model such distributions. The idea behind the proof is that the impact of any single input symbol on the output of the transformer is small if the input is long.

> Footnote 3: "Showing that soft attention transformers cannot achieve perfect accuracy on evaluating Boolean formulas would separate the complexity classes LTC^0 and NC^1, a widely conjectured but long-open problem in computational complexity." [p. 10]

## Lemma 5

**Lemma 5.** *Let a soft attention transformer be given, and let n be the input length. If we exchange one input symbol x_i (i < n), then the change in the resulting activation y_n^{(L)} at the decoder layer is bounded as O(1/n) with constants depending on the parameter matrices.* [p. 10]

This contrasts with recurrent networks: Changing a single input can have a nonnegligible impact on the final state even for very long input. For example, an RNN recognizing PARITY through a hidden state that encodes parity of the current prefix will flip its hidden state if a single input bit is flipped.

Lemma 5 entails that, as inputs become longer, soft attention transformers cannot achieve good cross-entropies on prediction problems that are very sensitive to individual input symbols: A Lipschitz-continuous prediction function, such as a ReLU MLP with a softmax output, will not be able to make very different predictions for inputs that are encoded into similar activations y_n^{(L)}.

## Setting for Theorem 6

[p. 10] To make all assumptions explicit, the following setting is assumed (though the results do not depend on the specific details). For PARITY, consider the distribution over bitstrings generated by a two-state automaton that -- if the number of 1s emitted so far is even -- terminates with probability p, and otherwise emits a 1 or 0 with equal probability each. Given a prefix of a string drawn from this distribution, the transformer is asked to predict the next symbol from Sigma = {0, 1, ENDOFSEQUENCE}. The next symbol can be ENDOFSEQUENCE if and only if the prefix has an even number of 1s. For 2DYCK, the experimental study of Skachkova et al. (2018) is followed, and the distribution generated by a PCFG that expands S -> (S)S or S -> [S]S with probability p/2 each, and S -> epsilon with probability 1 - p, is taken. The model is asked to predict the next character among Sigma = {(, ), [, ], ENDOFSEQUENCE}.

## Theorem 6

**Theorem 6.** *Let a soft attention transformer be given for PARITY or 2DYCK. As n -> infinity, cross-entropy on predicting the next symbol converges to unigram chance level (PARITY), or is at least separated from the optimal cross-entropy by some constant epsilon > 0 (2DYCK).* [p. 10]

## Proof of Theorem 6

[p. 10--11] **Proof for PARITY.** Exchanging a single bit flips membership in PARITY. Thus, for any x in PARITY, there is a string x' not in PARITY, differing only in one bit. As x and x' differ only in one bit, the transformer's output activations differ by O(1/n). Therefore, a Lipschitz-continuous prediction function cannot robustly assign different next-symbol probabilities after even and odd numbers of 1s, and cross-entropy will converge to unigram chance level.

**Proof for 2DYCK.** Consider a string x of length n, known to be a prefix of a word generated by the PCFG. One can show that there is a constant P_0 in (0, 1) (dependent on p but not n), such that x both ends with a closing bracket, and is unbalanced, with probability >= P_0.^4 After such an x, the next symbol is a closing bracket with constant nonzero probability (1 - p). If x can be followed by, say, ')' but not ']', then there is a string x', differing only in one input position, for which the next symbol can be ']' but not ')'. As x was assumed to end with a closing bracket, the exchanged symbol is not the last symbol of x, and thus the transformer's predictions on x and x' differ only by O(1/n). The prediction task can be decomposed into predicting (1) whether an opening or closing bracket, or ENDOFSEQUENCE, follows, and (2) whether a round or square bracket follows, in case a bracket follows. The cross-entropy loss is the sum of the cross-entropies incurred on these two successive prediction tasks. Therefore, when such a prefix x is followed by the correct closing bracket, say ')', the model will incur, as n -> infinity, a cross-entropy on the second task of at least log 2, reflecting at-chance performance in choosing between the two possible closing brackets. In contrast, optimal cross-entropy loss on (2) would be 0, as the bracket type (round or square) is actually fully determined by x. Thus, the overall cross-entropy on all prefixes x of length n is, asymptotically as n -> infinity, at least P_0 * (1 - p) * log 2 > 0 more than the optimal cross-entropy. QED.

> Footnote 4: "One can show this formally using a Markov chain argument. Let the height H(x) of a word x be the number of opening brackets minus the number of closing brackets in x. When iteratively sampling a symbol sequence using a pushdown automaton for 2DYCK, the height H_n of the prefix x up to length n forms a Markov chain taking values in N. The prefix x is unbalanced if and only if H_n > 0, this is always the case whenever n is odd. Restricting to even n, the chain {H_n : n = 0, 2, 4, ...} is aperiodic and takes values in {0, 2, 4, ...}. It is also positive recurrent, as words sampled from the PCFG have finite expected length (Skachkova et al., 2018, 2.2.1). Therefore, the Markov chain {H_n : n = 0, 2, 4, ...} converges to its stationary distribution (Mitzenmacher and Upfal, 2017), which -- by positive recurrence -- must assign some nonzero weight pi_{2i} to each height 2i (i >= 0). Hence, even when n is even, the prefix x is unbalanced with probability 1 - pi_0 asymptotically independent of n. Also, since the transition probabilities P(H_{n+1}|H_n) are independent of n, there is an asymptotically constant nonzero probability that x_1 ... x_{|x|-1} has height greater than x, i.e., the last bracket of x is a closing one." [p. 11]

## Proof of Lemma 5 [p. 11]

[p. 11] The activations at the decoder layer are compared for two inputs that only differ in the input at the i-th position. Let D = ||v_i - v'_i||_2 the norm of the difference of the input embeddings at this position. By induction over k = 1, ..., L, for some constant C > 0 (chosen below) the differences between the resulting activations y_j^{(k)}, y_j^{(k)'} are bounded as:

||y_i^{(k)} - y_i^{(k)'}|| <= C^{2k} D = O(1)

||y_j^{(k)} - y_j^{(k)'}|| <= (H^k C^{2k} D) / n = O(1/n)  (j != i)

Once this is shown, the influence of any individual input on the final prediction is O(1/n), with constants depending on the parameter matrices.

At this point, the paper remarks that a key property of transformers for this proof is that the number L of layers is bounded independently of the input length. A similar proof strategy can also be applied to other fixed-depth architectures that combine unboundedly many inputs in a smooth manner, such as 1D temporal convolutions with average pooling.

**Base case (k = 0):** ||y_i^{(0)} - y_i^{(0)'}|| <= D, and ||y_j^{(0)} - y_j^{(0)'}|| = 0 for j != i. Also note ||y_j^{(k)}||_2 <= 2 C_{f^{act}}^L (||p_j|| + ||v_j||), where C_{f^{act}} < infinity depends on the norms of the parameter matrices of f^{act}, which is implemented as a ReLU MLP (Vaswani et al., 2017). F is used for this upper bound for ||y_j^{(k)}||_2. Attention logits are bounded by A := F^2 C_{f^{att}} in the case of dot product attention, and A := 2F C_{f^{att}} in the case of additive attention. Then any attention weight \hat{a}_{j,i} = exp(a_i) / sum_j exp(a_j) is upper bounded by exp(A) / (exp(A) + (n-1) exp(-A)) <= exp(2A) / (n - 1).

Choose C := 2 * (1 + exp(2A) + L_{f^{act}}), where L_{f^{act}} is the Lipschitz constant of the ReLU MLP f^{act}. Recall that activations y_i^{(k)} are defined as f^{act}(y_i^{(k-1)}, b_{i,k,1}, ..., b_{i,k,H}), where b_{i,k,h} equals sum_{j=1}^{n} \hat{a}_{i,j}^{k,h} y_j^{(k-1)}. First calculate:

||b_{j,k,h} - b'_{j,k,h}|| <= sum_{w=1}^{n} \hat{a}_{j,w}^{k,h} ||y_w^{(k-1)} - y_w^{(k-1)'}||

= \hat{a}_{j,i}^{k,h} ||y_i^{(k-1)} - y_i^{(k-1)'}|| + sum_{w != i} \hat{a}_{j,w}^{k,h} ||y_w^{(k-1)} - y_w^{(k-1)'}||

which, using the induction hypothesis, is at most:

(exp(2A) / (n - 1)) * C^{2(k-1)} * D + (H^{k-1} * C^{2(k-1)} * D) / n

<= (H^{k-1} * C^{2k-1} * D) / n

Plugging this into the definition of y_i^{(k)}, the difference ||y_j^{(k)} - y_j^{(k)'}|| is at most:

L_{f^{act}} * (||y_j^{(k-1)} - y_j^{(k-1)'}|| + sum_{q=1}^{H} ||b_{i,k,q} - b'_{i,k,q}||)

First, if j = i, this is bounded by (as n -> infinity):

<= L_{f^{act}} * (C^{2(k-1)} * D + o(1)) <= C^{2k} D

Second, if j != i, this is bounded by:

<= L_{f^{act}} * ((H^{k-1} * C^{2(k-1)} * D) / n + H * (H^{k-1} * C^{2k-1} * D) / n)

<= (H^k * C^{2k} * D) / n

This proves the inductive step for k > 0. QED.
