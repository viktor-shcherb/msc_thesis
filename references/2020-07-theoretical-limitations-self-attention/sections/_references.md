# References

Only references actually cited in the section notes are listed here.

## Cited References

### Barrington et al. (1992)
- **Full entry:** David A. Mix Barrington, Kevin Compton, Howard Straubing, and Denis Therien. 1992. Regular languages in NC1. *Journal of Computer and System Sciences*, 44(3):478--499.
- **Cited in:** 04_regular-and-context-free-languages.md (footnote 2, on quasi-aperiodic syntactic morphisms and inability to compute PARITY).

### Bengio et al. (1994)
- **Full entry:** Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with Gradient Descent is Difficult. *IEEE Transactions on Neural Networks*, 5(2):157--166.
- **Cited in:** 01_introduction.md (theoretical work on recurrent neural networks).

### Bernardy (2018)
- **Full entry:** Jean-Philippe Bernardy. 2018. Can recurrent neural networks learn nested recursion? *LiLT (Linguistic Issues in Language Technology)*, 16(1).
- **Cited in:** 02_related-work.md (experimental study of recurrent architectures on well-bracketed strings), 04_regular-and-context-free-languages.md (experimental study of LSTMs on 2DYCK-like languages).

### Boppana (1997)
- **Full entry:** Ravi B. Boppana. 1997. The average sensitivity of bounded-depth circuits. *Information Processing Letters*, 63(5):257--261.
- **Cited in:** 05_results-for-hard-attention.md (sensitivity of functions in computational complexity).

### Cartling (2008)
- **Full entry:** Bo Cartling. 2008. On the implicit acquisition of a context-free grammar by a simple recurrent neural network. *Neurocomputing*, 71(7--9):1527--1537.
- **Cited in:** 02_related-work.md (experimental study of recurrent networks on counter languages).

### Chen et al. (2018)
- **Full entry:** Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, Mike Schuster, Noam Shazeer, Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. 2018. The best of both worlds: Combining recent advances in neural machine translation. In *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 76--86. Melbourne, Australia. Association for Computational Linguistics.
- **Cited in:** 01_introduction.md, 02_related-work.md (extensions of transformers with recurrence citing limitations of self-attention), 07_discussion.md (recurrent architectures compared to self-attention).

### Cheng et al. (2016)
- **Full entry:** Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016. Long short-term memory-networks for machine reading. In *Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing*, pages 551--561. Austin, Texas. Association for Computational Linguistics.
- **Cited in:** 02_related-work.md (prior work on self-attention).

### Child et al. (2019)
- **Full entry:** Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. *arXiv preprint arXiv:1904.10509v1*.
- **Cited in:** 01_introduction.md (scaling transformers to very long sequences).

### Chomsky (1957)
- **Full entry:** Noam Chomsky. 1957. *Syntactic Structures*. Mouton, The Hague.
- **Cited in:** 07_discussion.md (context-free grammars or beyond necessary to capture natural language syntax and meaning).

### Chomsky and Schutzenberger (1963)
- **Full entry:** Noam Chomsky and Marcel P. Schutzenberger. 1963. The algebraic theory of context-free languages. *Studies in Logic and the Foundations of Mathematics*, 35, pages 118--161. Elsevier.
- **Cited in:** 01_introduction.md (Dyck language definition), 04_regular-and-context-free-languages.md (Chomsky-Schutzenberger theorem relating context-free languages to 2DYCK).

### Clark et al. (2019)
- **Full entry:** Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. What does BERT look at? An analysis of BERT's attention. In *Proceedings of BlackboxNLP 2019*.
- **Cited in:** 01_introduction.md (analysis studies showing transformers encode syntactic knowledge), 03_self-attention.md (attention concentrates on a few positions in trained models).

### Dai et al. (2019)
- **Full entry:** Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive language models beyond a fixed-length context. In *Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-August 2, 2019, Volume 1: Long Papers*, pages 2978--2988.
- **Cited in:** 01_introduction.md (scaling transformers to very long sequences).

### De Palma et al. (2018)
- **Full entry:** Giacomo De Palma, Bobak Toussi Kiani, and Seth Lloyd. 2018. Deep neural networks are biased towards simple functions. *arXiv preprint arXiv:1812.10156v2*.
- **Cited in:** 05_results-for-hard-attention.md (sensitivity linked to generalization in feedforward networks).

### Dehghani et al. (2019)
- **Full entry:** Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. 2019. Universal transformers. In *International Conference on Learning Representations*.
- **Cited in:** 01_introduction.md, 02_related-work.md (suggestion that transformers cannot compute functions requiring sequential processing), 07_discussion.md (recurrent architectures compared to self-attention).

### Devlin et al. (2019)
- **Full entry:** Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages 4171--4186. Minneapolis, Minnesota. Association for Computational Linguistics.
- **Cited in:** 02_related-work.md (BERT encodes syntactic information).

### Everaert et al. (2015)
- **Full entry:** Martin B. H. Everaert, Marinus A. C. Huybregts, Noam Chomsky, Robert C. Berwick, and Johan J. Bolhuis. 2015. Structures, not strings: linguistics as part of the cognitive sciences. *Trends in Cognitive Sciences*, 19(12):729--743.
- **Cited in:** 01_introduction.md (hierarchical structure essential to modeling natural language syntax).

### Furst et al. (1984)
- **Full entry:** Merrick Furst, James B. Saxe, and Michael Sipser. 1984. Parity, circuits, and the polynomial-time hierarchy. *Mathematical Systems Theory*, 17(1):13--27.
- **Cited in:** 05_results-for-hard-attention.md (input restrictions in Boolean circuit theory; proved bounded-depth circuits cannot compute PARITY).

### Gers and Schmidhuber (2001)
- **Full entry:** Felix A. Gers and Jurgen Schmidhuber. 2001. LSTM recurrent networks learn simple context-free and context-sensitive languages. *IEEE Transactions on Neural Networks*, 12(6):1333--1340.
- **Cited in:** 01_introduction.md, 02_related-work.md (RNN capability to capture context-free languages / counter languages).

### Gibson and Thomas (1999)
- **Full entry:** Edward Gibson and James Thomas. 1999. Memory limitations and structural forgetting: The perception of complex ungrammatical sentences as grammatical. *Language and Cognitive Processes*, 14(3):225--248.
- **Cited in:** 07_discussion.md (center embeddings are very challenging for humans to process).

### Gopalan et al. (2016)
- **Full entry:** Parikshit Gopalan, Noam Nisan, Rocco A. Servedio, Kunal Talwar, and Avi Wigderson. 2016. Smooth boolean functions are easy: Efficient algorithms for low-sensitivity functions. In *Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science*, pages 59--70. ACM.
- **Cited in:** 05_results-for-hard-attention.md (sensitivity of functions in computational complexity).

### Gruning (2006)
- **Full entry:** Andre Gruning. 2006. Stack-like and queue-like dynamics in recurrent neural networks. *Connection Science*, 18(1):23--42.
- **Cited in:** 01_introduction.md (RNN capability to capture context-free languages), 07_discussion.md (infinite-precision RNNs and LSTMs can model stacks).

### Gulordava et al. (2018)
- **Full entry:** Kristina Gulordava, Piotr Bojanowski, Edouard Grave, Tal Linzen, and Marco Baroni. 2018. Colorless green recurrent networks dream hierarchically. In *Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)*, pages 1195--1205. New Orleans, Louisiana. Association for Computational Linguistics.
- **Cited in:** 01_introduction.md, 02_related-work.md (LSTMs modeling hierarchical structure in realistic natural language data).

### Hao et al. (2019)
- **Full entry:** Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, and Zhaopeng Tu. 2019. Modeling recurrence for transformer. In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages 1198--1207. Minneapolis, Minnesota. Association for Computational Linguistics.
- **Cited in:** 01_introduction.md, 02_related-work.md (extensions of transformers with recurrence), 07_discussion.md (recurrent architectures compared to self-attention).

### Hastad et al. (1994)
- **Full entry:** Johan Hastad, Ingo Wegener, Norbert Wurm, and Sang-Zin Yi. 1994. Optimal depth, very small size circuits for symmetrical functions in AC0. *Information and Computation*, 108(2):200--211.
- **Cited in:** 05_results-for-hard-attention.md (input restrictions in Boolean circuit theory).

### Hsieh et al. (2019)
- **Full entry:** Yu-Lun Hsieh, Minhao Cheng, Da-Cheng Juan, Wei Wei, Wen-Lian Hsu, and Cho-Jui Hsieh. 2019. On the robustness of self-attentive models. In *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*, pages 1520--1529. Florence, Italy. Association for Computational Linguistics.
- **Cited in:** 01_introduction.md (early theoretical study of self-attention), 02_related-work.md (adversarial robustness of transformers; theoretical result on single self-attention layer robustness).

### Kalinke and Lehmann (1998)
- **Full entry:** Yvonne Kalinke and Helko Lehmann. 1998. Computation in recurrent neural networks: From counters to iterated function systems. In *Australian Joint Conference on Artificial Intelligence*, pages 179--190. Springer.
- **Cited in:** 01_introduction.md, 02_related-work.md (RNN capability to capture context-free / counter languages).

### Kirov and Frank (2012)
- **Full entry:** Christo Kirov and Robert Frank. 2012. Processing of nested and cross-serial dependencies: An automaton perspective on SRN behaviour. *Connection Science*, 24(1):1--24.
- **Cited in:** 07_discussion.md (infinite-precision RNNs and LSTMs can model stacks).

### Korsky and Berwick (2019)
- **Full entry:** Samuel A. Korsky and Robert C. Berwick. 2019. On the computational power of RNNs. *arXiv preprint arXiv:1906.06349v2*.
- **Cited in:** 01_introduction.md, 02_related-work.md (arbitrary-precision RNNs can emulate pushdown automata, recognize all deterministic context-free languages).

### Kuncoro et al. (2018)
- **Full entry:** Adhiguna Kuncoro, Chris Dyer, John Hale, Dani Yogatama, Stephen Clark, and Phil Blunsom. 2018. LSTMs can learn syntax-sensitive dependencies well, but modeling structure makes them better. In *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 1426--1436.
- **Cited in:** 07_discussion.md (empirical research showing models struggle with syntactic generalization despite strong quantitative performance).

### Lewis and Vasishth (2005)
- **Full entry:** Richard L. Lewis and Shravan Vasishth. 2005. An activation-based model of sentence processing as skilled memory retrieval. *Cognitive Science*, 29(3):375--419.
- **Cited in:** 07_discussion.md (psycholinguistic models of memory in human sentence processing; processing models predict difficulty with center embedding because they cannot count brackets).

### Lin et al. (2017)
- **Full entry:** Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. 2017. A structured self-attentive sentence embedding. In *International Conference on Learning Representations*.
- **Cited in:** 02_related-work.md (prior work on self-attention).

### Lin et al. (2019)
- **Full entry:** Yongjie Lin, Yi Chern Tan, and Robert Frank. 2019. Open Sesame: Getting inside BERT's linguistic knowledge. In *Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP*, pages 241--253, Florence, Italy. Association for Computational Linguistics.
- **Cited in:** 01_introduction.md, 02_related-work.md (BERT encodes syntactic information).

### Linzen et al. (2016)
- **Full entry:** Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing the ability of LSTMs to learn syntax-sensitive dependencies. *Transactions of the Association for Computational Linguistics*, 4:521--535.
- **Cited in:** 01_introduction.md, 02_related-work.md (hierarchical structure in realistic natural language / subject-verb agreement).

### Marvin and Linzen (2018)
- **Full entry:** Rebecca Marvin and Tal Linzen. 2018. Targeted syntactic evaluation of language models. In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing*, pages 1192--1202. Brussels, Belgium. Association for Computational Linguistics.
- **Cited in:** 07_discussion.md (empirical research showing models struggle with syntactic generalization).

### McCoy et al. (2019)
- **Full entry:** R. Thomas McCoy, Junghyun Min, and Tal Linzen. 2019. BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance. *arXiv preprint arXiv:1911.02969v1*.
- **Cited in:** 07_discussion.md (quantitative performance metrics can be dissociated from syntactic knowledge on challenging benchmarks).

### Miller and Chomsky (1963)
- **Full entry:** George A. Miller and Noam Chomsky. 1963. Finitary models of language users. In R. Duncan Luce, Robert R. Bush, and Eugene Galanter, editors, *Handbook of Mathematical Psychology*, pages 419--492. John Wiley.
- **Cited in:** 07_discussion.md (center embeddings are very challenging for humans to process).

### Mitzenmacher and Upfal (2017)
- **Full entry:** Michael Mitzenmacher and Eli Upfal. 2017. *Probability and Computing*, 2nd edition. Cambridge University Press, Cambridge.
- **Cited in:** 05_results-for-hard-attention.md (Chernoff bound, Theorem 4.4; Lovasz Local Lemma, Theorem 6.17), 06_results-for-soft-attention.md (Markov chain convergence to stationary distribution in footnote 4).

### McNaughton and Papert (1971)
- **Full entry:** Robert McNaughton and Seymour A. Papert. 1971. *Counter-Free Automata (MIT Research Monograph No. 65)*. The MIT Press.
- **Cited in:** 04_regular-and-context-free-languages.md (counter-free / star-free languages definition).

### Merrill (2019)
- **Full entry:** William Merrill. 2019. Sequential neural networks as automata. In *Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges*, pages 1--13. Florence. Association for Computational Linguistics.
- **Cited in:** 01_introduction.md (theoretical work on recurrent neural networks), 02_related-work.md (in finite precision, LSTMs recognize a subset of counter languages, GRUs and simple RNNs recognize regular languages).

### Miller and Hardt (2019)
- **Full entry:** John Miller and Moritz Hardt. 2019. Stable recurrent models. In *International Conference on Learning Representations*.
- **Cited in:** 01_introduction.md (theoretical work on recurrent neural networks).

### Parikh et al. (2016)
- **Full entry:** Ankur Parikh, Oscar Tackstrom, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In *Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing*, pages 2249--2255. Austin, Texas. Association for Computational Linguistics.
- **Cited in:** 02_related-work.md (prior work on self-attention).

### Paulus et al. (2018)
- **Full entry:** Romain Paulus, Caiming Xiong, and Richard Socher. 2018. A deep reinforced model for abstractive summarization. In *International Conference on Learning Representations*.
- **Cited in:** 02_related-work.md (prior work on self-attention).

### Parker et al. (2017)
- **Full entry:** Dan Parker, Michael Shvartsman, and Julie A. Van Dyke. 2017. The cue-based retrieval theory of sentence comprehension: New findings and new challenges. *Language Processing and Disorders*, pages 121--144.
- **Cited in:** 07_discussion.md (psycholinguistic models of memory in human sentence processing).

### Perez et al. (2019)
- **Full entry:** Jorge Perez, Javier Marinkovic, and Pablo Barcelo. 2019. On the Turing completeness of modern neural network architectures. In *International Conference on Learning Representations*.
- **Cited in:** 01_introduction.md, 02_related-work.md (first theoretical study of transformers; Seq2Seq transformers emulate Turing machines with unbounded decoding steps), 03_self-attention.md (hard attention variant definition), 05_results-for-hard-attention.md (study of hard attention).

### Sennhauser and Berwick (2018)
- **Full entry:** Luzi Sennhauser and Robert Berwick. 2018. Evaluating the ability of LSTMs to learn context-free grammars. In *Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP*, pages 115--124.
- **Cited in:** 01_introduction.md, 02_related-work.md, 04_regular-and-context-free-languages.md (experimental study of RNNs/LSTMs on well-bracketed strings / 2DYCK).

### Shen et al. (2018a)
- **Full entry:** Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Shirui Pan, and Chengqi Zhang. 2018a. Disan: Directional self-attention network for RNN/CNN-free language understanding. In *Thirty-Second AAAI Conference on Artificial Intelligence*.
- **Cited in:** 01_introduction.md, 02_related-work.md (extensions of transformers with recurrence), 07_discussion.md (recurrent architectures compared to self-attention).

### Shieber (1985)
- **Full entry:** Stuart M. Shieber. 1985. Evidence against the context-freeness of natural language. In *Philosophy, Language, and Artificial Intelligence*, pages 79--89. Springer.
- **Cited in:** 07_discussion.md (argued that more powerful formalisms beyond context-free grammars are necessary for natural language).

### Shen et al. (2018b)
- **Full entry:** Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Sen Wang, and Chengqi Zhang. 2018b. Reinforced self-attention network: A hybrid of hard and soft attention for sequence modeling. In *IJCAI'18 Proceedings of the 27th International Joint Conference on Artificial Intelligence*, pages 4345--4352.
- **Cited in:** 03_self-attention.md (choice between soft and hard attention).

### Siegelman and Sontag (1995)
- **Full entry:** Hava Siegelman and Eduardo D. Sontag. 1995. On the computational power of neural nets. *Journal of Computer and System Sciences*, 50:132--150.
- **Cited in:** 01_introduction.md (theoretical work on RNNs), 02_related-work.md (classical result: RNNs with unlimited computation time can emulate Turing machines).

### Skachkova et al. (2018)
- **Full entry:** Natalia Skachkova, Thomas Trost, and Dietrich Klakow. 2018. Closing brackets with recurrent neural networks. In *Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP*, pages 232--239.
- **Cited in:** 02_related-work.md (experimental study of recurrent architectures on well-bracketed strings), 04_regular-and-context-free-languages.md (experimental study of LSTMs on 2DYCK-like languages), 06_results-for-soft-attention.md (experimental study followed for 2DYCK distribution setup; finite expected length of PCFG-sampled words in footnote 4).

### Suzgun et al. (2019)
- **Full entry:** Mirac Suzgun, Yonatan Belinkov, and Stuart M. Shieber. 2019. On evaluating the generalization of LSTM models in formal languages. *Proceedings of the Society for Computation in Linguistics (SCiL)*, pages 277--286.
- **Cited in:** 02_related-work.md (experimental study of recurrent networks on counter languages).

### Tabor (2000)
- **Full entry:** Whitney Tabor. 2000. Fractal encoding of context-free grammars in connectionist networks. *Expert Systems*, 17(1):41--56.
- **Cited in:** 07_discussion.md (infinite-precision RNNs and LSTMs can model stacks).

### Tenney et al. (2019)
- **Full entry:** Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT rediscovers the classical NLP pipeline. In *Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-August 2, 2019, Volume 1: Long Papers*, pages 4593--4601.
- **Cited in:** 01_introduction.md, 02_related-work.md (BERT encodes syntactic information).

### Tran et al. (2018)
- **Full entry:** Ke Tran, Arianna Bisazza, and Christof Monz. 2018. The importance of being recurrent for modeling hierarchical structure. In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing*, pages 4731--4736, Brussels, Belgium. Association for Computational Linguistics.
- **Cited in:** 01_introduction.md (experimental evidence that transformers might not be as strong as LSTMs for hierarchical structure), 02_related-work.md (comparison of transformers and LSTMs on hierarchical structure learning), 07_discussion.md (recurrent architectures compared to self-attention; empirical research on syntactic generalization).

### Vaswani et al. (2017)
- **Full entry:** Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In *Advances in Neural Information Processing Systems*, pages 5998--6008.
- **Cited in:** 01_introduction.md, 02_related-work.md, 03_self-attention.md (original transformer paper; self-attention definition, positional embeddings, value vectors, feedforward network with skip-connection), 04_regular-and-context-free-languages.md (construction of transformers in sequence-to-sequence tasks), 06_results-for-soft-attention.md (ReLU MLP implementation of f^{act} in Proof of Lemma 5).

### Voita et al. (2019)
- **Full entry:** Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In *Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-August 2, 2019, Volume 1: Long Papers*, pages 5797--5808.
- **Cited in:** 03_self-attention.md (analysis suggesting attention concentrates on one or a few positions; most important heads focus on a few positions).

### Weiss et al. (2018)
- **Full entry:** Gail Weiss, Yoav Goldberg, and Eran Yahav. 2018. On the practical computational power of finite precision rnns for language recognition. In *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)*, pages 740--745.
- **Cited in:** 01_introduction.md, 02_related-work.md (RNN capability to capture context-free / counter languages; formalization of language recognition as sequence-to-sequence task), 03_self-attention.md (formalization of language recognition).

### Yang et al. (2019)
- **Full entry:** Baosong Yang, Longyue Wang, Derek F. Wong, Lidia S. Chao, and Zhaopeng Tu. 2019. Assessing the ability of self-attention networks to learn word order. In *Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL, Florence, Italy, July 28-August 2, 2019, Volume 1: Long Papers*, pages 3635--3644.
- **Cited in:** 02_related-work.md (experimentally investigated self-attention for word order information).
