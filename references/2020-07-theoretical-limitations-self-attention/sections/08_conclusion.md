# 8 Conclusion [p. 13]

[p. 13] The paper formally investigated the capabilities of self-attention in modeling regular languages and hierarchical structure. It showed that transformers cannot model periodic regular languages or basic recursion, either with hard or soft attention, and even if infinite precision is allowed. This entails that self-attention cannot in general emulate stacks or general finite-state automata. The results theoretically confirm the idea that self-attention, by avoiding recurrence, has quite limited computational power.
