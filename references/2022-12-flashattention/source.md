# FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness

**Authors:** Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Re
**Affiliations:** Tri Dao, Daniel Y. Fu, Stefano Ermon, Christopher Re (Department of Computer Science, Stanford University); Atri Rudra (Department of Computer Science and Engineering, University at Buffalo, SUNY)

## Publication Status

- **arXiv preprint:** May 2022, arXiv:2205.14135
- **Peer-reviewed:** Yes
- **Conference:** Advances in Neural Information Processing Systems 35 (NeurIPS 2022), pp. 16344-16359, New Orleans, LA, USA, November 28 - December 9, 2022
- **Status:** Published conference paper

## Preferred Citation

Cite the NeurIPS 2022 version:

> Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & Re, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. In Advances in Neural Information Processing Systems 35 (NeurIPS 2022), pp. 16344-16359.

## Links

- arXiv: https://arxiv.org/abs/2205.14135
- Proceedings: https://proceedings.neurips.cc/paper_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html
- DOI: https://doi.org/10.5555/3600270.3601459
- Code: https://github.com/Dao-AILab/flash-attention
