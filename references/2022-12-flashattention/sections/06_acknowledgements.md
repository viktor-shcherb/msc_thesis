# Acknowledgements [p. 10–11]

[p. 10–11]

The implementation uses Apex's FMHA code (https://github.com/NVIDIA/apex/tree/master/apex/contrib/csrc/fmha) as a starting point. The authors thank Young-Jun Ko for the in-depth explanation of his FMHA implementation and for his thoughtful answers to questions about CUDA. They thank Sabri Eyuboglu, Megan Leszczynski, Laurel Orr, Yuhuai Wu, Beidi Chen, and Xun Huang for constructive feedback and suggestions on early drafts. They thank Markus Rabe and Charles Staats for helpful discussion of their attention algorithm.

Funding acknowledgements include: NIH (U54EB020405, Mobilize), NSF (CCF1763315, Beyond Sparsity; CCF1563078, Volume to Velocity; 1937301, RTML), ARL (W911NF-21-2-0251, Interactive Human-AI Teaming), ONR (N000141712266, Unifying Weak Supervision; N00014-20-1-2480, Understanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275, NEPTUNE), NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP & HAI-Azure Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI), Department of Defense (DoD) through NDSEG Fellowship Program, and members of the Stanford DAWN project: Facebook, Google, and VMWare. Atri Rudra's research is supported by NSF grant CCF-1763481.
