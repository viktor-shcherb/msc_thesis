# Ring Attention with Blockwise Transformers for Near-Infinite Context

**Authors:** Hao Liu, Matei Zaharia, Pieter Abbeel
**Affiliations:** UC Berkeley, Databricks

## Publication Status

- **arXiv preprint:** October 2023, arXiv:2310.01889
- **Peer-reviewed:** Yes
- **Conference/Journal:** International Conference on Learning Representations (ICLR) 2024, Vienna, Austria, May 7--11, 2024
- **Status:** Published conference paper

## Preferred Citation

Cite the ICLR 2024 version:

> Liu, H., Zaharia, M., & Abbeel, P. (2024). Ring Attention with Blockwise Transformers for Near-Infinite Context. In *International Conference on Learning Representations (ICLR)*.

## Links

- arXiv: https://arxiv.org/abs/2310.01889
- OpenReview: https://openreview.net/forum?id=WsRHpHH4s0
- Proceedings: https://proceedings.iclr.cc/paper_files/paper/2024/file/1119587863e78451f080da2a768c4935-Paper-Conference.pdf
- Code: https://github.com/haoliuhl/ringattention
