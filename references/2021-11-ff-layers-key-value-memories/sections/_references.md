# References

Only references cited in the section notes are listed below.

---

**Baevski and Auli (2019)**
Alexei Baevski and Michael Auli. 2019. Adaptive input representations for neural language modeling. In *International Conference on Learning Representations (ICLR)*.
Cited in 03_keys-capture-input-patterns.md (model used in experiments), 04_values-represent-distributions.md (adaptive softmax footnote), 10_appendix-b-implementation-details.md (model details).

**Brown et al. (2020)**
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In *Proceedings of Neural Information Processing Systems (NeurIPS)*.
Cited in 01_introduction.md as part of state-of-the-art NLP motivation.

**Clark et al. (2019)**
Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. What does BERT look at? An analysis of BERT's attention. In *BlackBoxNLP Workshop at ACL*.
Cited in 01_introduction.md and 06_related-work.md as prior work on analyzing self-attention layers.

**Dalvi et al. (2019)**
Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan Belinkov, Anthony Bau, and James Glass. 2019. What is one grain of sand in the desert? analyzing individual neurons in deep nlp models. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 33, pages 6309-6317.
Cited in 06_related-work.md as prior work on neuron functionality.

**Devlin et al. (2019)**
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In *North American Association for Computational Linguistics (NAACL)*, pages 4171-4186, Minneapolis, Minnesota.
Cited in 01_introduction.md as part of state-of-the-art NLP motivation.

**Durrani et al. (2020)**
Nadir Durrani, Hassan Sajjad, Fahim Dalvi, and Yonatan Belinkov. 2020. Analyzing individual neurons in pre-trained language models. In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*.
Cited in 06_related-work.md as prior work on neuron functionality.

**Erhan et al. (2009)**
Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2009. Visualizing higher-layer features of a deep network. *University of Montreal*, 1341(3):1.
Cited in 06_related-work.md as prior work on characterizing memory cells via maximal activations in vision.

**Han et al. (2020)**
Xiaochuang Han, Byron C. Wallace, and Yulia Tsvetkov. 2020. Explaining black box predictions and unveiling data artifacts through influence functions. In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*, pages 5553-5563.
Cited in 06_related-work.md as related interpretability method.

**Jacovi et al. (2018)**
Alon Jacovi, Oren Sar Shalom, and Yoav Goldberg. 2018. Understanding convolutional neural networks for text classification. In *Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP*, pages 56-65, Brussels, Belgium.
Cited in 06_related-work.md for showing CNNs extract key n-grams from inputs.

**Jawahar et al. (2019)**
Ganesh Jawahar, Benoit Sagot, and Djame Seddah. 2019. What does BERT learn about the structure of language? In *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*, pages 3651-3657, Florence, Italy.
Cited in 03_keys-capture-input-patterns.md and 06_related-work.md as prior work on inter-layer differences and shallow/semantic features.

**Liu et al. (2019)**
Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. 2019. Linguistic knowledge and transferability of contextual representations. In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages 1073-1094, Minneapolis, Minnesota.
Cited in 03_keys-capture-input-patterns.md as corroborating that lower layers encode shallow features and upper layers encode semantic features.

**Merity et al. (2017)**
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models. *International Conference on Learning Representations (ICLR)*.
Cited in 03_keys-capture-input-patterns.md (WikiText-103 dataset) and 10_appendix-b-implementation-details.md.

**Mu and Andreas (2020)**
Jesse Mu and Jacob Andreas. 2020. Compositional explanations of neurons. In *Proceedings of Neural Information Processing Systems (NeurIPS)*.
Cited in 06_related-work.md as prior work on neuron functionality.

**Nasr et al. (2019)**
Milad Nasr, Reza Shokri, and Amir Houmansadr. 2019. Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. In *2019 IEEE Symposium on Security and Privacy (SP)*, pages 739-753.
Cited in 07_discussion-and-conclusion.md as motivation for practical implications of memory cell analysis (white-box membership inference).

**Peters et al. (2018)**
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In *North American Chapter of the Association for Computational Linguistics (NAACL)*.
Cited in 03_keys-capture-input-patterns.md as corroborating shallow/semantic layer findings.

**Press et al. (2020)**
Ofir Press, Noah A. Smith, and Omer Levy. 2020. Improving transformer models by reordering their sublayers. In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*, pages 2996-3005.
Cited in 06_related-work.md as prior work highlighting importance of feed-forward layers.

**Pulugundla et al. (2021)**
Bhargav Pulugundla, Yang Gao, Brian King, Gokce Keskin, Harish Mallidi, Minhua Wu, Jasha Droppo, and Roland Maas. 2021. Attention-based neural beamforming layers for multi-channel speech recognition. *arXiv preprint arXiv:2105.05920*.
Cited in 06_related-work.md as prior work highlighting importance of feed-forward layers.

**Rethmeier et al. (2020)**
Nils Rethmeier, Vageesh Kumar Saxena, and Isabelle Augenstein. 2020. Tx-ray: Quantifying and explaining model-knowledge transfer in (un-) supervised nlp. In *Conference on Uncertainty in Artificial Intelligence*, pages 440-449. PMLR.
Cited in 06_related-work.md as prior work on neuron functionality and characterizing memory cells via maximal activations in NLP.

**Sukhbaatar et al. (2015)**
S. Sukhbaatar, J. Weston, and R. Fergus. 2015. End-to-end memory networks. In *Advances in Neural Information Processing Systems (NIPS)*.
Cited in 01_introduction.md and 02_ff-layers-as-key-value-memories.md as the original neural memory formulation.

**Sukhbaatar et al. (2019)**
Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou, and Armand Joulin. 2019. Augmenting self-attention with persistent memory. *arXiv preprint arXiv:1907.01470*.
Cited in 01_introduction.md and 02_ff-layers-as-key-value-memories.md as prior work making the analogous observation about FF layers and memory, and incorporating FF parameters as persistent memory cells.

**Tenney et al. (2019)**
Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT rediscovers the classical NLP pipeline. In *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*, pages 4593-4601, Florence, Italy.
Cited in 06_related-work.md as prior work on inter-layer differences.

**Vaswani et al. (2017)**
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In *Advances in Neural Information Processing Systems (NIPS)*, pages 5998-6008.
Cited in 01_introduction.md and 02_ff-layers-as-key-value-memories.md as the original transformer paper.

**Vig and Belinkov (2019)**
Jesse Vig and Yonatan Belinkov. 2019. Analyzing the structure of attention in a transformer language model. In *Proceedings of the 2019 ACL Workshop BlackBoxNLP: Analyzing and Interpreting Neural Networks for NLP*, pages 63-76, Florence, Italy.
Cited in 01_introduction.md and 06_related-work.md as prior work on analyzing self-attention layers.

**Vig et al. (2020)**
Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. 2020. Investigating gender bias in language models using causal mediation analysis. *Advances in Neural Information Processing Systems*, 33.
Cited in 06_related-work.md as prior work on neuron functionality.

**Voita et al. (2019)**
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*.
Cited in 01_introduction.md and 06_related-work.md as prior work on analyzing self-attention layers.

**Wiegreffe and Pinter (2019)**
Sarah Wiegreffe and Yuval Pinter. 2019. Attention is not not explanation. In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*, pages 11-20, Hong Kong, China.
Cited in 06_related-work.md as related interpretability method.

**Xu et al. (2020)**
Hongfei Xu, Qiuhui Liu, Deyi Xiong, and Josef van Genabith. 2020. Transformer with depth-wise lstm. *arXiv preprint arXiv:2007.06257*.
Cited in 06_related-work.md as prior work highlighting importance of feed-forward layers.
