# Transformer Feed-Forward Layers Are Key-Value Memories

**Authors:** Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy
**Affiliations:** Blavatnik School of Computer Science, Tel-Aviv University; Allen Institute for Artificial Intelligence; Cornell Tech

## Publication Status

- **arXiv preprint:** December 2020, arXiv:2012.14913
- **Peer-reviewed:** Yes
- **Conference:** Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP 2021), pages 5484--5495, Online and Punta Cana, Dominican Republic, November 7--11, 2021
- **Status:** Published conference paper

## Preferred Citation

Cite the EMNLP 2021 version:

> Geva, M., Schuster, R., Berant, J., & Levy, O. (2021). Transformer Feed-Forward Layers Are Key-Value Memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5484--5495. Association for Computational Linguistics.

## Links

- arXiv: https://arxiv.org/abs/2012.14913
- ACL Anthology: https://aclanthology.org/2021.emnlp-main.446/
- Code: https://github.com/mega002/ff-layers
