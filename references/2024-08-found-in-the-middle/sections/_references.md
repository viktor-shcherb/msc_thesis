# References

Only references actually cited in the section notes are included below.

## Anil et al., 2023
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. *arXiv preprint arXiv:2305.10403*.
- Cited in 01_introduction.md as an example of LLMs enabled by effective prompting.

## Asai et al., 2023
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. *arXiv preprint arXiv:2310.11511*.
- Cited in 01_introduction.md as an application requiring retrieval (fact checking) and as a RAG application.

## Bahdanau et al., 2015
Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In *3rd International Conference on Learning Representations, ICLR 2015*.
- Cited in 05_related-work.md as introducing the attention mechanism in RNN-based encoder-decoder architectures.

## Bertsch et al., 2023
Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R Gormley. 2023. Unlimiformer: Long-range transformers with unlimited length input. *arXiv preprint arXiv:2305.01625*.
- Cited in 05_related-work.md as an inference-time method that extends LLM context length.

## Borgeaud et al., 2021
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. 2021. Improving language models by retrieving from trillions of tokens. *arXiv preprint arXiv:2112.04426*.
- Cited in 05_related-work.md as a RAG framework reference.

## Borgeaud et al., 2022
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022. Improving language models by retrieving from trillions of tokens. In *International conference on machine learning*, pages 2206-2240. PMLR.
- Cited in 01_introduction.md as a RAG reference.

## Brown et al., 2020
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. *Advances in neural information processing systems*, 33:1877-1901.
- Cited in 01_introduction.md as an example of LLMs.

## Chen et al., 2023
Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. 2023. Walking down the memory maze: Beyond context limit through interactive reading. *arXiv preprint arXiv:2310.05029*.
- Cited in 05_related-work.md as a method that splits long inputs into shorter contexts.

## Clark et al., 2019
Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. 2019. What does bert look at? An analysis of bert's attention. *arXiv preprint arXiv:1906.04341*.
- Cited in 05_related-work.md as using self-attention to understand model behaviors.

## Dao et al., 2022
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. *Advances in Neural Information Processing Systems*, 35:16344-16359.
- Cited in 05_related-work.md as efficient training scheme for longer contexts.

## Devlin et al., 2018
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. *arXiv preprint arXiv:1810.04805*.
- Cited in 05_related-work.md as transformers achieving state-of-the-art in various domains.

## Dong et al., 2021
Yue Dong, Chandra Bhagavatula, Ximing Lu, Jena D Hwang, Antoine Bosselut, Jackie Chi Kit Cheung, and Yejin Choi. 2021. On-the-fly attention modulation for neural generation. *arXiv preprint arXiv:2101.00371*.
- Cited in 02_positional-attention-bias.md noting that attention weights correlate with model generations.

## Dosovitskiy et al., 2020
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. *arXiv preprint arXiv:2010.11929*.
- Cited in 05_related-work.md as transformers achieving state-of-the-art in various domains.

## Gemini Team, 2023
Gemini Team. 2023. Gemini: A family of highly capable multimodal models. *arXiv preprint arXiv:2312.11805*.
- Cited in 01_introduction.md and 05_related-work.md as an example of strong LLM capabilities.

## Hao et al., 2021
Yaru Hao, Li Dong, Furu Wei, and Ke Xu. 2021. Self-attention attribution: Interpreting information interactions inside transformer. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 35, pages 12963-12971.
- Cited in 05_related-work.md as using self-attention to understand model behaviors.

## He et al., 2023
Junqing He, Kunhao Pan, Dong Xiaoqun, Song Zhuoyang, Liu Yibo, Liang Yuxin, Wang Hao, Sun Qianguo, Zhang Songxin, Xie Zejian, et al. 2023. Never lost in the middle: Improving large language models via attention strengthening question answering. *arXiv e-prints*, arXiv:2311.09198.
- Cited in 05_related-work.md as constructing a dataset for training LLMs to focus on relevant documents among long contexts.
- Note: This is the same paper as "Junqing et al., 2023" cited elsewhere in the notes; the paper uses both citation forms.

## Hsieh et al., 2023
Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, Ranjay Krishna, and Tomas Pfister. 2023. Tool documentation enables zero-shot tool-usage with large language models. *arXiv preprint arXiv:2308.00675*.
- Cited in 01_introduction.md as an application of LLMs (tool usage).

## Izacard and Grave, 2021
Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In *Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume*, pages 874-880. Association for Computational Linguistics.
- Cited in 01_introduction.md and 05_related-work.md as an open-domain QA and RAG reference.

## Izacard et al., 2022a
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022a. Unsupervised dense information retrieval with contrastive learning. *Transactions on Machine Learning Research*.
- Cited in 05_related-work.md and 07_appendix-a.md as a RAG framework reference and as the Contriever retriever used for NaturalQuestions distractor passages.

## Izacard et al., 2022b
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022b. Few-shot learning with retrieval augmented language models. *arXiv preprint arXiv:2208.03299*.
- Cited in 01_introduction.md and 05_related-work.md as a RAG reference.

## Ji et al., 2023
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. *ACM Computing Surveys*, 55(12):1-38.
- Cited in 01_introduction.md in context of applications requiring retrieval from external knowledge.

## Jiang et al., 2023
Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. *ArXiv preprint*, abs/2310.06839.
- Cited in 01_introduction.md, 04_improving-long-context.md, and 05_related-work.md as a re-ranking/reordering method (LongLLMLingua).

## Karpukhin et al., 2020
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages 6769-6781. Association for Computational Linguistics.
- Cited in 01_introduction.md as a retrieval method for external knowledge sources and as re-ranking reference.

## Khandelwal et al., 2020
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In *International Conference on Learning Representations*.
- Cited in 01_introduction.md and 05_related-work.md as a RAG reference.

## Kwiatkowski et al., 2019
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. *Transactions of the Association for Computational Linguistics*, 7:453-466.
- Cited in 01_introduction.md, 02_positional-attention-bias.md, 03_found-in-the-middle.md, 04_improving-long-context.md, and 07_appendix-a.md as the NaturalQuestion benchmark used in experiments.

## Lee et al., 2019
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. *arXiv preprint arXiv:1906.00300*.
- Cited in 01_introduction.md as collaborative writing application.

## Lewis et al., 2020
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. *Advances in Neural Information Processing Systems*, 33:9459-9474.
- Cited in 05_related-work.md as a foundational RAG reference.

## Li et al., 2023a
Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. 2023a. How long can open-source llms truly promise on context length?
- Cited in 01_introduction.md, 02_positional-attention-bias.md, 04_improving-long-context.md, and 05_related-work.md as the source of the Vicuna model and as documenting lost-in-the-middle in long-context models.

## Li et al., 2023b
Dacheng Li, Rulin Shao, Anze Xie, Eric P Xing, Joseph E Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. 2023b. Lightseq: Sequence level parallelism for distributed training of long context transformers. *arXiv preprint arXiv:2310.03294*.
- Cited in 05_related-work.md as an efficient training scheme for longer contexts.

## Liang et al., 2023
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, et al. 2023. Holistic evaluation of language models. *Transactions on Machine Learning Research*.
- Cited in 04_improving-long-context.md as a prompt reordering baseline method.

## Liu et al., 2023
Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts. *arXiv preprint arXiv:2307.03172*.
- Cited in 01_introduction.md, 02_positional-attention-bias.md, 05_related-work.md, and 07_appendix-a.md as the original work documenting the lost-in-the-middle phenomenon.

## Luong et al., 2015
Minh-Thang Luong, Hieu Pham, and Christopher D Manning. 2015. Effective approaches to attention-based neural machine translation. In *Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing*, pages 1412-1421.
- Cited in 05_related-work.md as introducing attention in RNN-based encoder-decoder architectures.

## Min et al., 2023
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In *EMNLP*.
- Cited in 01_introduction.md as search and summarization requiring retrieval.

## Min et al., 2024
Sewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah A. Smith, and Luke Zettlemoyer. 2024. SILO language models: Isolating legal risk in a nonparametric datastore. In *ICLR*.
- Cited in 01_introduction.md as search and summarization application.

## OpenAI, 2022
OpenAI. 2022. Chatgpt.
- Cited in 01_introduction.md and 05_related-work.md as an example of LLM capabilities.

## Petroni et al., 2020
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, et al. 2020. Kilt: a benchmark for knowledge intensive language tasks. *arXiv preprint arXiv:2009.02252*.
- Cited in 05_related-work.md noting LLMs struggle with knowledge intensive tasks.

## Peysakhovich and Lerer, 2023
Alexander Peysakhovich and Adam Lerer. 2023. Attention sorting combats recency bias in long context language models. *arXiv preprint arXiv:2310.01427*.
- Cited in 01_introduction.md, 03_found-in-the-middle.md, 04_improving-long-context.md, 05_related-work.md, and 07_appendix-a.md as the attention sorting method and SynthWiki dataset source.

## Press et al., 2021
Ofir Press, Noah Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input length extrapolation. In *International Conference on Learning Representations*.
- Cited in 05_related-work.md as an inference-time method for extending context length.

## Ratner et al., 2023
Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. Parallel context windows for large language models. In *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 6383-6402. Association for Computational Linguistics.
- Cited in 05_related-work.md as an inference-time method for extending context length.

## Ravaut et al., 2023
Mathieu Ravaut, Shafiq Joty, Aixin Sun, and Nancy F Chen. 2023. On position bias in summarization with large language models. *arXiv preprint arXiv:2310.10570*.
- Cited in 01_introduction.md noting that models preferentially use information from beginning and end of prompts.

## Shen et al., 2023
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. In *Advances in Neural Information Processing Systems*.
- Cited in 05_related-work.md as an application of RAG (automatic task completion).

## Shi et al., 2023a
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Scharli, and Denny Zhou. 2023a. Large language models can be easily distracted by irrelevant context. In *International Conference on Machine Learning*, pages 31210-31227. PMLR.
- Cited in 02_positional-attention-bias.md as suggesting distractors may mislead the model.

## Shi et al., 2023b
Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Victoria Lin, Noah A Smith, Luke Zettlemoyer, Scott Yih, and Mike Lewis. 2023b. In-context pretraining: Language modeling beyond document boundaries. *arXiv preprint arXiv:2310.10638*.
- Cited in 05_related-work.md as efficient training scheme for longer contexts.

## Shi et al., 2023c
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023c. Replug: Retrieval-augmented black-box language models. *arXiv preprint arXiv:2301.12652*.
- Cited in 01_introduction.md as re-ranking requiring additional supervision or finetuning.

## Sun et al., 2023
Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. 2023. Is ChatGPT good at search? investigating large language models as re-ranking agents. In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages 14918-14937. Association for Computational Linguistics.
- Cited in 03_found-in-the-middle.md and 04_improving-long-context.md as query generation and relevance generation baseline methods, and as prompt reordering baseline.

## Thoppilan et al., 2022
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. *arXiv preprint arXiv:2201.08239*.
- Cited in 01_introduction.md as conversational interfaces (chatbots).

## Touvron et al., 2023
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*.
- Cited in 01_introduction.md and 05_related-work.md as an example of LLMs and decoder-only LLMs exhibiting lost-in-the-middle.

## Vashishth et al., 2019
Shikhar Vashishth, Shyam Upadhyay, Gaurav Singh Tomar, and Manaal Faruqui. 2019. Attention interpretability across nlp tasks. *arXiv preprint arXiv:1909.11218*.
- Cited in 05_related-work.md as using self-attention to understand model behaviors.

## Vaswani et al., 2017
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. *Advances in neural information processing systems*, 30.
- Cited in 05_related-work.md as introducing the self-attention mechanism and transformer architecture.

## Wang et al., 2023
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. 2023. How far can camels go? exploring the state of instruction tuning on open resources. *Advances in Neural Information Processing Systems*, 36:74764-74786.
- Cited in 04_improving-long-context.md as the source of the Tulu-2-7b model used in experiments.

## Wang et al., 2024
Zilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin Eisenschlos, Vincent Perot, Zifeng Wang, Lesly Miculicich, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee, and Tomas Pfister. 2024. Chain-of-table: Evolving tables in the reasoning chain for table understanding. In *International Conference on Learning Representations*.
- Cited in 01_introduction.md as structured tables retrieval.

## Xiao et al., 2023
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023. Efficient streaming language models with attention sinks. *arXiv*.
- Cited in 05_related-work.md as an inference-time method for extending context length.

## Xiong et al., 2023
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. 2023. Effective long-context scaling of foundation models. *arXiv preprint arXiv:2309.16039*.
- Cited in 01_introduction.md and 02_positional-attention-bias.md noting long-context LLM capabilities.

## Xu et al., 2023a
Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. 2023a. Retrieval meets long context large language models. *arXiv preprint arXiv:2310.03025*.
- Cited in 01_introduction.md as a RAG reference.

## Xu et al., 2023b
Xiaohan Xu, Chongyang Tao, Tao Shen, Can Xu, Hongbo Xu, Guodong Long, and Jian-guang Lou. 2023b. Re-reading improves reasoning in language models. *arXiv preprint arXiv:2309.06275*.
- Cited in 02_positional-attention-bias.md noting the question is repeated before and after documents.

## Zhang et al., 2023
Qingru Zhang, Chandan Singh, Liyuan Liu, Xiaodong Liu, Bin Yu, Jianfeng Gao, and Tuo Zhao. 2023. Tell your model where to attend: Post-hoc attention steering for llms. *arXiv preprint arXiv:2311.02262*.
- Cited in 02_positional-attention-bias.md and 08_appendix-b.md noting that attention weights correlate with model generations, and as future direction for finding the best attention heads to intervene.
