# 1 Introduction [p. 1-2]

[p. 1] Effective prompting of large language models (LLMs) (Brown et al., 2020; Anil et al., 2023; Touvron et al., 2023) has enabled a variety of user-facing applications, including conversational interfaces (chatbots) (Thoppilan et al., 2022), search and summarization (Min et al., 2024), open-domain question answering (Izacard and Grave, 2021), tool usage (Hsieh et al., 2023), fact checking (Asai et al., 2023), and collaborative writing (Lee et al., 2019). Some of these applications, such as search and summarization (Ji et al., 2023; Min et al., 2023; Asai et al., 2023), require retrieving information from external knowledge sources. Retrieval-augmented generation (RAG) fetches relevant documents (e.g. structured tables (Wang et al., 2024) and API documentation (Karpukhin et al., 2020)) from external knowledge sources and makes them available in the LLMs' input prompt (Khandelwal et al., 2020; Borgeaud et al., 2022; Izacard et al., 2022b; Xu et al., 2023a).

[p. 2] Despite the widespread utility of RAG (Li et al., 2023a; Xiong et al., 2023; OpenAI, 2022; Gemini Team, 2023), recent experiments highlight a striking deficiency: LLMs struggle to locate relevant documents when they are placed in the middle of their input prompts (Liu et al., 2023; Li et al., 2023a). They call this the *lost-in-the-middle* phenomenon.

To overcome this phenomenon, a few mechanistic strategies have been proposed (Jiang et al., 2023; Peysakhovich and Lerer, 2023). These methods *re-rank* the relevance of different documents and *re-order* the most relevant ones to either the beginning or end of the input context. Unfortunately, re-ranking usually requires additional supervision or dedicated finetuning for performant RAG performance (Karpukhin, 2020; Shi et al., 2023c; Sun et al., 2023). Worse, re-ranking methods do not fundamentally improve LLMs' ability to utilize and capture relevant information from the provided input contexts. The underlying causes of this behavior remains unclear, even though it has been observed across multiple decoder-only LLMs (Touvron et al., 2023; Li et al., 2023a; OpenAI, 2022).

## Three contributions

The paper makes three contributions:

**Contribution 1 (understanding the cause):** The authors establish a connection between lost-in-the-middle and LLMs' intrinsic attention bias (see Figure 1). They find that models often demonstrate a *U-shaped* attention distribution, with higher attention values assigned to the beginning and end of the input prompt. This correlates with the U-shaped RAG performance observed in prior literature (Liu et al., 2023). This focus on beginning and end also extends to content utilization: models preferentially use information from the beginning and end of their prompts (Ravaut et al., 2023; Peysakhovich and Lerer, 2023). The authors hypothesize that the positional attention bias may contribute to the phenomenon, wherein the bias could lead to over-reliance on content at the beginning/end of the input, regardless of its true relevance.

**Contribution 2 (calibration mechanism):** The authors propose a mechanism to disentangle positional bias from model's attention. They first estimate this bias through measuring the change in attention as they vary the relative position of a fixed context in the LLM's prompt. By quantifying and then removing this bias from the attention scores for a given query, they obtain the *calibrated attention* scores across the retrieved documents. This calibrated attention proves to be better correlated to the ground truth relevance of the document to a user query. In open-domain question answering tasks (Kwiatkowski et al., 2019), the proposed calibrated attention outperforms popular existing approaches for ranking the relevance of retrieved documents (up to 48 Recall@3 points). This finding challenges the recent belief that LLMs struggle to capture relevant context embedded in the middle of inputs, suggesting they may indeed be capable of doing so, but are only hindered by the overwhelming positional bias.

**Contribution 3 (found-in-the-middle):** The authors operationalize their calibration mechanism as a solution for this phenomenon, naming their attention intervention *found-in-the-middle*.

> "We show that calibrating the attention leads to improvements across two popular LLMs with different context window lengths on two RAG tasks." [p. 2]

Experiments demonstrate improvements over standard model generation by up to 15 percentage points on NaturalQuestion dataset (Kwiatkowski et al., 2019). The authors hope the work opens up future directions in understanding LLM's attention biases and their effect on downstream tasks.

**Figure 1** (p. 1): "(a) Lost-in-the-middle refers to models' U-shape RAG performance as the relevant context's (e.g., a gold document containing the answer to a query) position varies within the input; (b) We observe models exhibit U-shape attention weights favoring leading and ending contexts, regardless of their actual contents; (c) Models do attend to relevant contexts even when placed in the middle, but are eventually distracted by leading/ending contexts; (d) We propose a calibration mechanism, found-in-the-middle, that disentangles the effect of U-shape attention bias and allows models to attend to relevant context regardless their positions."

The figure contains four subplots: (a) shows a U-shaped curve of accuracy (y-axis) vs. gold document position in prompt (x-axis), with performance highest when the gold document is at the beginning or end. (b) shows a U-shaped curve of model attention (y-axis) vs. position in prompt (x-axis), with attention highest at beginning and end positions regardless of content. (c) shows model attention vs. position in prompt with a gold document marked in the middle -- the gold document receives some elevated attention but is still overwhelmed by positional bias from leading/ending contexts. (d) shows the "surface" after calibration (subtracting (c) minus (b)), where the gold document's attention now stands out clearly as a peak in the middle.
