# 6 Discussion [p. 9]

[p. 9] In this work, the authors understand and address the lost-in-the-middle phenomenon, by establishing a connection between the phenomenon and models' positional attention bias. They mitigate the bias by attention calibration which directly modifies the model's attention mechanism, enabling LLMs to more faithfully attend to contexts based on their relevance, rather than their position. Experiments show that attention calibration improves the performance compared to its uncalibrated counterpart especially when relevant context occurs in the middle of the input. They additionally show attention calibration can be applied on top of existing reordering pipelines to further improve models' performance.

## Limitations

**Simplification of the mechanism behind positional attention bias.** [p. 9] The authors proposed a simple hypothesis to model the positional attention bias, as shown in Eq. 1. However, the intrinsic mechanisms that drive this bias could be more intricate and dynamic than their current model accounts for. It is possible that some aspects of attention bias are learnable or adaptive, responding to subtle aspects of the data or training process that their current approach does not consider.

**Computational overhead.** [p. 9] The method of calibrating positional attention bias, while effective, introduces additional computational overhead. Specifically, they require extra $O(K)$ model forward passes to calibrate attention at each position, compared to vanilla model generation. However, in this study they aim to discover and calibrate the positional attention bias from a scientific perspective. They expect that their discovery can enable future research into developing more calibration methods with lower computational overhead.

**Positional attention bias may be beneficial.** [p. 9] The method aims to completely remove positional attention bias. However, it is important to note that this positional bias might actually be beneficial in certain contexts. In some specific tasks or scenarios, the natural tendency of models to focus more on the beginning and end of inputs could align well with the structure of the task or the nature of the data. Therefore, understanding the tasks and the applications is required before adopting their proposed calibration method.

**The root cause of attention bias is unclear.** [p. 9] The authors aim to discover and understand the connection between the lost-in-the-middle problem and LLMs' intrinsic attention bias. However, their work does not definitively pinpoint the root cause of attention bias in LLMs. The cause of such a bias could be attributed to the distribution of pretraining corpora, the transformer model architecture, and the optimization process. Future research needs to delve deeper into the origins of this phenomenon.

## Ethical Statement

[p. 9] The research focuses on enhancing the performance of large language models using existing public datasets, ensuring that no personal or sensitive data was collected or utilized. The attention calibration method is aimed at improving the efficiency and accuracy of retrieval-augmented generation, with potential benefits across various domains including search engines, question-answering systems, and other text-based applications. The authors note that as their technique builds upon pre-trained language models, it may inadvertently inherit and propagate existing biases inherent in these models. Apart from this significant concern, they do not identify any other immediate risks arising from the methodologies or findings presented in the paper.
