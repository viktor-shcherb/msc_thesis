# Hacker News Discussion â€” Mistral AI Announce 7B v0.2 Base Model Release

**URL:** https://news.ycombinator.com/item?id=39802932
**Type:** other
**Fetched:** 2026-02-15
**Priority:** supplementary

## Discussion Context

This Hacker News thread discusses the March 2024 release of Mistral 7B v0.2 base model weights, which occurred three months after the instruction-tuned variant was released in December 2023.

## Main Discussion Points

### Non-MoE Model Release

A user questioned Mistral's decision to release a non-MoE (Mixture of Experts) model:

> "Interesting that they aren't releasing a model with MoE this time given the success mixtral had."

### Community Response

The response highlighted practical reasoning for the base model release. One commenter explained that:
- Non-MoE variants offer substantial utility for many applications
- Innovations discovered while training smaller models could inform future MoE iterations

## Limited Technical Discussion

The thread notably lacked:
- Detailed technical comparisons between v0.1 and v0.2
- Explanations for why base weights were released after the instruct variant
- Performance benchmarks (participants acknowledged these were not yet available)

The community expected that Mistral would publish comparative metrics soon.

## Overall Tone

The overall tone reflected cautious interest rather than extensive technical debate. The community viewed this as a potentially valuable addition to the model ecosystem despite the departure from the MoE approach that had gained traction with Mixtral.

## Why Base Released After Instruct

The discussion does not provide clear answers to the question of why the base model weights were released three months after the instruction-tuned variant. This appears to have been a business/release strategy decision by Mistral AI that was not publicly explained in this forum or elsewhere in the captured sources.
