# 2 Related Work [p. 2]

## 2.1 Long Context Language Models

[p. 2] Feeding long context leads to bottlenecks in language model training and inference due to computational resources. Some community efforts focus on developing efficient attention mechanisms to build efficient language models (Sun et al., 2023; Ding et al., 2023; Li et al., 2023b; Fu et al., 2023; Peng et al., 2023a). In addition to optimizing the attention mechanism, some works (Bulatov et al., 2023; Dai et al., 2019; Mohtashami & Jaggi, 2023) focus on chunking the input to model both the current text in the chunk and the previous context states, effectively extending the length of context processing.

Besides the efficiency challenge, the scalability of positional embedding is also crucial. ALiBi (Press et al., 2022) and xPos (Sun et al., 2022) emphasize the significance of local context to enhance the language model's ability to perform extrapolation. Moreover, position interpolation (PI) (Chen et al., 2023) and NTK-aware (LocalLLaMA, 2023b, a) are the most popular approaches based on RoPE (Su et al., 2022) to efficiently and effectively extend the context length. However, these works mainly validated their methods with perplexity (PPL) (Sun et al., 2021; LocalLLaMA, 2023b), and there has not been systematic validation on practical tasks.

## 2.2 Long Sequences Benchmarks

[p. 2-3] Tay et al. (2020) introduce the Long Range Arena (LRA), a benchmark encompassing five distinct classification tasks. CAB (Zhang et al., 2023) is another benchmark for different efficient attention designs by comparing both efficiency and accuracy. In the language domain, previous work on LCLMs tends to report PPL to evaluate language models (Su et al., 2022; Peng et al., 2023b) on longer context. However, PPL may not usually correlate with the actual performance (Sun et al., 2021). ZeroScrolls (Shaham et al., 2022; 2023) and LongBench (Bai et al., 2023) are concurrent long context evaluation suites. L-Eval differs from them in 3 aspects: (1) Manually selected samples. Testing samples are automatically filtered by their benchmarks, while those for L-Eval are manually filtered. (2) Standardized metrics. The authors are the first to investigate the correlations between traditional lexical metrics and recently proposed LLM metrics with human judgment on long context settings. L-Eval no longer mainly relies on N-gram metrics. (3) More closed-ended tasks. Due to fairness issues in open-ended tasks, L-Eval has more closed-ended tasks reflecting unbiased results. [p. 3]
