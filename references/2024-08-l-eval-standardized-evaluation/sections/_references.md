# References

Only references actually cited in the section notes are listed below.

## Angelidis et al., 2021
Stefanos Angelidis, Reinald Kim Amplayo, Yoshihiko Suhara, Xiaolan Wang, and Mirella Lapata. Extractive opinion summarization in quantized transformer spaces. *Transactions of the Association for Computational Linguistics*, 9:277–293, 2021.
- Cited in 03_dataset-construction.md and 08_appendix-b.md as the source of the SPACE dataset for aspect-based review summarization.

## Bai et al., 2023
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: A bilingual, multitask benchmark for long context understanding, 2023.
- Cited in 02_related-work.md as a concurrent long context evaluation suite.

## Bulatov et al., 2023
Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Scaling transformer to 1m tokens and beyond with rmt, 2023.
- Cited in 02_related-work.md as a chunking-based approach for extending context processing.

## Chen et al., 2022
Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. Summscreen: A dataset for abstractive screenplay summarization, 2022.
- Cited in 03_dataset-construction.md and 08_appendix-b.md as the source of the SummScreen dataset.

## Chen et al., 2023
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation, 2023.
- Cited in 02_related-work.md as position interpolation (PI), a popular approach based on RoPE to extend context length.

## Chung et al., 2018
Yu-An Chung, Hung-Yi Lee, and James Glass. Supervised and unsupervised transfer learning for question answering. In *NAACL HLT*, 2018.
- Cited in 08_appendix-b.md as a source of the TOEFL-QA data used in the TOFEL task.

## Chiang et al., 2023
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.
- Cited in 05_benchmarking-llms.md and 07_appendix.md as the source of Vicuna1.3.

## Cobbe et al., 2021
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math problems, 2021.
- Cited in 03_dataset-construction.md and 08_appendix-b.md as the source of the GSM8k dataset.

## Dai et al., 2019
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*, pp. 2978–2988, Florence, Italy, July 2019.
- Cited in 02_related-work.md as a chunking-based approach for context extension.

## Dao et al., 2022
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. In *Advances in Neural Information Processing Systems*, volume 35, pp. 16344–16359, 2022.
- Cited in 01_introduction.md and 05_benchmarking-llms.md as an efficient attention mechanism used for all experiments.

## Dasigi et al., 2021
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A dataset of information-seeking questions and answers anchored in research papers, 2021.
- Cited in 03_dataset-construction.md and 08_appendix-b.md as the source of the Qasper dataset.

## Ding et al., 2023
Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens, 2023.
- Cited in 01_introduction.md and 02_related-work.md as an efficient architecture for extending context length.

## Du et al., 2022
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pp. 320–335, 2022.
- Cited in 01_introduction.md, 05_benchmarking-llms.md, and 07_appendix.md as the source of Chatglm.

## Dubois et al., 2023
Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods to learn from human feedback, 2023.
- Cited in 01_introduction.md and 04_evaluation-metrics.md as prior work on LLM-based evaluation.

## Fabbri et al., 2019
Alexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir R. Radev. Multi-news: a large-scale multi-document summarization dataset and abstractive hierarchical model, 2019.
- Cited in 03_dataset-construction.md and 08_appendix-b.md as the source of the Multi-News dataset.

## Feng et al., 2021
Song Feng, Siva Sankalp Patel, Hui Wan, and Sachindra Joshi. MultiDoc2dial: Modeling dialogues grounded in multiple documents. In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*, 2021.
- Cited in 03_dataset-construction.md and 08_appendix-b.md as the source of the MultiDoc2Dial dataset.

## Fu et al., 2023
Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In *The Eleventh International Conference on Learning Representations*, 2023.
- Cited in 02_related-work.md as an efficient attention mechanism approach.

## Hendrycks et al., 2021a
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021a.
- Cited in 01_introduction.md as an existing multi-task benchmark for short-prompt LMs.

## Hendrycks et al., 2021b
Dan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. Cuad: An expert-annotated nlp dataset for legal contract review, 2021b.
- Cited in 03_dataset-construction.md and 08_appendix-b.md as the source of the CUAD dataset.

## Huang et al., 2021
Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. In *Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pp. 1419–1436, 2021.
- Cited in 03_dataset-construction.md and 08_appendix-b.md as the source of the GovReport dataset.

## Kocisky et al., 2017
Tomas Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge, 2017.
- Cited in 03_dataset-construction.md and 08_appendix-b.md as the source of the NarrativeQA dataset.

## Kwiatkowski et al., 2019
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. *Transactions of the Association for Computational Linguistics*, 7:452–466, 2019.
- Cited in 03_dataset-construction.md and 08_appendix-b.md as the source of the NQ dataset.

## Li et al., 2023a
Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can open-source llms truly promise on context length?, June 2023a.
- Cited in 01_introduction.md, 03_dataset-construction.md, 07_appendix.md, and 08_appendix-b.md as the source of Longchat and the first topic retrieval task.

## Li et al., 2023b
Mukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jun Zhang, Zhiyong Wu, and Lingpeng Kong. In-context learning with many demonstration examples, 2023b.
- Cited in 01_introduction.md and 02_related-work.md for longer-input training approaches and as showing positive effects of more in-context examples.

## Li et al., 2023c
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models, 2023c.
- Cited in 01_introduction.md and 04_evaluation-metrics.md as prior work on LLM-based evaluation in short context settings.

## Liang et al., 2023
Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system. *arXiv preprint arXiv:2304.13343*, 2023.
- Cited in 01_introduction.md as an efficient architecture approach.

## Lin, 2004
Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In *Text Summarization Branches Out*, pp. 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics.
- Cited in 01_introduction.md as the source of the ROUGE metric.

## Liu et al., 2023
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts, 2023.
- Cited in 03_dataset-construction.md and 08_appendix-b.md regarding retrieval tasks for testing long context dependency ("lost in the middle").

## LocalLLaMA, 2023a
LocalLLaMA. Dynamically scaled rope further increases performance of long context llama with zero fine-tuning, July 2023a. URL https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/
- Cited in 02_related-work.md and 07_appendix.md as an NTK-aware approach based on RoPE (dynamic NTK).

## LocalLLaMA, 2023b
LocalLLaMA. Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation., June 2023b. URL https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/
- Cited in 01_introduction.md, 02_related-work.md, 05_benchmarking-llms.md, and 07_appendix.md as a popular scaled positional embedding technique (NTK-aware RoPE).

## Mohtashami & Jaggi, 2023
Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers. *arXiv preprint arXiv:2305.16300*, 2023.
- Cited in 01_introduction.md and 02_related-work.md for chunking-based context extension.

## Nijkamp et al., 2023
Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech Kryscinski, Lidiya Murakhovs'ka, Prafulla Kumar Choubey, Alex Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq Rayhan Joty, and Caiming Xiong. Long sequence modeling with xgen: A 7b llm trained on 8k input sequence length. Salesforce AI Research Blog, 2023.
- Cited in 05_benchmarking-llms.md as the source of XGen-8k-inst.

## Pang et al., 2022
Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel R. Bowman. Quality: Question answering with long input texts, yes!, 2022.
- Cited in 03_dataset-construction.md and 08_appendix-b.md as the source of QuALITY.

## Peng et al., 2023a
Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind, Stanisilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. Rwkv: Reinventing rnns for the transformer era, 2023a.
- Cited in 02_related-work.md for efficient attention mechanisms.

## Peng et al., 2023b
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models, 2023b.
- Cited in 02_related-work.md for PPL evaluation on longer context.

## Press et al., 2022
Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In *International Conference on Learning Representations*, 2022.
- Cited in 02_related-work.md as the source of ALiBi.

## Qin et al., 2023
Zhen Qin, Weixuan Sun, Kaiyue Lu, Hui Deng, Dongxu Li, Xiaodong Han, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Linearized relative positional encoding. *CoRR*, abs/2307.09270, 2023.
- Cited in 01_introduction.md for scaled positional embedding.

## Shaham et al., 2022
Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. Scrolls: Standardized comparison over long language sequences, 2022.
- Cited in 01_introduction.md and 02_related-work.md as a long context evaluation suite (Scrolls).

## Shaham et al., 2023
Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. Zeroscrolls: A zero-shot benchmark for long text understanding, 2023.
- Cited in 01_introduction.md and 02_related-work.md as a concurrent long context evaluation suite (ZeroScrolls).

## Sharma et al., 2019
Eva Sharma, Chen Li, and Lu Wang. BIGPATENT: A large-scale dataset for abstractive and coherent summarization. In *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*, pp. 2204–2213, Florence, Italy, July 2019.
- Cited in 03_dataset-construction.md and 08_appendix-b.md as the source of BigPatent.

## Su et al., 2022
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2022.
- Cited in 01_introduction.md and 02_related-work.md as the source of RoPE.

## Sun et al., 2021
Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language models actually use long-range context? In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*, pp. 807–822, 2021.
- Cited in 02_related-work.md regarding PPL not correlating with actual performance.

## Sun et al., 2022
Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer, 2022.
- Cited in 01_introduction.md, 02_related-work.md, and 05_benchmarking-llms.md regarding xPos and scaled positional embedding.

## Sun et al., 2023
Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models, 2023.
- Cited in 02_related-work.md for efficient attention mechanisms.

## Suzgun et al., 2022
Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them, 2022.
- Cited in 01_introduction.md as an existing multi-task benchmark.

## Tay et al., 2020
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers, 2020.
- Cited in 02_related-work.md as the source of Long Range Arena (LRA).

## Touvron et al., 2023a
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023a.
- Cited in 05_benchmarking-llms.md and 07_appendix.md as the source of the Llama model family.

## Touvron et al., 2023b
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b.
- Cited in 07_appendix.md as the source of Llama2 and Llama2-chat.

## Tseng et al., 2016
Bo-Hsiang Tseng, Sheng-Syun Shen, Hung-Yi Lee, and Lin-Shan Lee. Towards machine comprehension of spoken content: Initial toefl listening comprehension test by machine. In *INTERSPEECH*, 2016.
- Cited in 03_dataset-construction.md and 08_appendix-b.md as the source of the TOEFL dataset.

## Wang et al., 2023
Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun Xi, Siyuan Cheng, Kangwei Liu, Guozhou Zheng, and Huajun Chen. Easyedit: An easy-to-use knowledge editing framework for large language models, 2023.
- Cited in 08_appendix-b.md regarding parametric knowledge in LLMs in the SFiction task description.

## Yuan et al., 2021
Weizhe Yuan, Pengfei Liu, and Graham Neubig. Can we automate scientific reviewing?, 2021.
- Cited in 08_appendix-b.md as the source of processed PDF files used for the OpenReview task.

## Zhang et al., 2023
Jun Zhang, Shuyang Jiang, Jiangtao Feng, Lin Zheng, and Lingpeng Kong. Cab: Comprehensive attention benchmarking on long sequence modeling, 2023.
- Cited in 01_introduction.md, 02_related-work.md, and 03_dataset-construction.md; CAB benchmark and previous evaluation suites.

## Zheng et al., 2023
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
- Cited in 01_introduction.md, 04_evaluation-metrics.md, and 05_benchmarking-llms.md for LLM-based evaluation and as the source of Vicuna1.5-16k.

## Zhong et al., 2021
Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. Qmsum: A new benchmark for query-based multi-domain meeting summarization, 2021.
- Cited in 03_dataset-construction.md and 08_appendix-b.md as the source of QMSum.
