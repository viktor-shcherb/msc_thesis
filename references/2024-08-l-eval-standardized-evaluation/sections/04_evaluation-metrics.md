# 4 Towards Standardized Long Context Evaluation Metrics [p. 4-6]

[p. 4-5] This section presents various evaluation metrics for text generation, including exam evaluation for closed-ended tasks and different levels of open-ended evaluation, most of which are reference-based metrics. Experiments are also conducted to study the correlation between automated metrics and human scoring.

**Exam evaluation** -- This is designed for closed-ended tasks, i.e., multiple-choice questions. The evaluation metric used for these tasks follows the exact match format (accuracy %), similar to grading exam papers. Each question's score is calculated as 100 divided by the number of questions. [p. 5]

**Human evaluation** -- This is the most accurate evaluation for open-ended tasks. Despite that some works show GPT-4 can be coherent with human judgment, LLMs cannot replace human evaluation. Human evaluators score the outputs on a scale of 1 to 5, which signifies from poor output to excellent output. To save human laboratories, a subset used for the human evaluation is proposed which has 12 long documents with 85 open-ended questions (**85-question subset**). [p. 5]

**Large language model judges for evaluating LCLMs** -- In short context settings, evaluation using LLMs is the most accurate metric for automatically evaluating models on open-ended tasks (Zheng et al., 2023; Li et al., 2023c; Dubois et al., 2023). These works assume the LLM evaluator is a "super model", but this assumption does not hold in long context settings because it is impossible to feed the entire lengthy inputs into LLMs like GPT-4. Unlike short context evaluation, GPT-4 is unable to infer the ground truth answer itself. Consequently, evaluation results mainly depend on the reference answer and user questions. In L-Eval, the pair-wise battle format is adopted and Turbo-16k-0613 is selected as the base model, reporting the *win-rate vs. Turbo-16k-0613 %* which means how many samples can beat Turbo-16k. Two LLM judges are studied: GPT-4 and GPT-3.5 in the experiment section. LLM evaluators have been reported to favor more detailed and lengthy answers (Zheng et al., 2023). This bias becomes more pronounced in long context settings as the invisible input makes it difficult for the judge to accurately determine the correctness of specific details and information. Therefore, the judgment model must bear in mind that details not corroborated by the reference answers should not be considered beneficial. The judgment prompt is enhanced with: > "Additional details or information that are not mentioned in the reference answer cannot be considered as advantages and do not let them sway your judgment." [p. 5]

If only a portion of tasks needs evaluation, using LLM judges is recommended. Verifying the 1000+ open-ended questions via GPT-4 is unaffordable. A subset for GPT-4 evaluation consisting of 17 diverse long documents with 96 open-ended questions (**96-question subset**) is manually split. Testing the 4 datasets in Table 2 needs about $100. Evaluating outputs from the 96-question subset with GPT-4 only needs about $5. [p. 6]

**N-gram matching evaluation** -- Considering that assessing all tasks is still expensive for human/LLM evaluators, L-Eval also takes into account n-gram metrics. N-gram metrics like ROUGE-L (R-L) and F-1 score are widely used in traditional datasets and they are also widely adopted in the text generation benchmarks via performing lexical matching. It is worth noting that n-gram matching metrics are very sensitive to the length of the ground truth, exhibiting a length bias. The related analysis is in Section 4.1. [p. 6]

## 4.1 Length Instruction Enhanced Long Context Evaluation

[p. 6] In preliminary experiments, LLMs tend to generate very long responses bringing obstacles for the reference-based evaluation (see Delta-L in Table 2). This length bias results in a significant influence on the n-gram metrics. For instance, Claude-100k only achieves a 9.84 F-1 score due to undesired output length.

In L-Eval, the authors argue that long context language models should further focus on more accurate content rather than accurate length. Practically, issues about undesired generation length can be easily solved by prompting the model. **Length-Instruction-Enhanced** (LIE) evaluation is first adopted in LLMs evaluation benchmarks which is simple but effective in overcoming the length bias, i.e., the number of words of ground truth is directly exposed to LCLMs. LIE evaluation in this work is implemented by injecting the model with the desired length into the original instruction (e.g., [Origin Instruction]: *Please summarize the opinions of the professor.* [Length Instruction]: *We need a 50-word summary*, where 50 is the number of words in the reference answer). The results of Claude-100k in Table 2 demonstrate a substantial improvement in terms of the F-1 score: there is a near **50-point gap** depending on whether or not the model generates with the expected length. [p. 6]

**Experimental validation** -- To validate the LIE evaluation, a human evaluation on the 85-question subset is conducted. Three annotators verify 7 models and calculate the Kendall-Tau correlation coefficient (tau) between these metrics and the average human score. The main results are shown in Figure 2 (Blue bar) and experimental settings are in Section A.2. Results indicate that all these automatic metrics (except GPT-4) **fail to correlate** to human judgment. Compared with N-gram metrics, LLM judges are more accurate and robust to output length. As can be seen from Figure 2, the improvements brought by length instruction are marked with yellow, and after adding the length instructions, tau has been improved from 0.5 to 0.8 for ROUGE-L and tau of GPT-4 evaluator has even reached to 1. In Figure 3, the scores are converted to rankings (the best one is 5 and the worst is 1). [p. 6]

**Figure 2** (p. 6): "Kendall-Tau correlation coefficient of different automatic metrics with the average human score."
Bar chart with automatic metrics on the x-axis (ROUGE-L, F-1, GPT-3.5, GPT-4) and Kendall-Tau Correlation coefficient on the y-axis (0.4 to 1.0). Two series: "origin" (green/blue) and "length instruction" (yellow/orange). Origin values: ROUGE-L ~0.52, F-1 ~0.52, GPT-3.5 ~0.62, GPT-4 ~0.81. With length instruction: ROUGE-L ~0.81, F-1 ~0.76, GPT-3.5 ~0.76, GPT-4 ~1.0. The text states that tau improved from 0.5 to 0.8 for ROUGE-L and tau of GPT-4 reached 1.0 with length instruction. This demonstrates that LIE substantially improves correlation with human judgment for all metrics.

**Figure 3** (p. 6): "The ranking of six models under various evaluation metrics (Human-avg, Human-1, GPT-4, GPT-3.5, R-L, and F-1) with or without length instruction. Human-avg represents the average score from human evaluation, and Human-1 signifies the score given by the first human annotator."
Two radar/spider plots side-by-side: (a) Without Length Instruction, (b) With Length Instruction. Six axes correspond to the metrics. Six models are shown: Claude-100k, Turbo-16k, Turbo-4k, Llama2-13b-e, Llama2-7b-e, Vicuna-7b-16k. Without length instruction, the radar shapes of different metrics are inconsistent -- e.g., ROUGE-L and F-1 rankings diverge from human rankings. With length instruction, the radar shapes across all metrics converge more closely to the human rankings, demonstrating the effectiveness of LIE evaluation.

---
[p. 7 continued]

[p. 7] The scores of six models are shown under 6 different evaluation systems. Figure 3 (a) shows the results given by metrics without length instruction. These hexagons are often distorted because these metrics usually cannot achieve good correlation. When comparing the models enhanced with length instruction in (b), the hexagons become more regular. [p. 7]

**Table 2** (p. 7): "Results on 2 open-ended summarization and 2 abstractive QA tasks. **GPT-4** means the win-rate with Turbo-16k using GPT-4 as the judge. ΔL means the difference of generated answer length with ground truth length. The best results are underlined. Results in red mean decoding in a desired length makes a big difference in performance."

|                           | SPACE |       |      | QMSum |       |      | NQ    |       |      | NrtvQA |       |      |
|---------------------------|-------|-------|------|-------|-------|------|-------|-------|------|--------|-------|------|
| Model                     | R-L   | GPT-4 | ΔL   | R-L   | GPT-4 | ΔL   | F-1   | GPT-4 | ΔL   | F-1    | GPT-4 | ΔL   |
| Claude-100k               | 15.43 | 45.65 | **165** | 14.04 | 58.77 | 183  | 9.84  | 56.19 | **135** | 10.39  | 68.96 | **127** |
| + Length Instruction       | 18.61 | 61.40 | 27   | 18.13 | 58.89 | 22   | 57.76 | 51.00 | 1    | 19.09  | 57.77 | 0    |
| Chatglm2-32k              | 17.56 | 24.13 | -23  | 20.06 | 38.84 | **287** | 31.45 | 33.71 | 3    | 12.24  | 34.67 | 74   |
| + Length Instruction       | 16.61 | 17.11 | 11   | 20.83 | 33.75 | 9    | 37.94 | 33.71 | -1   | 14.00  | 34.52 | -2   |
| Longchat-7b-16k           | 15.10 | 15.61 | **120** | 9.31  | 25.56 | 40   | 8.83  | 32.33 | **105** | 8.36   | 31.80 | 83   |
| + Length Instruction       | 17.06 | 36.23 | -3   | 13.21 | 30.20 | 70   | 20.21 | 35.00 | 37   | 15.17  | 43.38 | 40   |
| Llama2-13b-chat           | 16.83 | 32.46 | **102** | 14.72 | 30.79 | **116** | 8.29  | 38.99 | **90** | 7.20   | 30.69 | **130** |
| + Length Instruction       | 19.23 | 43.15 | -7   | 19.65 | 34.82 | -1   | 35.43 | 41.07 | 6    | 13.48  | 45.07 | 14   |
