# 1 Introduction [p. 1-2]

[p. 1] Currently, significant effort is being dedicated to extending the context length of large language models. Popular solutions mainly involve further pretraining or finetuning standard models on longer inputs using more efficient architectures (Ding et al., 2023; Dao et al., 2022; Liang et al., 2023; Mohtashami & Jaggi, 2023; Li et al., 2023b), as well as scaled positional embedding (Su et al., 2022; Sun et al., 2022; LocalLLaMA, 2023b; Qin et al., 2023).

There are extensive multi-task benchmarks (Hendrycks et al., 2021a; Suzgun et al., 2022) for language models with short prompts, yet a high-quality one in long context modeling has not yet been established. Meanwhile, almost all previous long-sequence text generation benchmarks relied primarily on n-gram matching metrics (Zhang et al., 2023; Shaham et al., 2022), such as ROUGE (Lin, 2004). Whether these commonly used metrics correlate well with human judgment when testing LCLMs in a zero-shot setting remains a question. Furthermore, the open-source community has released a considerable number of language models with 16k, or 32k context length (Li et al., 2023a; Du et al., 2022). A comprehensive comparative study of these models can be of great value.

To address these issues, the authors propose *L-Eval* to call for a more standardized evaluation of long context language models. For dataset construction, L-Eval has 20 sub-tasks: 4 sub-tasks are annotated from scratch (Section 3.1), 4 sub-tasks are re-annotated from public datasets (Section 3.2), and the remaining 12 sub-tasks are manually cleaned from previous long sequence datasets. Tasks are divided into two groups: closed-ended tasks and open-ended tasks. The closed-ended group primarily tests the reasoning and understanding ability regarding a longer context, and the open-ended group consists of more summarization tasks that require aggregation of long document information. [p. 1-2]

[p. 2] In the design of L-Eval, diversity and quality are prioritized over quantity, ensuring correctness by manually validating all samples after data collection (Section 3.3). Data diversity, indicative in question styles, domain selection, and input lengths, is detailed in Table 1.

The authors study the limitations of traditional metrics based on lexical matching, demonstrating that these metrics often fail to correlate with human evaluation results. Further experiments suggest that LLM judges (Li et al., 2023c; Zheng et al., 2023) provide superior accuracy in the evaluation of open-ended tasks. Section 4 explains how a short-context LLM judge is set in a long-context evaluation setting. Considering the influence of generation length on performance and to avoid drawing misleading conclusions, the authors propose the Length-Instruction-Enhanced (LIE) evaluation technique for all reference-based metrics, including those employing an LLM judger. The empirical results demonstrate a substantial improvement brought by LIE evaluation in the Kendall-Tau correlation coefficient (tau) with human judgments (Figure 2), for all automatic metrics.

A comprehensive study with 16 different LLMs (Section 5.1) was conducted in L-Eval. Key findings summarized:

1. There is still a significant gap between open-source LCLMs and commercial models, for both closed-ended tasks (Table 3) and open-ended tasks evaluated by LLMs and human (Table 4, 5). However, this gap is not accurately reflected by n-gram metrics.
2. While current efforts on open-source LCLMs improve performance on closed-ended tasks, they significantly fall short on open-ended tasks. This is largely due to the models' misunderstanding of instructions as the input context length increases.
3. Experiments on GPT-3.5-Turbo with both dense and sparse retrievers show that end-to-end full-context models outperform traditional retrieval-based systems.
4. Training-free scaled positional embeddings can enhance the retrieval capability of LLMs over longer input, while it may adversely affect their reasoning ability.

More interesting conclusions can be found in Section 5.2 and Section A.3. The authors hope *L-Eval* and the findings contribute to a deeper understanding of current LCLM research and the further development of models and evaluation metrics. [p. 2]
