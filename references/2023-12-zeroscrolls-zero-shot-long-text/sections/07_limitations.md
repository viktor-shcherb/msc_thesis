# 7 Limitations [p. 9–10]

[p. 9–10] As language models improve, evaluating them presents a growing challenge given their ability to consistently generate coherent and reasonable text, which is harder to score, even with gold references at hand. Specifically in the zero-shot setting, where models must infer the output format from the prompt, ROUGE and F1 (ngram metrics) can assign low scores for semantically equivalent generations, with different word choices or answer lengths. Additionally, to conduct fair evaluation, the authors use common prompt templates across models for every task, while model-specific prompts, as well as chain-of-thought prompting may improve model performance on this benchmark. Finally, the state of the art is a moving target, and as the authors write these lines new long-range models, alignment methods, decoding algorithms, and prompting techniques become available; they invite researchers to evaluate their ideas on the ZeroSCROLLS leaderboard. [p. 9–10]
