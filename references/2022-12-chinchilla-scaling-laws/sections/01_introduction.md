# 1. Introduction [p. 1-2]

[p. 1] Recently a series of Large Language Models (LLMs) have been introduced (Brown et al., 2020; Lieber et al., 2021; Rae et al., 2021; Smith et al., 2022; Thoppilan et al., 2022), with the largest dense language models now having over 500 billion parameters. These large autoregressive transformers (Vaswani et al., 2017) have demonstrated impressive performance on many tasks using a variety of evaluation protocols such as zero-shot, few-shot, and fine-tuning.

The compute and energy cost for training large language models is substantial (Rae et al., 2021; Thoppilan et al., 2022) and rises with increasing model size. In practice, the allocated training compute budget is often known in advance: how many accelerators are available and for how long we want to use them. Since it is typically only feasible to train these large models once, accurately estimating the best model hyperparameters for a given compute budget is critical (Tay et al., 2021).

Kaplan et al. (2020) showed that there is a power law relationship between the number of parameters in an autoregressive language model (LM) and its performance. As a result, the field has been training larger and larger models, expecting performance improvements. One notable conclusion in Kaplan et al. (2020) is that large models should not be trained to their lowest possible loss to be compute optimal. The authors reach the same conclusion but estimate that large models should be trained for many more training tokens than recommended by Kaplan et al. Specifically, given a 10x increase computational budget, Kaplan et al. suggests that the size of the model should increase 5.5x while the number of training tokens should only increase 1.8x. Instead, the authors find that model size and the number of training tokens should be scaled in equal proportions.

[p. 1-2] Following Kaplan et al. (2020) and the training setup of GPT-3 (Brown et al., 2020), many of the recently trained large models have been trained for approximately 300 billion tokens (Table 1), in line with the approach of predominantly increasing model size when increasing compute.

**Figure 1** (p. 2): "Overlaid predictions. We overlay the predictions from our three different approaches, along with projections from Kaplan et al. (2020). We find that all three methods predict that current large models should be substantially smaller and therefore trained much longer than is currently done. In Figure A3, we show the results with the predicted optimal tokens plotted against the optimal number of parameters for fixed FLOP budgets. Chinchilla outperforms Gopher and the other large models (see Section 4.2)."

The figure is a log-log plot with FLOPs on the x-axis (from 10^17 to 10^25) and Parameters on the y-axis (from 10M to 1T). It shows four lines representing the predicted optimal model size as a function of FLOPs: Approach 1 (blue), Approach 2 (orange), Approach 3 (green), and Kaplan et al. (2020) (black dashed). All three new approaches predict substantially smaller optimal model sizes than Kaplan et al. (2020) for a given FLOP budget. Specific models are marked: Chinchilla (70B), Gopher (280B), GPT-3 (175B), and Megatron-Turing NLG (530B). The existing large models all lie well above the optimal-size curves from the three new approaches, indicating they are oversized (and undertrained) for their compute budgets.

[p. 2] The paper revisits the question: *Given a fixed FLOPs budget, how should one trade-off model size and the number of training tokens?* The final pre-training loss is modelled as L(N, D) as a function of the number of model parameters N and the number of training tokens D. Since the computational budget C is a deterministic function FLOPs(N, D) of the number of seen training tokens and model parameters, the interest is in minimizing L under the constraint FLOPs(N, D) = C:

N_opt(C), D_opt(C) = argmin_{N, D s.t. FLOPs(N, D) = C} L(N, D).  (1)

The functions N_opt(C) and D_opt(C) describe the optimal allocation of a computational budget C. These functions are empirically estimated based on the losses of over 400 models, ranging from under 70M to over 16B parameters, and trained on 5B to over 400B tokens -- with each model configuration trained for several different training horizons. The approach leads to considerably different results than that of Kaplan et al. (2020).

Based on the estimated compute-optimal frontier, the authors predict that for the compute budget used to train *Gopher*, an optimal model should be 4 times smaller, while being training on 4 times more tokens. This is verified by training a more *compute-optimal* 70B model, called *Chinchilla*, on 1.4 trillion tokens. Not only does *Chinchilla* outperform its much larger counterpart, *Gopher*, but its reduced model size reduces inference cost considerably and greatly facilitates downstream uses on smaller hardware. The energy cost of a large language model is amortized through its usage for inference and fine-tuning. The benefits of a more optimally trained smaller model, therefore, extend beyond the immediate benefits of its improved performance.

**Footnotes:**
- Footnote 1: "For example, knowing the number of accelerators and a target training duration." [p. 2]
- Footnote 2: "For simplicity, we perform our analysis on the smoothed training loss which is an unbiased estimate of the test loss, as we are in the infinite data regime (the number of training tokens is less than the number of tokens in the entire corpus)." [p. 2]
