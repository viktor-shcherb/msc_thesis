# 5. Discussion & Conclusion [p. 15-16]

[p. 15] The trend so far in large language model training has been to increase the model size, often without increasing the number of training tokens. The largest dense transformer, MT-NLG 530B, is now over 3x larger than GPT-3's 170 billion parameters from just two years ago. However, this model, as well as the majority of existing large models, have all been trained for a comparable number of tokens -- around 300 billion. While the desire to train these mega-models has led to substantial engineering innovation, the authors hypothesize that the race to train larger and larger models is resulting in models that are substantially underperforming compared to what could be achieved with the same compute budget.

Three predictive approaches towards optimally setting model size and training duration are proposed, based on the outcome of over 400 training runs. All three approaches predict that *Gopher* is substantially over-sized and estimate that for the same compute budget a smaller model trained on more data will perform better. This hypothesis is directly tested by training *Chinchilla*, a 70B parameter model, and showing that it outperforms *Gopher* and even larger models on nearly every measured evaluation task.

[p. 15-16] Whilst the method allows predictions on how to scale large models when given additional compute, there are several limitations:

- Due to the cost of training large models, only two comparable training runs at large scale (*Chinchilla* and *Gopher*) are available, and there are no additional tests at intermediate scales.
- The assumption that the efficient computational frontier can be described by a power-law relationship between the compute budget, model size, and number of training tokens. However, some concavity is observed in log (*N_opt*) at high compute budgets (see Appendix E). This suggests that the optimal size of large models may still be overestimated.
- The training runs for the analysis have all been trained on less than an epoch of data; future work may consider the multiple epoch regime.

Despite these limitations, the comparison of *Chinchilla* to *Gopher* validates the performance predictions, showing that training a better (and more lightweight) model at the same compute budget has been enabled.

[p. 16] Though there has been significant recent work allowing larger and larger models to be trained, the analysis suggests an increased focus on dataset scaling is needed. Speculatively, scaling to larger and larger datasets is expected to be only beneficial when the data is high-quality. This calls for responsibly collecting larger datasets with a high focus on dataset quality. Larger datasets will require extra care to ensure train-test set overlap is properly accounted for, both in the language modelling loss but also with downstream tasks. Finally, training for trillions of tokens introduces many ethical and privacy concerns. Large datasets scraped from the web will contain toxic language, biases, and private information. With even larger datasets being used, the quantity (if not the frequency) of such information increases, which makes dataset introspection all the more important. *Chinchilla* does suffer from bias and toxicity but interestingly it seems less affected than *Gopher*. Better understanding how performance of large language models and toxicity interact is an important future research question.

While the methodology has been applied towards the training of auto-regressive language models, the authors expect that there is a similar trade-off between model size and the amount of data in other modalities. As training large models is very expensive, choosing the optimal model size and training steps beforehand is essential. The methods proposed are easy to reproduce in new settings.
