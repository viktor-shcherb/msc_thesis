# 1 Introduction [p. 1-3]

Language models have become an important building block in user-facing language technologies, including conversational interfaces, search and summarization, and collaborative writing (Shuster et al., 2022; Thoppilan et al., 2022; Lee et al., 2022, *inter alia*). These models perform downstream tasks primarily via prompting: all relevant task specification and data is formatted as a textual input context, and the model returns a generated text completion. These input contexts can contain thousands of tokens, especially when language models are used to process long documents (e.g., legal or scientific documents, conversation histories, etc.) or when language models are augmented with external information (e.g., relevant documents from a search engine, database query results, etc; Petroni et al., 2020; Ram et al., 2023; Shi et al., 2023; Mallen et al., 2023; Schick et al., 2023, *inter alia*). [p. 1]

Handling these use-cases requires language models to successfully operate over long sequences. Existing language models are generally implemented with Transformers (Vaswani et al., 2017), which require memory and compute that increases quadratically in sequence length. As a result, Transformer language models were often trained with relatively small context windows (between 512-2048 tokens). Recent improvements in hardware (e.g., faster GPUs with more memory) and algorithms (Dai et al., 2019; Dao et al., 2022; Poli et al., 2023; Rubin and Berant, 2023, *inter alia*) have resulted in language models with larger context windows (e.g., 4096, 32K, and even 100K tokens), but it remains unclear how these extended-context language models make use of their input contexts when performing downstream tasks. [p. 1-2]

[p. 2] The authors empirically investigate this question via controlled experiments with a variety of state-of-the-art open (MPT-30B-Instruct, LongChat-13B (16K)) and closed (OpenAI's GPT-3.5-Turbo and Anthropic's Claude-1.3) language models in settings that require accessing and using information within an input context. The experiments make controlled changes to the input context size and the position of the relevant information within the input context and study their effects on language model performance. If language models can robustly use information within long input contexts, then their performance should be *minimally affected* by the position of the relevant information in the input context.

The paper first experiments with multi-document question answering, which requires models to reason over provided documents to find relevant information and use it to answer a given question; this task mimics the retrieval-augmented generation setup underlying many commercial generative search and question answering applications (e.g., Bing Chat). In this setting, the authors control (i) the input context length by changing the number of documents in the input context (akin to retrieving more or less documents in retrieval-augmented generation), and (ii) the position of the relevant information within the input context by changing the order of the documents to place the relevant document at the beginning, middle or end of the context. [p. 2]

Key findings from the introduction:

- Changing the position of relevant information in the input context can substantially affect model performance, indicating that current language models do not robustly access and use information in long input contexts. [p. 2]
- A distinctive U-shaped performance curve is observed (Figure 1): language model performance is highest when relevant information occurs at the very beginning (primacy bias) or end of its input context (recency bias), and performance significantly degrades when models must access and use information in the middle of their input context. [p. 2]
- When relevant information is placed in the middle of its input context, GPT-3.5-Turbo's performance on the multi-document question task is lower than its performance when predicting *without any documents* (i.e., the closed-book setting; 56.1%). [p. 2]
- Models often have identical performance to their extended-context counterparts, indicating that extended-context models are not necessarily better at using their input context. [p. 2]

The authors also study a synthetic key-value retrieval task, designed as a minimal testbed for the basic ability to retrieve matching tokens from the input context. Models are given a collection of JSON-formatted key-value pairs and must return the value associated with a specific key. Although some models perform the synthetic key-value retrieval task perfectly, other models struggle to simply retrieve matching tokens that occur in the middle of their input context and continue to exhibit a U-shaped performance curve. [p. 2]

To better understand why language models struggle to robustly access and use information in their input contexts, the authors study the role of model architecture (decoder-only vs. encoder-decoder), query-aware contextualization, and instruction fine-tuning. Findings: [p. 2]

- Encoder-decoder models are relatively robust to changes in the position of relevant information within their input context, but only when evaluated on sequences within its training-time sequence length. When evaluated on sequences longer than those seen during training, a U-shaped performance curve is observed. [p. 2]
- Query-aware contextualization (placing the query before *and* after the documents or key-value pairs) enables near-perfect performance on the synthetic key-value task, but minimally changes trends in multi-document QA. [p. 2]
- Even base language models (i.e., without instruction fine-tuning) show a U-shaped performance curve as the position of relevant information in the input context is varied. [p. 2-3]

[p. 3] The results indicate that prompting language models with longer input contexts is a trade-off -- providing the language model with more information may help it perform the downstream task, but it also increases the amount of content that the model must reason over, potentially decreasing accuracy. To better understand this trade-off in practice, the authors perform a case study with retriever-reader models on open-domain question answering. In contrast to the controlled multi-document QA task (where the context always contains exactly *one* document that answers the question), none or many of the top *k* documents may contain the answer in the open-domain QA setting. When retrieving from Wikipedia to answer queries from NaturalQuestions-Open, model performance saturates long before retriever recall saturates, indicating that current models fail to effectively use additional retrieved documents -- using 50 documents instead of 20 retrieved documents only marginally improves performance (~1.5% for GPT-3.5-Turbo and ~1% for claude-1.3). [p. 3]

The analysis provides a better understanding of how language models use their input context and introduces new evaluation protocols for future long-context models; to claim that a language model can robustly use information within long input contexts, it is necessary to show that its performance is minimally affected by the position of the relevant information in the input context (e.g., minimal difference in best- and worst-case performance). Code and evaluation data are released.^1 [p. 3]

^1 nelsonliu.me/papers/lost-in-the-middle
