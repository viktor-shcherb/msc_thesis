# 4 Why Are Language Models Not Robust to Changes in the Position of Relevant Information? [p. 7-9]

The multi-document question answering and key-value retrieval results show that language models struggle to robustly access and use information in long input contexts, since performance degrades significantly when changing the position of relevant information. To better understand why, the authors perform preliminary investigations into the role of model architecture (decoder-only vs. encoder-decoder), query-aware contextualization, and instruction fine-tuning. [p. 7]

## 4.1 Effect of Model Architecture [p. 7-8]

The open models evaluated are all decoder-only models -- at each timestep, they may only attend to prior tokens. To better understand the potential effects of model architecture on how language models use context, the authors compare decoder-only and encoder-decoder language models. [p. 7]

The authors experiment with Flan-T5-XXL (Raffel et al., 2020; Chung et al., 2022) and Flan-UL2 (Tay et al., 2023). Flan-T5-XXL is trained with a sequence of 512 tokens (encoder and decoder). Flan-UL2 is initially trained with sequences of 512 tokens (encoder and decoder), but is then pre-trained for an extra 100K steps with 1024 tokens (encoder and decoder) before instruction fine-tuning on sequences with 2048 tokens in the encoder and 512 tokens in the decoder. However, since these models use relative positional embeddings, they can (in principle) extrapolate beyond these maximum context lengths; Shaham et al. (2023) find that both models can perform well with sequences of up to 8K tokens. [p. 7]

Figure 8 compares the performance of decoder-only and encoder-decoder models. When Flan-UL2 is evaluated on sequences within its 2048-token training-time context window (Figure 8; left subplot), its performance is relatively robust to changes in the position of relevant information within the input context (1.9% absolute difference between best- and worst-case performance). When evaluated on settings with sequences longer than 2048 tokens (Figure 8; center and right), Flan-UL2 performance begins to degrade when relevant information is placed in the middle. Flan-T5-XXL shows a similar trend, where longer input contexts result in a greater performance degradation when placing relevant information in the middle of the input context. [p. 7-8]

The authors hypothesize that encoder-decoder models may make better use of their context windows because their bidirectional encoder allows processing each document in the context of future documents, potentially improving relative importance estimation between documents. [p. 8]

**Figure 8** (p. 8): "When encoder-decoder models (Flan-UL2 and Flan-T5-XXL) evaluated on sequences that are *shorter* than their encoder's training-time maximum sequence length (2048 and 512 tokens, respectively), they are relatively robust to changes in the position of relevant information within their input context (left subplot). In contrast, when these models are evaluated on sequences *longer* than those seen during training (center and right subplots), we observe a U-shaped performance curve -- performance is higher when relevant information occurs at the beginning or end of the input context, as opposed to the middle of the input context."

Three panels showing results for 10 (approximately 2K tokens), 20 (approximately 4K tokens), and 30 (approximately 6K tokens) total retrieved documents. Each panel plots accuracy (y-axis, approximately 50-70%) vs. position of document with the answer (x-axis). Four models are shown: mpt-30b-instruct, longchat-13b-16k, flan-t5-xxl, and flan-ul2. In the 10-document setting (left), Flan-UL2 shows a relatively flat curve around 60-65%. In the 20- and 30-document settings (center and right), both encoder-decoder models begin to show U-shaped curves similar to the decoder-only models, though Flan-UL2's curve is less pronounced. MPT-30B-Instruct consistently shows the strongest U-shape.

## 4.2 Effect of Query-Aware Contextualization [p. 8-9]

The multi-document QA and key-value retrieval experiments place the query (i.e., question to answer or key to retrieve) after the data to process (i.e., the documents or the key-value pairs). As a result, decoder-only models cannot attend to query tokens when contextualizing documents or key-value pairs, since the query only appears at the end of the prompt and decoder-only models can only attend to prior tokens at each timestep. In contrast, encoder-decoder models (which seem more robust to changes in the position of relevant information; section 4.1) use a bidirectional encoder to contextualize input contexts -- can this observation be used to improve decoder-only models by placing the query before *and* after the data, enabling query-aware contextualization of documents (or key-value pairs)? [p. 8]

Query-aware contextualization dramatically improves performance on the key-value retrieval task -- all models achieve near-perfect performance on the 75, 140, and 300 key-value pair settings. For example, GPT-3.5-Turbo (16K) with query-aware contextualization achieves perfect performance when evaluated with 300 key-value pairs. In contrast, without query-aware contextualization, the worst-case performance is 45.6% (Figure 7). [p. 8-9]

Despite the significant impact on key-value retrieval performance, query-aware contextualization minimally affects performance trends in the multi-document question answering task (Figure 9); it slightly improves performance when the relevant information is located at the very beginning of the input context, but slightly decreases performance in other settings. [p. 9]

**Figure 9** (p. 8): "Query-aware contextualization (placing the query before *and* after the documents) does not substantially improve robustness of language models to changing the position of relevant information in multi-document QA; performance slightly increases when relevant information occurs at the very beginning, but otherwise slightly decreases."

Single panel showing results for 20 total retrieved documents (approximately 4K tokens, query-aware contextualization). Plots accuracy (y-axis, approximately 50-80%) vs. position of document with the answer (x-axis, 1st to 20th). Six models are shown: claude-1.3, claude-1.3-100k, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613, mpt-30b-instruct, and longchat-13b-16k. The U-shaped pattern persists for most models; query-aware contextualization does not substantially change the overall trends compared to Figure 5.

## 4.3 Effect of Instruction Fine-Tuning [p. 9]

The models evaluated are all instruction fine-tuned -- after their initial pre-training, they undergo supervised fine-tuning on a dataset of instructions and responses. The task specification and/or instruction is commonly placed at the beginning of the input context in supervised instruction fine-tuning data, which might lead instruction fine-tuned language models to place more weight on the start of the input context. To better understand the potential effects of instruction fine-tuning on how language models use long input contexts, the authors compare the multi-document question answering performance of MPT-30B-Instruct against its base model (i.e., before instruction fine-tuning) MPT-30B. The same experimental setup as section 2 is used. [p. 9]

Figure 10 compares the multi-document QA performance of MPT-30B and MPT-30B-Instruct as a function of the position of the relevant information in the input context. Surprisingly, both MPT-30B and MPT-30B-Instruct exhibit a U-shaped performance curve, where performance is highest when relevant information occurs at the very beginning or very end of the context. Although the absolute performance of MPT-30B-Instruct is uniformly higher than that of MPT-30B, their overall performance trends are similar. The authors also observe that instruction fine-tuning slightly reduces the worst-case performance disparity from nearly 10% between the base model best- and worst-case performance to around 4%. [p. 9]

These observations complement prior work, which found that non-instruction fine-tuned language models are biased towards recent tokens (i.e., the end of the input context; Khandelwal et al., 2018; Press et al., 2021). This recency bias has been observed in past work when evaluating models on next-word prediction of contiguous text, a setting where language models minimally benefit from long-range information (Sun et al., 2021). In contrast, the present results show that language models are capable of using longer-range information (i.e., the beginning of the input context) when prompted with instruction-formatted data. The authors hypothesize that non-instruction fine-tuned language models learn to use these long contexts from similarly-formatted data that may occur in Internet text seen during pre-training, e.g., StackOverflow questions and answers. [p. 9]

To better understand the effect of additional fine-tuning and model scale, the authors also experimented with Llama-2 models of varying sizes (7B, 13B, and 70B) with and without additional supervised fine-tuning and reinforcement learning from human feedback (Appendix E). They find that the U-shaped performance curve only appears in sufficiently large language models (with or without additional fine-tuning) -- the 7B Llama-2 models are solely recency biased, while the 13B and 70B models exhibit a U-shaped performance curve. In addition, the Llama-2 supervised fine-tuning and reinforcement learning from human feedback procedure slightly mitigates the positional bias in smaller models (13B, akin to trends shown when comparing MPT-30B and MPT-30B-Instruct), but minimally affects trends on larger models (70B). [p. 9]

**Figure 10** (p. 9): "Multi-document QA performance of MPT-30B-Instruct compared against its base model (i.e., before instruction fine-tuning) MPT-30B. Both models have a U-shaped performance curve, where performance is much higher when relevant information occurs at the start or end of the input context, indicating that the instruction fine-tuning process itself is not necessarily responsible for these performance trends."

Single panel showing results for 20 total retrieved documents (approximately 4K tokens). Plots accuracy (y-axis, approximately 44-56%) vs. position of document with the answer (x-axis, 1st to 20th). Two models are shown: mpt-30b (green solid line) and mpt-30b-instruct (brown dashed line). Both show a U-shaped curve. MPT-30B-Instruct is uniformly higher (approximately 50-54%) than MPT-30B (approximately 44-53%). MPT-30B has a larger gap between best and worst performance (nearly 10%) compared to MPT-30B-Instruct (around 4%).
