# Qwen Technical Report

**Authors:** Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu
**Affiliation:** Alibaba Group (Qwen Team)

## Publication Status

- **arXiv preprint:** September 2023, arXiv:2309.16609
- **Peer-reviewed:** No
- **Conference/Journal:** None
- **Status:** Preprint

Qwen (Qianwen, meaning "thousands of prompts" in Chinese) was released as a technical report alongside model weights in sizes of 1.8B, 7B, and 14B parameters. The report describes the full Qwen model family including base pretrained language models, chat models finetuned with SFT and RLHF, and specialized models for coding (Code-Qwen) and mathematics (Math-Qwen). The 7B and 14B models were open-sourced.

## Preferred Citation

> Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., ... & Zhu, T. (2023). Qwen Technical Report. arXiv:2309.16609.

## Links

- arXiv: https://arxiv.org/abs/2309.16609
- Code: https://github.com/QwenLM/Qwen
- Models: https://huggingface.co/Qwen
