# References

References cited in the section notes, with bibliographic details from the paper's bibliography.

---

**Allal et al., 2023** — Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. "SantaCoder: Don't reach for the stars!" *arXiv preprint arXiv:2301.03988*, 2023.
Cited in 04c_code-evaluation.md and 06_related-work.md as a coding model baseline.

**Almazrouei et al., 2023** — Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, et al. "Falcon-40B: An open large language model with state-of-the-art performance," 2023.
Cited in 02f_experimental-results.md and 06_related-work.md as an open-source LLM baseline (Falcon).

**Anil et al., 2023** — Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, et al. "PaLM 2 technical report." *arXiv preprint arXiv:2305.10403*, 2023.
Cited in 01_introduction.md, 02c_architecture.md, 06_related-work.md as PaLM 2; and 04c_code-evaluation.md for code capabilities.

**Anthropic, 2023a** — Anthropic. "Introducing Claude," 2023a.
Cited in 01_introduction.md and 06_related-work.md as a proprietary AI assistant.

**Anthropic, 2023b** — Anthropic. "Claude 2. Technical report," 2023b.
Cited in 01_introduction.md and 06_related-work.md as a proprietary AI assistant.

**Aribandi et al., 2021** — Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, et al. "ExT5: Towards extreme multi-task scaling for transfer learning." *arXiv preprint arXiv:2111.10952*, 2021.
Cited in 02a_data.md for multi-task instruction data during pretraining.

**Askell et al., 2021** — Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, et al. "A general language assistant for alignment." *arXiv preprint arXiv:2112.00861*, 2021.
Cited in 06_related-work.md for alignment research.

**Austin et al., 2021** — Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, et al. "Program synthesis with large language models." *arXiv preprint arXiv:2108.07732*, 2021.
Cited in 01_introduction.md, 02f_experimental-results.md, 04c_code-evaluation.md, and 06_related-work.md as the MBPP benchmark.

**AutoGPT, 2023** — AutoGPT. "AutoGPT: The heart of the open-source agent ecosystem," 2023.
Cited in 01_introduction.md for agent applications.

**Ba et al., 2016** — Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. "Layer normalization." *CoRR*, abs/1607.06450, 2016.
Cited in 02c_architecture.md as the traditional layer normalization technique replaced by RMSNorm.

**Bai et al., 2022a** — Jinze Bai, Rui Men, Hao Yang, et al. "OFASys: A multi-modal multi-task learning system for building generalist models." *CoRR*, abs/2212.04408, 2022a.
Cited in 01_introduction.md for generalist agent models.

**Bai et al., 2022b** — Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, et al. "Training a helpful and harmless assistant with reinforcement learning from human feedback." *arXiv preprint arXiv:2204.05862*, 2022b.
Cited in 03b_rlhf.md for preference model pretraining (PMP), 06_related-work.md for alignment, and 08a_appendix-training-details.md for human-assistant format from Anthropic.

**Bai et al., 2022c** — Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, et al. "Constitutional AI: Harmlessness from AI feedback." *arXiv preprint arXiv:2212.08073*, 2022c.
Cited in 06_related-work.md for alignment research.

**Bai et al., 2023** — Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. "Qwen-VL: A versatile vision-language model for understanding, localization, text reading, and beyond." *CoRR*, abs/2308.12966, 2023.
Cited in 01_introduction.md and 03d_tool-use-code-interpreter-agent.md for QWEN-VL multimodal model.

**Beltagy et al., 2020** — Iz Beltagy, Matthew E Peters, and Arman Cohan. "Longformer: The long-document transformer." *arXiv preprint arXiv:2004.05150*, 2020.
Cited in 02e_context-length-extension.md for window attention technique.

**Black et al., 2022** — Sid Black, Stella Biderman, Eric Hallahan, et al. "GPT-NeoX-20B: An open-source autoregressive language model." *arXiv preprint arXiv:2204.06745*, 2022.
Cited in 06_related-work.md as an open-source LLM.

**bloc97, 2023** — bloc97. "NTK-aware scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation," 2023.
Cited in 02e_context-length-extension.md for NTK-aware interpolation technique.

**Bommasani et al., 2021** — Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, et al. "On the opportunities and risks of foundation models." *arXiv preprint arXiv:2108.07258*, 2021.
Cited in 06_related-work.md for foundation model concept.

**Brown et al., 2020** — Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, et al. "Language models are few-shot learners." *Advances in Neural Information Processing Systems*, 33:1877-1901, 2020.
Cited in 01_introduction.md as GPT-3, and 02a_data.md for benchmark decontamination approach.

**ChatGLM2 Team, 2023** — ChatGLM2 Team. "ChatGLM2-6B: An open bilingual chat LLM," 2023.
Cited in 02f_experimental-results.md, 03c_evaluation-aligned-models.md, and 06_related-work.md as a baseline model.

**Clark et al., 2018** — Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. "Think you have solved question answering? Try arc, the AI2 reasoning challenge." *CoRR*, abs/1803.05457, 2018.
Cited in 08a_appendix-training-details.md as ARC benchmark.

**Chen et al., 2021** — Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, et al. "Evaluating large language models trained on code." *CoRR*, abs/2107.03374, 2021.
Cited in 01_introduction.md, 02f_experimental-results.md, 03c_evaluation-aligned-models.md, 04c_code-evaluation.md, and 06_related-work.md as Codex / HumanEval benchmark.

**Chen et al., 2023a** — Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. "Extending context window of large language models via positional interpolation." *arXiv preprint arXiv:2306.15595*, 2023a.
Cited in 02e_context-length-extension.md for position interpolation (PI).

**Chen et al., 2023b** — Weize Chen, Yusheng Su, Jingwei Zuo, et al. "Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents." *arXiv preprint arXiv:2308.10848*, 2023b.
Cited in 06_related-work.md for multi-agent interaction.

**Chen et al., 2023c** — Zhihong Chen, Feng Jiang, Junying Chen, et al. "Phoenix: Democratizing ChatGPT across languages." *arXiv preprint arXiv:2304.10453*, 2023c.
Cited in 06_related-work.md for open-source chat data/models.

**Chern et al., 2023a** — Ethan Chern, Haoyang Zou, Xuefeng Li, et al. "Generative ai for math: Abel." 2023a.
Cited in 05b_math-evaluation.md and 06_related-work.md as GAIRMath-Abel math model baseline.

**Chiang & Cholak, 2022** — David Chiang and Peter Cholak. "Overcoming a theoretical limitation of self-attention." In *Proceedings of the 60th Annual Meeting of the ACL (Volume 1: Long Papers)*, pp. 7654-7664, 2022.
Cited in 02e_context-length-extension.md for LogN-Scaling technique.

**Chiang et al., 2023** — Wei-Lin Chiang, Zhuohan Li, Zi Lin, et al. "Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality," March 2023.
Cited in 06_related-work.md as Vicuna open-source chat model.

**Chowdhery et al., 2022** — Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, et al. "PaLM: Scaling language modeling with pathways." *arXiv preprint arXiv:2204.02311*, 2022.
Cited in 01_introduction.md, 02c_architecture.md, 06_related-work.md as PaLM model.

**Christiano et al., 2017** — Paul F. Christiano, Jan Leike, Tom B. Brown, et al. "Deep reinforcement learning from human preferences." In *NeurIPS 2017*, pp. 4299-4307, 2017.
Cited in 03b_rlhf.md for RLHF methodology.

**Chung et al., 2022** — Hyung Won Chung, Le Hou, Shayne Longpre, et al. "Scaling instruction-finetuned language models." *arXiv preprint arXiv:2210.11416*, 2022.
Cited in 06_related-work.md for instruction finetuning.

**Cobbe et al., 2021** — Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, et al. "Training verifiers to solve math word problems." *arXiv preprint arXiv:2110.14168*, 2021.
Cited in 01_introduction.md, 02f_experimental-results.md, 03c_evaluation-aligned-models.md, and 05b_math-evaluation.md as GSM8K benchmark.

**Conneau et al., 2019** — Alexis Conneau, Kartikay Khandelwal, Naman Goyal, et al. "Unsupervised cross-lingual representation learning at scale." *arXiv preprint arXiv:1911.02116*, 2019.
Cited in 02b_tokenization.md as XLM-R tokenizer baseline.

**Conover et al., 2023** — Mike Conover, Matt Hayes, Ankit Mathur, et al. "Free Dolly: Introducing the world's first truly open instruction-tuned LLM," 2023.
Cited in 06_related-work.md as Dolly open-source chat data.

**Dai et al., 2023** — Wenliang Dai, Junnan Li, Dongxu Li, et al. "InstructBLIP: Towards general-purpose vision-language models with instruction tuning." *arXiv preprint arXiv:2305.06500*, 2023.
Cited in 01_introduction.md for multimodal instruction understanding.

**Dao et al., 2022** — Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. "FlashAttention: Fast and memory-efficient exact attention with io-awareness." In *NeurIPS*, 2022.
Cited in 02d_training.md and 04a_code-pretraining.md for Flash Attention technique.

**Dauphin et al., 2017** — Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. "Language modeling with gated convolutional networks." In *ICML*, pp. 933-941, PMLR, 2017.
Cited in 02c_architecture.md for Gated Linear Unit (GLU).

**Dettmers et al., 2022** — Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. "LLM.int8(): 8-bit matrix multiplication for transformers at scale." *arXiv preprint arXiv:2208.07339*, 2022.
Cited in 06_related-work.md for quantization technique used in Q-LoRA.

**Dettmers et al., 2023** — Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. "QLoRA: Efficient finetuning of quantized LLMs." *arXiv preprint arXiv:2305.14314*, 2023.
Cited in 06_related-work.md as Q-LoRA parameter-efficient tuning method, and as Guanaco open-source chat model.

**Devlin et al., 2018** — Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. "BERT: Pre-training of deep bidirectional transformers for language understanding." *arXiv preprint arXiv:1810.04805*, 2018.
Cited in 01_introduction.md and 06_related-work.md for early Transformer pretraining.

**Ding et al., 2023** — Ning Ding, Yulin Chen, Bokai Xu, et al. "Enhancing chat language models by scaling high-quality instructional conversations." *arXiv preprint arXiv:2305.14233*, 2023.
Cited in 06_related-work.md for open-source chat data/models.

**Driess et al., 2023** — Danny Driess, Fei Xia, Mehdi SM Sajjadi, et al. "Palm-e: An embodied multimodal language model." *arXiv preprint arXiv:2303.03378*, 2023.
Cited in 06_related-work.md for embodied multimodal agents.

**Du et al., 2021** — Zhengxiao Du, Yujie Qian, Xiao Liu, et al. "GLM: General language model pretraining with autoregressive blank infilling." *arXiv preprint arXiv:2103.10360*, 2021.
Cited in 06_related-work.md for LLM scaling research.

**Du et al., 2022** — Nan Du, Yanping Huang, Andrew M Dai, et al. "GLaM: Efficient scaling of language models with mixture-of-experts." In *ICML*, pp. 5547-5569, PMLR, 2022.
Cited in 06_related-work.md for MoE-based LLM scaling.

**Ethayarajh et al., 2022** — Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. "Understanding dataset difficulty with V-usable information." In *ICML*, volume 162, pp. 5988-6008, PMLR, 2022.
Cited in 03b_rlhf.md as a human preference benchmark dataset (Stanford SHP).

**Fedus et al., 2022** — William Fedus, Barret Zoph, and Noam Shazeer. "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity." *JMLR*, 23(1):5232-5270, 2022.
Cited in 06_related-work.md for LLM scaling research.

**Frantar et al., 2022** — Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. "GPTQ: Accurate post-training quantization for generative pre-trained transformers." *arXiv preprint arXiv:2210.17323*, 2022.
Cited in 06_related-work.md for quantization used in paged attention.

**Fried et al., 2022** — Daniel Fried, Armen Aghajanyan, Jessy Lin, et al. "Incoder: A generative model for code infilling and synthesis." *ArXiv*, abs/2204.05999, 2022.
Cited in 06_related-work.md as InCoder coding model.

**Google, 2023** — Google. "An important next step on our AI journey," 2023.
Cited in 01_introduction.md for AI assistant capabilities (Bard).

**Hendrycks & Gimpel, 2016** — Dan Hendrycks and Kevin Gimpel. "Bridging nonlinearities and stochastic regularizers with Gaussian error linear units." *CoRR*, abs/1606.08415, 2016.
Cited in 02c_architecture.md as GeLU activation function baseline.

**Hendrycks et al., 2020** — Dan Hendrycks, Collin Burns, Steven Basart, et al. "Measuring massive multitask language understanding." *arXiv preprint arXiv:2009.03300*, 2020.
Cited in 02f_experimental-results.md, 03c_evaluation-aligned-models.md, and 08a_appendix-training-details.md as MMLU benchmark.

**Hendrycks et al., 2021** — Dan Hendrycks, Collin Burns, Saurav Kadavath, et al. "Measuring mathematical problem solving with the math dataset." *arXiv preprint arXiv:2103.03874*, 2021.
Cited in 01_introduction.md, 02f_experimental-results.md, 05b_math-evaluation.md, and 06_related-work.md as MATH benchmark.

**Hoffmann et al., 2022** — Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, et al. "Training compute-optimal large language models." *arXiv preprint arXiv:2203.15556*, 2022.
Cited in 02a_data.md for data scaling importance, 06_related-work.md for LLM scaling and coding capabilities.

**Hong et al., 2023** — Sirui Hong, Xiawu Zheng, Jonathan Chen, et al. "Metagpt: Meta programming for multi-agent collaborative framework." *arXiv preprint arXiv:2308.00352*, 2023.
Cited in 01_introduction.md and 06_related-work.md for multi-agent interaction.

**Hu et al., 2021** — Edward J Hu, Yelong Shen, Phillip Wallis, et al. "LoRA: Low-rank adaptation of large language models." *arXiv preprint arXiv:2106.09685*, 2021.
Cited in 06_related-work.md as LoRA parameter-efficient tuning method.

**Hu et al., 2023** — Chenxu Hu, Jie Fu, Chenzhuang Du, et al. "Chatdb: Augmenting llms with databases as their symbolic memory." *arXiv preprint arXiv:2306.03901*, 2023.
Cited in 06_related-work.md for external memory/knowledge base usage.

**Huang et al., 2023** — Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, et al. "C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models." *arXiv preprint arXiv:2305.08322*, 2023.
Cited in 02f_experimental-results.md, 03c_evaluation-aligned-models.md, 08a_appendix-training-details.md as C-Eval benchmark, and 08b_appendix-human-evaluation.md as a source for human evaluation dataset.

**Hugging Face, 2023** — Hugging Face. "Transformers agents," 2023.
Cited in 03d_tool-use-code-interpreter-agent.md for the Hugging Face Agent framework.

**Inc., 2023a** — Baichuan Inc. "Baichuan-7B: A large-scale 7B pretraining language model," 2023a.
Cited in 02b_tokenization.md as Baichuan tokenizer baseline.

**Inc., 2023b** — XVERSE Technology Inc. "XVERSE-13B: A multilingual large language model," 2023b.
Cited in 02f_experimental-results.md as XVERSE baseline.

**InternLM Team, 2023** — InternLM Team. "InternLM: A multilingual language model with progressively enhanced capabilities," 2023.
Cited in 02b_tokenization.md, 02f_experimental-results.md, 03c_evaluation-aligned-models.md, and 06_related-work.md as InternLM baseline.

**Jain, 2022** — Shantanu Jain. "tiktoken: A fast BPE tokeniser for use with OpenAI's models," 2022.
Cited in 02b_tokenization.md as the starting point for QWEN's tokenizer.

**Ji et al., 2023** — Yunjie Ji, Yong Deng, Yan Gong, et al. "Exploring the impact of instruction data scaling on large language models." *arXiv preprint arXiv:2303.14742*, 2023.
Cited in 06_related-work.md for open-source chat data.

**Jiang et al., 2023** — Zixuan Jiang, Jiaqi Gu, Hanqing Zhu, and David Z. Pan. "Pre-RMSNorm and Pre-CRMSNorm transformers." *CoRR*, abs/2305.14858, 2023.
Cited in 02c_architecture.md for RMSNorm.

**Kingma & Ba, 2014** — Diederik P Kingma and Jimmy Ba. "Adam: A method for stochastic optimization." *arXiv preprint arXiv:1412.6980*, 2014.
Cited in 02d_training.md, 04a_code-pretraining.md, and 04b_code-supervised-fine-tuning.md for AdamW optimizer.

**Kwon et al., 2023** — Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, et al. "Efficient memory management for large language model serving with PagedAttention." In *Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles*, 2023.
Cited in 06_related-work.md for paged attention in Q-LoRA.

**LangChain, Inc., 2023** — LangChain, Inc. "LangChain: Building applications with LLMs through composability," 2023.
Cited in 01_introduction.md and 06_related-work.md for tool-use framework.

**Lepikhin et al., 2020** — Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, et al. "GShard: Scaling giant models with conditional computation and automatic sharding." *arXiv preprint arXiv:2006.16668*, 2020.
Cited in 06_related-work.md for LLM scaling research.

**Lewkowycz et al., 2022** — Aitor Lewkowycz, Anders Andreassen, David Dohan, et al. "Solving quantitative reasoning problems with language models," 2022.
Cited in 05b_math-evaluation.md as Minerva baseline, and 06_related-work.md for math pretraining.

**Li et al., 2022** — Yujia Li, David H. Choi, Junyoung Chung, et al. "Competition-level code generation with AlphaCode." *CoRR*, abs/2203.07814, 2022.
Cited in 04a_code-pretraining.md and 06_related-work.md as AlphaCode coding model.

**Li et al., 2023a** — Chenliang Li, Hehong Chen, Ming Yan, et al. "ModelScope-Agent: Building your customizable agent system with open-source large language models." *arXiv preprint arXiv:2309.00986*, 2023a.
Cited in 06_related-work.md for agents calling different AI models.

**Li et al., 2023b** — Guohao Li, Hasan Abed Al Kader Hammoud, et al. "Camel: Communicative agents for 'mind' exploration of large scale language model society." *arXiv preprint arXiv:2303.17760*, 2023b.
Cited in 06_related-work.md for multi-agent interaction.

**Li et al., 2023c** — Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. "CMMLU: Measuring massive multitask language understanding in Chinese." *arXiv preprint arXiv:2306.09212*, 2023c.
Cited in 08a_appendix-training-details.md as CMMLU benchmark.

**Li et al., 2023d** — Raymond Li, Loubna Ben Allal, Yangtian Zi, et al. "StarCoder: May the source be with you!" *CoRR*, abs/2305.06161, 2023d.
Cited in 04a_code-pretraining.md, 04c_code-evaluation.md, and 06_related-work.md as StarCoder/Starcoder-Base coding model.

**Lightman et al., 2023** — Hunter Lightman, Vineet Kosaraju, Yura Burda, et al. "Let's verify step by step." *arXiv preprint arXiv:2305.20050*, 2023.
Cited in 03b_rlhf.md as a human preference benchmark dataset (OpenAI PRM800K), 05b_math-evaluation.md for MATH test set, and 06_related-work.md for math pretraining.

**Liu & Wan, 2021** — Chenxiao Liu and Xiaojun Wan. "CodeQA: A question answering dataset for source code comprehension." In *Findings of EMNLP 2021*, pp. 2618-2632, 2021.
Cited in 06_related-work.md for code question answering task.

**Liu et al., 2019** — Yinhan Liu, Myle Ott, Naman Goyal, et al. "RoBERTa: A robustly optimized BERT pretraining approach." *arXiv preprint arXiv:1907.11692*, 2019.
Cited in 06_related-work.md for early Transformer pretraining.

**Liu et al., 2023a** — Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. "Visual instruction tuning." *arXiv preprint arXiv:2304.08485*, 2023a.
Cited in 01_introduction.md for multimodal instruction understanding.

**Liu et al., 2023b** — Xiao Liu, Hanyu Lai, Hao Yu, et al. "WebGLM: Towards an efficient web-enhanced question answering system with human preferences." *arXiv preprint arXiv:2306.07906*, 2023b.
Cited in 06_related-work.md for search engine integration.

**Liu et al., 2023c** — Yue Liu, Thanh Le-Cong, Ratnadira Widyasari, et al. "Refining ChatGPT-generated code: Characterizing and mitigating code quality issues." *CoRR*, abs/2307.12596, 2023c.
Cited in 06_related-work.md for code refinement task.

**Longpre et al., 2023** — Shayne Longpre, Le Hou, Tu Vu, et al. "The Flan collection: Designing data and methods for effective instruction tuning." *arXiv preprint arXiv:2301.13688*, 2023.
Cited in 06_related-work.md for instruction tuning methods.

**Loshchilov & Hutter, 2017** — Ilya Loshchilov and Frank Hutter. "Decoupled weight decay regularization." *arXiv preprint arXiv:1711.05101*, 2017.
Cited in 02d_training.md, 04a_code-pretraining.md, and 04b_code-supervised-fine-tuning.md for AdamW optimizer.

**Lu et al., 2023** — Keming Lu, Hongyi Yuan, Zheng Yuan, et al. "#InsTag: Instruction tagging for analyzing supervised fine-tuning of large language models." *CoRR*, abs/2308.07074, 2023.
Cited in 03b_rlhf.md for balanced sampling algorithm for reward model prompt diversity.

**Luo et al., 2023a** — Haipeng Luo, Qingfeng Sun, Can Xu, et al. "WizardMath: Empowering mathematical reasoning for large language models via reinforced evol-instruct." *arXiv preprint arXiv:2308.09583*, 2023a.
Cited in 05b_math-evaluation.md and 06_related-work.md as WizardMath baseline.

**Luo et al., 2023b** — Ziyang Luo, Can Xu, Pu Zhao, et al. "WizardCoder: Empowering code large language models with evol-instruct." *arXiv preprint arXiv:2306.08568*, 2023b.
Cited in 04c_code-evaluation.md as WizardCoder baseline.

**Mosaic ML, 2023** — Mosaic ML. "MPT-30B: Raising the bar for open-source foundation models," 2023.
Cited in 02f_experimental-results.md and 06_related-work.md as MPT baseline.

**Muennighoff et al., 2022** — Niklas Muennighoff, Thomas Wang, Lintang Sutawika, et al. "Crosslingual generalization through multitask finetuning." *arXiv preprint arXiv:2211.01786*, 2022.
Cited in 06_related-work.md for instruction tuning methods.

**Muennighoff et al., 2023** — Niklas Muennighoff, Qian Liu, Armel Zebaze, et al. "OctoPack: Instruction tuning code large language models." *CoRR*, abs/2308.07124, 2023.
Cited in 01_introduction.md, 04c_code-evaluation.md, and 06_related-work.md as HUMANEVALPACK benchmark / OctoPack.

**Nakano et al., 2021** — Reiichiro Nakano, Jacob Hilton, Suchir Balaji, et al. "WebGPT: Browser-assisted question-answering with human feedback." *arXiv preprint arXiv:2112.09332*, 2021.
Cited in 06_related-work.md for search engine integration.

**Nye et al., 2021** — Maxwell Nye, Anders Andreassen, Guy Gur-Ari, et al. "Show your work: Scratchpads for intermediate computation with language models." *ArXiv*, abs/2112.00114, 2021.
Cited in 06_related-work.md for scratchpad technique in math reasoning.

**OpenAI, 2022** — OpenAI. "Introducing ChatGPT," 2022; and OpenAI. "ChatML," 2022.
Cited in 01_introduction.md, 03a_supervised-finetuning.md, 03c_evaluation-aligned-models.md, and 06_related-work.md for ChatGPT and ChatML format.

**OpenAI, 2023** — OpenAI. "GPT4 technical report." *arXiv preprint arXiv:2303.08774*, 2023.
Cited in 01_introduction.md, and 06_related-work.md for GPT-4.

**OpenCompass Team, 2023** — OpenCompass Team. "OpenCompass: A universal evaluation platform for foundation models," 2023.
Cited in 02f_experimental-results.md, 03c_evaluation-aligned-models.md, and 08a_appendix-training-details.md as evaluation platform for baseline scores.

**Ouyang et al., 2022** — Long Ouyang, Jeffrey Wu, Xu Jiang, et al. "Training language models to follow instructions with human feedback." In *NeurIPS*, 2022.
Cited in 01_introduction.md, 03a_supervised-finetuning.md, 03b_rlhf.md, and 06_related-work.md for RLHF and alignment methodology.

**Peng et al., 2023a** — Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. "YaRN: Efficient context window extension of large language models." *arXiv preprint arXiv:2309.00071*, 2023a.
Cited in 02e_context-length-extension.md for dynamic NTK-aware interpolation formal discussion.

**Peng et al., 2023b** — Zhiliang Peng, Wenhui Wang, Li Dong, et al. "Kosmos-2: Grounding multimodal large language models to the world." *arXiv preprint arXiv:2306.14824*, 2023b.
Cited in 01_introduction.md for multimodal instruction understanding.

**Qwen Team, Alibaba Group, 2023a** — Qwen Team, Alibaba Group. "Evaluation benchmark for code interpreter," 2023a.
Cited in 03d_tool-use-code-interpreter-agent.md for code interpreter benchmark.

**Qwen Team, Alibaba Group, 2023b** — Qwen Team, Alibaba Group. "Evaluation benchmark for tool usage through ReAct prompting," 2023b.
Cited in 03d_tool-use-code-interpreter-agent.md for ReAct prompting benchmark.

**Radford et al., 2018** — Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. "Improving language understanding by generative pre-training." Technical report, OpenAI, 2018.
Cited in 01_introduction.md, 02d_training.md, and 06_related-work.md for early Transformer pretraining / GPT.

**Rae et al., 2021** — Jack W Rae, Sebastian Borgeaud, Trevor Cai, et al. "Scaling language models: Methods, analysis & insights from training gopher." *arXiv preprint arXiv:2112.11446*, 2021.
Cited in 06_related-work.md for LLM scaling and coding capabilities.

**Rafailov et al., 2023** — Rafael Rafailov, Archit Sharma, Eric Mitchell, et al. "Direct preference optimization: Your language model is secretly a reward model." *arXiv preprint arXiv:2305.18290*, 2023.
Cited in 06_related-work.md as DPO alternative to RLHF.

**Raffel et al., 2020** — Colin Raffel, Noam Shazeer, Adam Roberts, et al. "Exploring the limits of transfer learning with a unified text-to-text transformer." *JMLR*, 21(1):5485-5551, 2020.
Cited in 01_introduction.md, 02a_data.md, and 06_related-work.md for T5 / transfer learning scaling.

**Ramachandran et al., 2017** — Prajit Ramachandran, Barret Zoph, and Quoc V Le. "Searching for activation functions." *arXiv preprint arXiv:1710.05941*, 2017.
Cited in 02c_architecture.md for Swish activation function.

**Reed et al., 2022** — Scott E. Reed, Konrad Zolna, Emilio Parisotto, et al. "A generalist agent." *Trans. Mach. Learn. Res.*, 2022.
Cited in 01_introduction.md for generalist agent models.

**Roziere et al., 2023** — Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, et al. "Code Llama: Open foundation models for code." *arXiv preprint arXiv:2308.12950*, 2023.
Cited in 04a_code-pretraining.md, 04c_code-evaluation.md, and 06_related-work.md as CODE LLAMA.

**Sanh et al., 2021** — Victor Sanh, Albert Webson, Colin Raffel, et al. "Multitask prompted training enables zero-shot task generalization." *arXiv preprint arXiv:2110.08207*, 2021.
Cited in 06_related-work.md for instruction tuning methods.

**Scao et al., 2022** — Teven Le Scao, Angela Fan, Christopher Akiki, et al. "BLOOM: A 176B-parameter open-access multilingual language model." *arXiv preprint arXiv:2211.05100*, 2022.
Cited in 06_related-work.md for LLM scaling research.

**Schick et al., 2023** — Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, et al. "Toolformer: Language models can teach themselves to use tools." *arXiv preprint arXiv:2302.04761*, 2023.
Cited in 01_introduction.md and 06_related-work.md for tool-use capabilities.

**Schulman et al., 2017** — John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. "Proximal policy optimization algorithms." *arXiv preprint arXiv:1707.06347*, 2017.
Cited in 03b_rlhf.md and 06_related-work.md for PPO algorithm.

**Shazeer, 2020** — Noam Shazeer. "GLU variants improve transformer." *arXiv preprint arXiv:2002.05202*, 2020.
Cited in 02c_architecture.md for SwiGLU activation function.

**Shen et al., 2023** — Yongliang Shen, Kaitao Song, Xu Tan, et al. "HuggingGPT: Solving AI tasks with ChatGPT and its friends in HuggingFace." *arXiv preprint arXiv:2303.17580*, 2023.
Cited in 06_related-work.md for agents calling different AI models.

**Shoeybi et al., 2019** — Mohammad Shoeybi, Mostofa Patwary, Raul Puri, et al. "Megatron-LM: Training multi-billion parameter language models using model parallelism." *arXiv preprint arXiv:1909.08053*, 2019.
Cited in 06_related-work.md for early LLM scaling.

**Si et al., 2023** — Qingyi Si, Tong Wang, Naibin Gu, Rui Liu, and Zheng Lin. "Alpaca-CoT: An instruction-tuning platform with unified interface of instruction collection, parameter-efficient methods, and large language models," 2023.
Cited in 06_related-work.md for math instruction-following datasets.

**Song et al., 2023** — Feifan Song, Bowen Yu, Minghao Li, et al. "Preference ranking optimization for human alignment." *arXiv preprint arXiv:2306.17492*, 2023.
Cited in 06_related-work.md as PRO alternative to RLHF.

**Stability AI, 2023** — Stability AI. "StableBeluga2," 2023.
Cited in 02f_experimental-results.md as StableBeluga2 baseline.

**Stiennon et al., 2020** — Nisan Stiennon, Long Ouyang, Jeffrey Wu, et al. "Learning to summarize with human feedback." *Advances in Neural Information Processing Systems*, 33:3008-3021, 2020.
Cited in 03b_rlhf.md as a human preference benchmark dataset (OpenAI Summ.).

**Su, 2023a** — Jianlin Su. "Improving transformer: Length extrapolation ability and position robustness," 2023a.
Cited in 02e_context-length-extension.md for LogN-Scaling technique.

**Su, 2023b** — Jianlin Su. "The magical effect of the Bias term: RoPE + Bias = better length extrapolation," 2023b.
Cited in 02c_architecture.md for adding biases to QKV attention layer.

**Su et al., 2021** — Jianlin Su, Yu Lu, Shengfeng Pan, et al. "Roformer: Enhanced transformer with rotary position embedding." *arXiv preprint arXiv:2104.09864*, 2021.
Cited in 02c_architecture.md for RoPE positional embedding.

**Sun et al., 2023a** — Tianxiang Sun, Xiaotian Zhang, Zhengfu He, et al. "MOSS: Training conversational language models from synthetic data," 2023a.
Cited in 06_related-work.md as MOSS open-source chat data/model.

**Suzgun et al., 2022** — Mirac Suzgun, Nathan Scales, Nathanael Scharli, et al. "Challenging big-bench tasks and whether chain-of-thought can solve them." *arXiv preprint arXiv:2210.09261*, 2022.
Cited in 02f_experimental-results.md, 03c_evaluation-aligned-models.md as BBH benchmark, and 06_related-work.md for math reasoning.

**Szafraniec et al., 2023** — Marc Szafraniec, Baptiste Roziere, Hugh Leather, et al. "Code translation with compiler representations." In *ICLR 2023*, 2023.
Cited in 06_related-work.md for code translation task.

**Taori et al., 2023** — Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, et al. "Stanford Alpaca: An instruction-following LLaMA model," 2023.
Cited in 06_related-work.md as Alpaca open-source chat data/model.

**Taylor et al., 2022** — Ross Taylor, Marcin Kardas, Guillem Cucurull, et al. "Galactica: A large language model for science," 2022.
Cited in 06_related-work.md for math pretraining.

**Thoppilan et al., 2022** — Romal Thoppilan, Daniel De Freitas, Jamie Hall, et al. "LaMDA: Language models for dialog applications." *CoRR*, abs/2201.08239, 2022.
Cited in 01_introduction.md and 06_related-work.md for LLM scaling research.

**Touvron et al., 2023a** — Hugo Touvron, Thibaut Lavril, Gautier Izacard, et al. "LLaMA: Open and efficient foundation language models." *arXiv preprint arXiv:2302.13971*, 2023a.
Cited in 01_introduction.md, 02a_data.md, 02b_tokenization.md, 02c_architecture.md, 02f_experimental-results.md, and 06_related-work.md as LLaMA.

**Touvron et al., 2023b** — Hugo Touvron, Louis Martin, Kevin Stone, et al. "Llama 2: Open foundation and fine-tuned chat models." *CoRR*, abs/2307.09288, 2023b.
Cited in 01_introduction.md, 02a_data.md, 02c_architecture.md, 02f_experimental-results.md, 03c_evaluation-aligned-models.md, and 06_related-work.md as LLAMA 2.

**Vaswani et al., 2017** — Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. "Attention is all you need." *Advances in Neural Information Processing Systems*, 30, 2017.
Cited in 06_related-work.md for the Transformer architecture.

**Wang et al., 2017** — Yan Wang, Xiaojiang Liu, and Shuming Shi. "Deep neural solver for math word problems." In *EMNLP*, 2017.
Cited in 05b_math-evaluation.md as Math23K benchmark.

**Wang et al., 2021** — Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. "CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation." *arXiv preprint arXiv:2109.00859*, 2021.
Cited in 06_related-work.md as CodeT5 coding model.

**Wang et al., 2022** — Xuezhi Wang, Jason Wei, Dale Schuurmans, et al. "Self-consistency improves chain of thought reasoning in language models." *ArXiv*, abs/2203.11171, 2022.
Cited in 06_related-work.md for self-consistency prompting in math.

**Wang et al., 2023a** — Guanzhi Wang, Yuqi Xie, Yunfan Jiang, et al. "Voyager: An open-ended embodied agent with large language models." *arXiv preprint arXiv:2305.16291*, 2023a.
Cited in 01_introduction.md and 06_related-work.md for embodied agents.

**Wang et al., 2023b** — Yizhong Wang, Hamish Ivison, Pradeep Dasigi, et al. "How far can camels go? Exploring the state of instruction tuning on open resources." *arXiv preprint arXiv:2306.04751*, 2023b.
Cited in 06_related-work.md for open-source chat models.

**Wang et al., 2023c** — Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, et al. "Self-Instruct: Aligning language models with self-generated instructions." In *ACL 2023*, pp. 13484-13508, 2023c.
Cited in 03d_tool-use-code-interpreter-agent.md and 06_related-work.md for self-instruct approach.

**Wang et al., 2023d** — Yue Wang, Hung Le, Akhilesh Deepak Gotmare, et al. "CodeT5+: Open code large language models for code understanding and generation." *CoRR*, abs/2305.07922, 2023d.
Cited in 04c_code-evaluation.md as CodeT5+ / InstructCodeT5+ baseline.

**Wei et al., 2022a** — Jason Wei, Maarten Bosma, Vincent Y. Zhao, et al. "Finetuned language models are zero-shot learners." In *ICLR 2022*, 2022a.
Cited in 03a_supervised-finetuning.md and 06_related-work.md for instruction tuning.

**Wei et al., 2022b** — Jason Wei, Yi Tay, Rishi Bommasani, et al. "Emergent abilities of large language models." *Trans. Mach. Learn. Res.*, 2022b.
Cited in 06_related-work.md for emergent math reasoning abilities.

**Wei et al., 2022c** — Jason Wei, Xuezhi Wang, Dale Schuurmans, et al. "Chain-of-thought prompting elicits reasoning in large language models." *NeurIPS*, 35:24824-24837, 2022c.
Cited in 06_related-work.md for chain-of-thought prompting.

**Wolf et al., 2019** — Thomas Wolf, Lysandre Debut, Victor Sanh, et al. "HuggingFace's transformers: State-of-the-art natural language processing." *arXiv preprint arXiv:1910.03771*, 2019.
Cited in 06_related-work.md for the open-source community ecosystem.

**Xu et al., 2023a** — Benfeng Xu, An Yang, Junyang Lin, et al. "ExpertPrompting: Instructing large language models to be distinguished experts." *arXiv preprint arXiv:2305.14688*, 2023a.
Cited in 06_related-work.md for open-source chat data.

**Xu et al., 2023b** — Can Xu, Qingfeng Sun, Kai Zheng, et al. "WizardLM: Empowering large language models to follow complex instructions." *arXiv preprint arXiv:2304.12244*, 2023b.
Cited in 06_related-work.md as WizardLM / Evol-Instruct.

**Xu et al., 2023c** — Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. "Baize: A open-source chat model with parameter-efficient tuning on self-chat data." *arXiv preprint arXiv:2304.01196*, 2023c.
Cited in 06_related-work.md for open-source chat data/models.

**Xu et al., 2023d** — Yuzhuang Xu, Shuo Wang, Peng Li, et al. "Exploring large language models for communication games." *arXiv preprint arXiv:2309.04658*, 2023d.
Cited in 06_related-work.md for multi-agent interaction.

**Yang, 2023** — Jianxin Yang. "Firefly," 2023.
Cited in 06_related-work.md for open-source chat data.

**Yang et al., 2023** — Aiyuan Yang, Bin Xiao, Bingning Wang, et al. "Baichuan 2: Open large-scale language models." Technical report, Baichuan Inc., 2023.
Cited in 02f_experimental-results.md, 03c_evaluation-aligned-models.md, and 06_related-work.md as Baichuan2 baseline.

**Yao et al., 2022** — Shunyu Yao, Jeffrey Zhao, Dian Yu, et al. "ReAct: Synergizing reasoning and acting in language models." *arXiv preprint arXiv:2210.03629*, 2022.
Cited in 03d_tool-use-code-interpreter-agent.md and 06_related-work.md for ReAct prompting format.

**Ye et al., 2023** — Qinghao Ye, Haiyang Xu, Guohai Xu, et al. "mPLUG-Owl: Modularization empowers large language models with multimodality." *arXiv preprint arXiv:2304.14178*, 2023.
Cited in 01_introduction.md for multimodal instruction understanding.

**Yu et al., 2023** — Longhui Yu, Weisen Jiang, Han Shi, et al. "Metamath: Bootstrap your own mathematical questions for large language models," 2023.
Cited in 06_related-work.md for math instruction-following datasets.

**Yuan et al., 2023a** — Zheng Yuan, Hongyi Yuan, Chengpeng Li, et al. "Scaling relationship on learning mathematical reasoning with large language models," 2023a.
Cited in 05b_math-evaluation.md and 06_related-work.md as RFT math baseline and math fine-tuning dataset.

**Yuan et al., 2023b** — Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. "How well do large language models perform in arithmetic tasks?" *arXiv preprint arXiv:2304.02015*, 2023b.
Cited in 05b_math-evaluation.md as Math401 benchmark.

**Yuan et al., 2023c** — Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. "RRHF: Rank responses to align language models with human feedback without tears," 2023c.
Cited in 06_related-work.md as RRHF alternative to RLHF.

**Yue et al., 2023** — Xiang Yue, Xingwei Qu, Ge Zhang, et al. "MAmmoTH: Building math generalist models through hybrid instruction tuning." *arXiv preprint arXiv:2309.05653*, 2023.
Cited in 06_related-work.md for math instruction-following datasets.

**Zeng et al., 2022** — Aohan Zeng, Xiao Liu, Zhengxiao Du, et al. "GLM-130B: An open bilingual pre-trained model." *arXiv preprint arXiv:2210.02414*, 2022.
Cited in 02a_data.md for multi-task instruction data, and 06_related-work.md for LLM scaling.

**Zhang et al., 2022** — Susan Zhang, Stephen Roller, Naman Goyal, et al. "OPT: Open pre-trained transformer language models." *arXiv preprint arXiv:2205.01068*, 2022.
Cited in 06_related-work.md for LLM scaling research.

**Zhang et al., 2023a** — Fengji Zhang, Bei Chen, Yue Zhang, et al. "RepoCoder: Repository-level code completion through iterative retrieval and generation." *CoRR*, abs/2303.12570, 2023a.
Cited in 06_related-work.md for code completion task.

**Zhang et al., 2023b** — Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. "Evaluating the performance of large language models on GAOKAO benchmark." *CoRR*, abs/2305.12474, 2023b.
Cited in 08a_appendix-training-details.md as Gaokao-Bench benchmark.

**Zheng et al., 2023** — Qinkai Zheng, Xiao Xia, Xu Zou, et al. "CodeGeeX: A pre-trained model for code generation with multilingual evaluations on humaneval-x." *CoRR*, abs/2303.17568, 2023.
Cited in 01_introduction.md, 04c_code-evaluation.md, and 06_related-work.md as CodeGeeX/CodeGeeX2 coding model.

**Zhong et al., 2023a** — Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. "AGIEval: A human-centric benchmark for evaluating foundation models." *CoRR*, abs/2304.06364, 2023a.
Cited in 08a_appendix-training-details.md as AGIEval benchmark.

**Zhong et al., 2023b** — Wanjun Zhong, Lianghong Guo, Qiqi Gao, and Yanlin Wang. "MemoryBank: Enhancing large language models with long-term memory." *arXiv preprint arXiv:2305.10250*, 2023b.
Cited in 06_related-work.md for external memory usage.

**Zhou et al., 2022** — Denny Zhou, Nathanael Scharli, Le Hou, et al. "Least-to-most prompting enables complex reasoning in large language models." *ArXiv*, abs/2205.10625, 2022.
Cited in 06_related-work.md for least-to-most prompting in math.

**Bisk et al., 2020** — Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. "PIQA: Reasoning about physical commonsense in natural language." In *The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020*, pp. 7432-7439. AAAI Press, 2020.
Cited in 08a_appendix-training-details.md as PIQA benchmark (reasoning evaluation).

**Chern et al., 2023b** — I Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu, et al. "Factool: Factuality detection in generative ai -- a tool augmented framework for multi-task and multi-domain scenarios." *arXiv preprint arXiv:2307.13528*, 2023b.
Cited in 08b_appendix-human-evaluation.md as FacTool, a source for the human evaluation dataset.

**Clark et al., 2019** — Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. "BoolQ: Exploring the surprising difficulty of natural yes/no questions." In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019*, pp. 2924-2936, 2019.
Cited in 08a_appendix-training-details.md as BoolQ benchmark (knowledge and understanding evaluation).

**Hu et al., 2020** — Hai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra Kubler, and Lawrence S. Moss. "OCNLI: Original Chinese natural language inference." In *Findings of the Association for Computational Linguistics: EMNLP 2020*, volume EMNLP 2020 of *Findings of ACL*, pp. 3512-3526, 2020.
Cited in 08a_appendix-training-details.md as OCNLI benchmark (reasoning evaluation, Chinese NLI).

**Kwiatkowski et al., 2019** — Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. "Natural questions: A benchmark for question answering research." *Trans. Assoc. Comput. Linguistics*, 7:452-466, 2019.
Cited in 08a_appendix-training-details.md as NaturalQuestions benchmark (knowledge and understanding evaluation).

**Paperno et al., 2016** — Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. "The LAMBADA dataset: Word prediction requiring a broad discourse context." In *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016*, 2016.
Cited in 08a_appendix-training-details.md as LAMBADA benchmark (knowledge and understanding evaluation).

**Sap et al., 2019** — Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. "SocialIQA: Commonsense reasoning about social interactions." *CoRR*, abs/1904.09728, 2019.
Cited in 08a_appendix-training-details.md as SIQA benchmark (reasoning evaluation).

**Talmor et al., 2019** — Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. "CommonsenseQA: A question answering challenge targeting commonsense knowledge." In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019*, pp. 4149-4158, 2019.
Cited in 08a_appendix-training-details.md as CommonsenseQA benchmark (knowledge and understanding evaluation).

**Zellers et al., 2019** — Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. "HellaSwag: Can a machine really finish your sentence?" In *Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019*, pp. 4791-4800, 2019.
Cited in 08a_appendix-training-details.md as HellaSwag benchmark (reasoning evaluation).
