# 7 Conclusion [p. 22]

[p. 22] The report presents the QWEN series of large language models, which showcase the latest advancements in natural language processing. With 14B, 7B, and 1.8B parameters, these models have been pre-trained on massive amounts of data, including trillions of tokens, and fine-tuned using cutting-edge techniques such as SFT and RLHF. Additionally, the QWEN series includes specialized models for coding and mathematics, such as CODE-QWEN, CODE-QWEN-CHAT, and MATH-QWEN-CHAT, which have been trained on domain-specific data to excel in their respective fields.

> "Our results demonstrate that the QWEN series is competitive with existing open-source models and even matches the performance of some proprietary models on comprehensive benchmarks and human evaluation." [p. 22]

[p. 22] The authors believe that the open access of QWEN will foster collaboration and innovation within the community, enabling researchers and developers to build upon their work and push the boundaries of what is possible with language models. By providing these models to the public, they hope to inspire new research and applications that will further advance the field and contribute to understanding of the variables and techniques introduced in realistic settings.

> "In a nutshell, the QWEN series represents a major milestone in our development of large language models, and we are excited to see how it will be used to drive progress and innovation in the years to come." [p. 22]
