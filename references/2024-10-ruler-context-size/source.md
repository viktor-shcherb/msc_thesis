# RULER: What's the Real Context Size of Your Long-Context Language Models?

**Authors:** Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, Boris Ginsburg
**Affiliation:** NVIDIA

## Publication Status

- **arXiv preprint:** April 2024, arXiv:2404.06654
- **Peer-reviewed:** Yes
- **Conference:** COLM 2024 (1st Conference on Language Modeling), October 7--9, 2024, University of Pennsylvania
- **Status:** Published conference paper

## Preferred Citation

Cite the COLM 2024 version:

> Hsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D., Jia, F., Zhang, Y., & Ginsburg, B. (2024). RULER: What's the Real Context Size of Your Long-Context Language Models? In Proceedings of the 1st Conference on Language Modeling (COLM 2024).

## Links

- arXiv: https://arxiv.org/abs/2404.06654
- OpenReview: https://openreview.net/forum?id=kIoBbc76Sy
- Code: https://github.com/NVIDIA/RULER
