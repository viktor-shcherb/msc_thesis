# Appendix E: Passkey Retrieval and Vanilla NIAH Results [p. 24]

This appendix presents detailed performance results for passkey retrieval and vanilla NIAH tasks across various context lengths.

## Passkey Retrieval Results

**Table 10: Performance of selected aligned and base models across length 4K to 128K in passkey retrieval of RULER** [p. 24]

Almost all models have perfect score at their claimed length.

| Models | Claimed Length | 4K | 8K | 16K | 32K | 64K | 128K | Avg. |
|--------|----------------|-----|-----|------|------|------|------|------|
| Gemini-1.5 | 1M | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 |
| GPT-4 | 128K | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 |
| Llama3.1 (70B) | 128K | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 97.8 | 99.6 |
| Llama3.1 (8B) | 128K | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 |
| Qwen2 (72B) | 128K | 100.0 | 100.0 | 100.0 | 99.8 | 100.0 | 100.0 | 99.9 |
| Command-R-plus (104B) | 128K | 100.0 | 100.0 | 99.8 | 99.8 | 100.0 | 97.2 | 99.5 |
| GLM4 (9B) | 1M | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 |
| GradientAI/Llama3 (70B) | 1M | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 93.6 | 98.9 |
| Mixtral-8x22B (39B/141B) | 64K | 100.0 | 100.0 | 100.0 | 100.0 | 99.6 | 0.0 | 83.3 |
| Yi (34B) | 200K | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 |
| Phi3-medium (14B) | 128K | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 88.0 | 98.0 |
| Mistral-v0.2 (7B) | 32K | 100.0 | 100.0 | 100.0 | 100.0 | 99.6 | 69.6 | 94.9 |
| LWM (7B) | 1M | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 |
| DBRX (36B/132B) | 32K | 100.0 | 100.0 | 100.0 | 100.0 | 0.0 | 0.0 | 66.7 |
| Together (7B) | 32K | 100.0 | 100.0 | 100.0 | 100.0 | 0.0 | 0.0 | 66.7 |
| LongChat (7B) | 32K | 100.0 | 100.0 | 100.0 | 99.4 | 0.0 | 0.0 | 66.6 |
| LongAlpaca (13B) | 32K | 88.2 | 88.0 | 86.4 | 82.4 | 0.0 | 0.0 | 57.6 |
| Mixtral-base (8x7B) | 32K | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 46.8 | 91.1 |
| Mistral-base (7B) | 32K | 100.0 | 100.0 | 100.0 | 100.0 | 99.6 | 70.8 | 95.1 |
| Jamba-base (52B) | 256K | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 |
| LWM-base (7B) | 1M | 99.8 | 100.0 | 99.6 | 99.6 | 98.2 | 96.0 | 98.9 |
| LongLoRA-base (7B) | 100K | 97.2 | 95.4 | 97.0 | 96.4 | 96.2 | 60.1 | 90.4 |
| Yarn-base (7B) | 128K | 100.0 | 100.0 | 99.0 | 100.0 | 99.2 | 39.6 | 89.6 |
| Together-base (7B) | 32K | 100.0 | 100.0 | 99.8 | 100.0 | 0.0 | 0.0 | 66.6 |

## Vanilla NIAH Results

**Table 11: Performance of selected aligned and base models across length 4K to 128K in vanilla NIAH of RULER** [p. 24]

Almost all models have perfect score at their claimed length.

| Models | Claimed Length | 4K | 8K | 16K | 32K | 64K | 128K | Avg. |
|--------|----------------|-----|-----|------|------|------|------|------|
| Gemini-1.5 | 1M | 100.0 | 100.0 | 100.0 | 98.0 | 100.0 | 100.0 | 99.7 |
| GPT-4 | 128K | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 |
| Llama3.1 (70B) | 128K | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 99.6 | 99.9 |
| Llama3.1 (8B) | 128K | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 99.6 | 99.9 |
| Qwen2 (72B) | 128K | 100.0 | 100.0 | 100.0 | 100.0 | 99.8 | 56.4 | 92.7 |
| Command-R-plus (104B) | 128K | 100.0 | 100.0 | 100.0 | 100.0 | 99.8 | 86.0 | 97.6 |
| GLM4 (9B) | 128K | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 |
| GradientAI/Llama3 (70B) | 1M | 100.0 | 100.0 | 100.0 | 99.6 | 99.2 | 97.8 | 99.4 |
| Mixtral-8x22B (39B/141B) | 64K | 100.0 | 100.0 | 100.0 | 100.0 | 99.6 | 24.2 | 87.3 |
| Yi (34B) | 200K | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 |
| Phi3-medium (14B) | 128K | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 73.4 | 95.6 |
| Mistral-v0.2 (7B) | 32K | 100.0 | 100.0 | 100.0 | 97.0 | 70.0 | 7.4 | 79.1 |
| LWM (7B) | 1M | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 |
| DBRX (36B/132B) | 32K | 100.0 | 100.0 | 90.0 | 93.2 | 0.8 | 0.0 | 64.0 |
| Together (7B) | 32K | 100.0 | 100.0 | 100.0 | 99.8 | 0.0 | 0.0 | 66.6 |
| LongChat (7B) | 32K | 100.0 | 100.0 | 97.6 | 98.4 | 0.0 | 0.0 | 66.0 |
| LongAlpaca (13B) | 32K | 90.2 | 90.2 | 88.4 | 83.4 | 0.0 | 0.0 | 58.7 |
| Mixtral-base (8x7B) | 32K | 100.0 | 100.0 | 100.0 | 100.0 | 85.2 | 34.8 | 86.7 |
| Mistral-base (7B) | 32K | 100.0 | 100.0 | 100.0 | 100.0 | 94.8 | 0.4 | 82.5 |
| Jamba-base (52B) | 256K | 100.0 | 100.0 | 98.8 | 99.8 | 99.8 | 86.4 | 97.5 |
| LWM-base (7B) | 1M | 100.0 | 99.4 | 97.8 | 98.6 | 98.2 | 98.6 | 98.8 |
| LongLoRA-base (7B) | 100K | 99.8 | 100.0 | 100.0 | 99.8 | 100.0 | 0.0 | 83.3 |
| Yarn-base (7B) | 128K | 97.4 | 97.8 | 91.4 | 85.4 | 86.6 | 20.0 | 79.8 |
| Together-base (7B) | 32K | 100.0 | 100.0 | 100.0 | 99.8 | 0.0 | 0.0 | 66.6 |
