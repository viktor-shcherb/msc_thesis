# References

Only references cited in the section notes are included below.

## Barrault et al. (2019)
Loic Barrault, Ondrej Bojar, Marta R. Costa-jussa, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias Muller, Santanu Pal, Matt Post, and Marcos Zampieri. 2019. Findings of the 2019 conference on machine translation (WMT19). In *Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)*, pages 1-61, Florence, Italy. Association for Computational Linguistics.
- Cited in 01_introduction.md as a standard NLP benchmark (WMT).

## Barrault et al. (2020)
Loic Barrault, Magdalena Biesialska, Ondrej Bojar, Marta R. Costa-jussa, Christian Federmann, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Matthias Huck, Eric Joanis, Tom Kocmi, Philipp Koehn, Chi-kiu Lo, Nikola Ljubesic, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (WMT20). In *Proceedings of the Fifth Conference on Machine Translation*, pages 1-55, Online. Association for Computational Linguistics.
- Cited in 01_introduction.md as a standard NLP benchmark (WMT).

## Beltagy et al. (2020)
Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer.
- Cited in 01_introduction.md and 05_experiments.md as the architecture behind LED, one of the two main baselines.

## Brown et al. (2020)
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In *Advances in Neural Information Processing Systems*, volume 33, pages 1877-1901. Curran Associates, Inc.
- Cited in 01_introduction.md (Figure 1 caption) marking GPT-3's max sequence length.

## Carletta et al. (2005)
Jean Carletta, Simone Ashby, Sebastien Bourban, Mike Flynn, Mael Guillemot, Thomas Hain, Jaroslav Kadlec, Vasilis Karaiskos, Wessel Kraaij, Melissa Kronenthal, Guillaume Lathoud, Mike Lincoln, Agnes Lisowska, Iain McCowan, Wilfried Post, Dennis Reidsma, and Pierre Wellner. 2005. The ami meeting corpus: A pre-announcement. In *International workshop on machine learning for multi-modal interaction*, pages 28-39, Berlin, Heidelberg. Springer.
- Cited in 03_scrolls-benchmark.md as source of industrial product meeting transcripts used in QMSum.

## Chen et al. (2021)
Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2021. Summscreen: A dataset for abstractive screenplay summarization.
- Cited in 03_scrolls-benchmark.md as the source dataset for SummScreenFD.

## Choromanski et al. (2020)
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. 2020. Rethinking attention with performers.
- Cited in 02_background.md as an efficient transformer using perplexity for evaluation.

## Cohan et al. (2018)
Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents. In *Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)*, pages 615-621, New Orleans, Louisiana. Association for Computational Linguistics.
- Cited in 02_background.md and 03_scrolls-benchmark.md as the source of arXiv/PubMed summarization datasets; cited in 04_quantitative-analysis.md for comparison of bigram spread.

## Dasigi et al. (2021)
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021. A dataset of information-seeking questions and answers anchored in research papers. In *Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pages 4599-4610, Online. Association for Computational Linguistics.
- Cited in 03_scrolls-benchmark.md as the source dataset for Qasper; cited in 05_experiments.md for inter-annotator overlap of 60.9% F1.

## Devlin et al. (2019)
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.
- Cited in 01_introduction.md (Figure 1 caption) marking BERT's max sequence length.

## Fillmore et al. (1998)
Charles J. Fillmore, Nancy Ide, Dan Jurafsky, and Catherine Macleod. 1998. An american national corpus: a proposal. In *LREC*.
- Cited in 03_scrolls-benchmark.md as a source for QuALITY documents.

## Fisch et al. (2019)
Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. 2019. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In *Proceedings of the 2nd Workshop on Machine Reading for Question Answering*, pages 1-13, Hong Kong, China. Association for Computational Linguistics.
- Cited in 03_scrolls-benchmark.md regarding the F1 evaluation metric practice.

## Fournier et al. (2021)
Quentin Fournier, Gaetan Marceau Caron, and Daniel Aloise. 2021. A practical survey on faster and lighter transformers.
- Cited in 01_introduction.md and 02_background.md as a survey of efficient transformer alternatives.

## Hermann et al. (2015)
Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In *Advances in Neural Information Processing Systems*, volume 28. Curran Associates, Inc.
- Cited in 04_quantitative-analysis.md as the source of CNN/DM summarization dataset for comparison.

## Huang et al. (2021)
Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021. Efficient attentions for long document summarization. In *Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pages 1419-1436, Online. Association for Computational Linguistics.
- Cited in 03_scrolls-benchmark.md as the source dataset for GovReport; cited in 04_quantitative-analysis.md as inspiration for the bigram spread metric.

## Ide and Suderman (2004)
Nancy Ide and Keith Suderman. 2004. The American national corpus first release. In *Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC'04)*, Lisbon, Portugal. European Language Resources Association (ELRA).
- Cited in 03_scrolls-benchmark.md as a source for QuALITY documents.

## Janin et al. (2003)
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters. 2003. The icsi meeting corpus. In *2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03).*, volume 1, pages I-I.
- Cited in 03_scrolls-benchmark.md as source of academic group meeting transcripts used in QMSum.

## Khandelwal et al. (2018)
Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. 2018. Sharp nearby, fuzzy far away: How neural language models use context. In *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 284-294, Melbourne, Australia. Association for Computational Linguistics.
- Cited in 01_introduction.md and 02_background.md showing that next-token prediction is mostly a local task.

## Kočiský et al. (2018)
Tomas Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. 2018. The NarrativeQA reading comprehension challenge. *Transactions of the Association for Computational Linguistics*, 6:317-328.
- Cited in 03_scrolls-benchmark.md as the source dataset for NarrativeQA.

## Koreeda and Manning (2021)
Yuta Koreeda and Christopher D. Manning. 2021. Contractnli: A dataset for document-level natural language inference for contracts.
- Cited in 03_scrolls-benchmark.md as the source dataset for ContractNLI.

## Kwiatkowski et al. (2019)
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. *Transactions of the Association for Computational Linguistics*, 7:452-466.
- Cited in 04_quantitative-analysis.md for comparison of bigram spread with Natural Questions.

## Lewis et al. (2020)
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*, pages 7871-7880, Online. Association for Computational Linguistics.
- Cited in 01_introduction.md and 05_experiments.md as one of the two main baselines (BART).

## Lhoest et al. (2021)
Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Sasko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clement Delangue, Theo Matussiere, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Francois Lagunas, Alexander Rush, and Thomas Wolf. 2021. Datasets: A community library for natural language processing. In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*, pages 175-184, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.
- Cited in 01_introduction.md as the library through which SCROLLS is available.

## Lin (2004)
Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In *Text Summarization Branches Out*, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.
- Cited in 03_scrolls-benchmark.md as the source of the ROUGE evaluation metric.

## Loshchilov and Hutter (2019)
Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In *International Conference on Learning Representations*.
- Cited in 12_appendix-d-hyperparameters.md as the optimizer (AdamW) used for finetuning baselines.

## Lo et al. (2020)
Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. 2020. S2ORC: The semantic scholar open research corpus. In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*, pages 4969-4983, Online. Association for Computational Linguistics.
- Cited in 03_scrolls-benchmark.md as the source corpus from which Qasper papers were filtered.

## Pang et al. (2021)
Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel R. Bowman. 2021. QuALITY: Question answering with long input texts, yes! arXiv preprint arXiv:2112.08608.
- Cited in 03_scrolls-benchmark.md as the source dataset for QuALITY; cited in 05_experiments.md for human agreement of 93.5% EM.

## Peng et al. (2021)
Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. 2021. Random feature attention. In *International Conference on Learning Representations*.
- Cited in 02_background.md as an efficient transformer using perplexity for evaluation.

## Press et al. (2021a)
Ofir Press, Noah A. Smith, and Mike Lewis. 2021a. Shortformer: Better language modeling using shorter inputs. In *Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)*, pages 5493-5505, Online. Association for Computational Linguistics.
- Cited in 02_background.md showing that masking distant tokens can improve perplexity.

## Press et al. (2021b)
Ofir Press, Noah A. Smith, and Mike Lewis. 2021b. Train short, test long: Attention with linear biases enables input length extrapolation.
- Cited in 02_background.md showing that down-weighting distant tokens can improve perplexity.

## Rajpurkar et al. (2016)
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In *Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing*, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.
- Cited in 01_introduction.md as a standard NLP benchmark; cited in 03_scrolls-benchmark.md and 04_quantitative-analysis.md for SQuAD evaluation practice and bigram spread comparison.

## Rajpurkar et al. (2018)
Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for SQuAD. In *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)*, pages 784-789, Melbourne, Australia. Association for Computational Linguistics.
- Cited in 01_introduction.md as a standard NLP benchmark (SQuAD 2.0).

## Roy et al. (2021)
Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2021. Efficient content-based sparse attention with routing transformers. *Transactions of the Association for Computational Linguistics*, 9:53-68.
- Cited in 02_background.md as an efficient transformer using perplexity for evaluation.

## Sun et al. (2021)
Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. 2021. Do long-range language models actually use long-range context? In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages 807-822, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.
- Cited in 01_introduction.md and 02_background.md showing that next-token prediction mostly relies on local context.

## Tay et al. (2020a)
Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. 2020a. Synthesizer: Re-thinking self-attention in transformer models.
- Cited in 02_background.md as an efficient transformer using perplexity for evaluation.

## Tay et al. (2020b)
Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020b. Efficient transformers: A survey.
- Cited in 01_introduction.md and 02_background.md as a survey of efficient transformer alternatives.

## Tay et al. (2021)
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2021. Long range arena: A benchmark for efficient transformers. In *International Conference on Learning Representations*.
- Cited in 01_introduction.md and 02_background.md as the Long Range Arena benchmark, which SCROLLS aims to complement/supersede for NLP.

## Vaswani et al. (2017)
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In *Advances in Neural Information Processing Systems*, volume 30. Curran Associates, Inc.
- Cited in 02_background.md as the original transformer architecture with O(n^2) self-attention.

## Wang et al. (2018)
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In *Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP*, pages 353-355, Brussels, Belgium. Association for Computational Linguistics.
- Cited in 01_introduction.md as a standard NLP benchmark (GLUE).

## Wang et al. (2019)
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. In *Advances in Neural Information Processing Systems*, volume 32. Curran Associates, Inc.
- Cited in 01_introduction.md as a standard NLP benchmark (SuperGLUE).

## Wolf et al. (2020)
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*, pages 38-45, Online. Association for Computational Linguistics.
- Cited in 05_experiments.md as the Transformers library used for the experiments.

## Zhong et al. (2021)
Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. 2021. QMSum: A new benchmark for query-based multi-domain meeting summarization. In *Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pages 5905-5921, Online. Association for Computational Linguistics.
- Cited in 03_scrolls-benchmark.md as the source dataset for QMSum.
