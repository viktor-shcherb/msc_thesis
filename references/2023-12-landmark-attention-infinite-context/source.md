# Random-Access Infinite Context Length for Transformers

**Authors:** Amirkeivan Mohtashami, Martin Jaggi
**Affiliation:** EPFL (École Polytechnique Fédérale de Lausanne)

## Publication Status

- **arXiv preprint:** May 2023, arXiv:2305.16300
- **Peer-reviewed:** Yes
- **Conference:** Advances in Neural Information Processing Systems 36 (NeurIPS 2023), New Orleans, LA, USA, December 10--16, 2023
- **Status:** Published conference paper

## Notes

The arXiv version uses the title "Landmark Attention: Random-Access Infinite Context Length for Transformers." The NeurIPS proceedings version drops the "Landmark Attention:" prefix. The method is commonly referred to as "Landmark Attention" in follow-up literature.

## Preferred Citation

Cite the NeurIPS 2023 version:

> Mohtashami, A. & Jaggi, M. (2023). Random-Access Infinite Context Length for Transformers. In Advances in Neural Information Processing Systems 36 (NeurIPS 2023).

## Links

- arXiv: https://arxiv.org/abs/2305.16300
- NeurIPS Proceedings: https://proceedings.neurips.cc/paper_files/paper/2023/hash/ab05dc8bf36a9f66edbff6992ec86f56-Abstract-Conference.html
- Code: https://github.com/epfml/landmark-attention
