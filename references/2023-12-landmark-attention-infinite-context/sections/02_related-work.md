# 2 Related Work [p. 3-4]

[p. 3] With the evolution of state-of-the-art commercial models and their applications towards very long context window lengths, such as 32k tokens (GPT-4 [25]) or even 100k (Claude [2]), the research question of efficient while accurate long context models is receiving increased attention.

## Retrieval-Augmented Language Models

Retrieval-augmented language models use a separate module, called a retriever, to find a set of relevant documents in the knowledge base, which are then prepended to the input. The augmented input is then fed into the main model, called the reader. Various methods have been proposed for training retrievers and readers [16]. For example, REALM [11] jointly trains the reader and retriever, where both components are transformers. Atlas [13] further investigates the effect of various losses on the performance of the retriever. Previous work has also looked into using the attention in the reader to build a retriever but used manually crafted rules to reduce the token scores to document scores [14, 18, 31]. In contrast, the landmark approach is trained to directly produce meaningful landmark embeddings on the fly, without needing any notion of corpus for retrieval.

## Memory for Transformers

Various methods have been proposed to introduce memorization capabilities to Transformers through recurrence [5, 40]. Transformer-XL [9] feeds the input to the model in windows of a fixed length and allows each token to attend to tokens in the current window as well as the preceding window. Memory Transformers [6] introduce special memory tokens that are prepended to the input, and their representation at the final layer of the model is used for the next input. Infinite Memory Transformers [23] map the input to a continuous space and then sample points to be used for memory in the next step according to the probability distribution defined by the attention mechanism. However, while these methods improve upon the memory-less variants, they do not allow for attending to *specific* tokens in the past, as the model only has access to a compressed version of this information. In fact, Mu et al. [24] in simultaneous work propose adding special "gist" tokens which are trained to summarize the prompt so far, and find that the model is incapable of remembering specific details that should be copied into the output. Furthermore, the decision about whether to keep or discard a piece of information needs to be made without knowledge of future tokens, which makes it likely that aspects of information will be lost, especially if the topic changes. In contrast, using the authors' method, the model always has the possibility of retrieving and attending to any tokens in the past. Nonetheless, these methods can be combined with theirs, allowing the model to benefit from both full access to previous tokens as well as access to a summarized version in terms of the recurrent memory state.

## Approximate and Sparse Attention

[p. 3] Various methods have also been proposed to reduce the memory footprint of attention. However, similar to recurrent memories, these approximations significantly reduce the flexibility of attention in attending to arbitrary individual tokens. For example, Child et al. [7] limit the attention to a local window around each token, while BigBird additionally suggests attending to a random subset of previous tokens as well as several globally accessible tokens [42]. Longformer [3] further introduces dilated sliding window patterns to increase attention's receptive field and manually picks the window sizes for each layer. Linformer [39] uses a low-rank approximation of the attention matrix while Performer [8] uses a non-softmax kernel to obtain a more efficient implementation. Reformer [19] uses locality-sensitive-hashing (LSH) to retrieve the closest key vectors which should account for the highest scores in the attention matrix. Combiner [30] utilizes a hierarchical attention mechanism and heuristic reduction techniques, such as max-pooling, to derive key and query vectors for input blocks. The block weight is determined based on the pooled key vector, while the weight of each token within the block is determined by the pooled query vector. However, this approach limits the control of the current token over the weights of the tokens inside the block, resulting in reduced flexibility of attention. In contrast, the proposed method enables the current token's query vector to control the weight for each token, and the gating mechanism is learned through the attention process instead of relying on heuristic reductions.

## kNN Augmented Transformers

[p. 3] k-nearest-neighbor (kNN) augmentation has been proposed as an alternative method for allowing transformers to access external memory. For example, kNN-LM [17] stores the hidden representation of tokens in memory and uses the distribution of the next token among the stored vectors that are closest to the current token to predict the next token. Memorizing Transformer [41] performs a nearest-neighbor search over previous keys and computes the attention to the top nearest items. However, these methods obtain the final results by interpolating between the kNN prediction and the local attention prediction using a tuned parameter as interpolation weight. Therefore, the interpolation does not depend on the current input and does not consider whether the memory contains relevant information.

## Context Length Extrapolation

[p. 4] Transformers have a well-known limitation in extrapolating to contexts longer than what was observed during training [27], even when relative positional encoding is used [36]. Current solutions to address this problem often result in weakened attention scores for long-range tokens, which undermines the benefits of a longer context [27, 36]. Moreover, these methods only work when combined with windowed attention, which restricts direct attention to long-range tokens [36]. The authors hypothesize that the limitation may partially stem from the model's learning of its own positional encoding with the use of causal masking, as demonstrated in [12]. This limitation poses a challenge as the goal is to enable access to long-range tokens during inference at distances that were not observed during training. Solutions are discussed in Section 3.2.1.
