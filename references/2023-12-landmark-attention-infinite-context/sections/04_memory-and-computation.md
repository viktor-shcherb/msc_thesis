# 3.3 Memory & Computation [p. 7]

[p. 7] During training, the method has only a negligible overhead due to the computation of GroupedSoftmax. In particular, the method does not require maintaining a cache of previous values at training time. Furthermore, the training context length is decoupled from the inference context length since it is possible to perform inference at any context length using the method described in Section 3.2 regardless of the train context length. As such, when comparing training time in terms of inference context length, the method offers constant training time (O(1)) whereas training time for a standard transformer scales quadratically with the operational (inference) context length.

Furthermore, in comparison with standard attention over the whole input, the method reduces inference time by computing the attention score over a smaller set of token pairs. For instance, during auto-regressive generation, where tokens are generated sequentially by using the previously generated token as input for obtaining the next one, the traditional approach involves computing attention across all preceding tokens when generating the next token. In contrast, this method allows computing attention solely over landmark tokens, followed by attention computation over the retrieved blocks. Given the constant size of the blocks, the cost of computing the attention over the retrieved blocks remains constant regardless of the total context length. While the cost of finding the most relevant landmark tokens increases linearly with the context length, the rate of increase is 1 every ℓ_block + 1 tokens. This immediately reduces the number of operations by a factor of block length ℓ_block. For example, when using ℓ_block = 50 as in the experiments, this can lead to a 50x boost. Importantly, the same reduction can be obtained in terms of memory. In the standard Transformer, a cache of all previous key and values (KV-cache) is needed to perform the generation efficiently. In contrast, the method only needs immediate access to the landmark tokens and can offload the blocks to slow memory (e.g. CPU), loading only the retrieved blocks. The reduction in memory and compute can be further improved if the search for retrieving blocks is performed using more advanced data structures such as FAISS [15].

It is worth noting that the additional computational overhead introduced by performing two matrix multiplications (one for block selection and another for attention to the retrieved blocks) instead of a single matrix multiplication in the standard setting becomes relatively negligible, especially when dealing with larger inputs.

Finally, the method can be naturally combined with flash attention [10], reducing the overhead further. This is discussed in Appendix F. In this work, to reduce the complexity and allow flexibility in experiments, a high-level implementation (not combined with Flash Attention) is used. However, a version of efficient implementation in Triton [37] is also published.
