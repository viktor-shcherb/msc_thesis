# 6 Conclusion [p. 10]

[p. 10] This work presents a novel method for training attention to retrieve relevant blocks from memory. Unlike previous methods that rely on recurrence to create memory, this approach enables direct access to previous tokens, ensuring accurate information retrieval without the problem of slowly forgetting past data. The method achieves comparable performance to recurrent methods such as Transformer-XL while utilizing less computational resources. Additionally, the attention-based retrieval process allows for tracking and interpretability, providing insights into the information used to generate the output. Importantly, the results highlight the ability of the approach to handle significantly longer context lengths than those encountered during training. Moreover, the authors have shown that this capability can efficiently be incorporated into existing pre-trained models through fine-tuning, showcasing improved retrieval capabilities in the LLaMA 7B language model. Overall, the method enables efficient inference with arbitrary context lengths, making it suitable for accessing large inputs and processing fine-grained information within the large context.
