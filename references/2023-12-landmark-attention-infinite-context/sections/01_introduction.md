# 1 Introduction [p. 1-2]

[p. 1] Large transformers have revolutionized language modeling and demonstrated remarkable abilities to perform various tasks with zero or few examples [4]. This success is largely attributed to the attention mechanism, which allows each token to access the representation of any other token in each layer. However, this flexibility comes with quadratic computational cost and highly problematic memory footprint, limiting the number of tokens that can be attended to, and thus the context length.

Researchers have proposed various solutions to overcome this limitation, including incorporating a form of recurrent memory inside the Transformer architecture, such as Transformer-XL [9]. However, these approaches often sacrifice the random-access flexibility of attention.

An alternative approach is retrieval-based methods that incorporate additional static knowledge by searching for relevant documents in a knowledge base and adding them to the context. However, this requires a separate mechanism (a retriever) to identify relevant documents. Such retrieval models cannot easily be updated to work on fresh long input data, and are not fully compatible with the standard attention mechanism itself, and thus may fail to mimic attention over long documents.

[p. 2] The authors propose a novel approach for overcoming the context length limit by allowing earlier blocks of the input to be directly incorporated into the attention itself. The input is broken into blocks of fixed length, and a special token called a **landmark** is introduced for each block, acting as a gate for attending to its corresponding block. The gating mechanism is controlled by the attention score to the landmark token. At inference time, attention scores on the landmarks allow retrieval of any previous block and integration with standard attention. The idea is illustrated in Figure 1. The approach maintains the random-access flexibility of attention and offers an alternative to recurrent memory approaches.

The model can process any context length at inference time regardless of the context length used at training time. The input is split into chunks and fed sequentially to the model, maintaining a cache of previous tokens (KV cache). When processing each chunk, landmark tokens are used to select the most relevant blocks, and only those blocks are used for computing attention. This reduces computation cost by a factor of block length. In experiments with blocks of 50 tokens, this translates to almost 50x reduction of computation. The overhead of computing attention for retrieved blocks does not depend on input length and becomes negligible for very large inputs.

Furthermore, the same reduction in memory usage is possible since all tokens in a block (except the landmark itself) can be swapped out and only loaded when the corresponding landmark token is activated (see Appendix G). This reduction can be further improved by using special data structures designed for retrieving closest neighbors, such as FAISS [15].

The authors demonstrate efficacy both for training models from scratch and for fine-tuning pre-trained models. In both cases, the model effectively utilizes landmark tokens to retrieve relevant blocks from memory, enabling inference at arbitrary context lengths much longer than those encountered during training. The model obtains comparable performance with Transformer-XL trained to use recurrence on a much larger window. More importantly, fine-tuning LLaMA 7B [38] with this method allows it to retrieve relevant information from contexts with over 32k tokens, which is the context length of GPT-4 [25].

## Primary Advantages

The primary advantages of the method are summarized as [p. 2]:

- Enabling inference at any context length, irrespective of the context length utilized during training, without incurring additional training costs.
- Instantly reducing inference time and memory usage (compared to a model trained to operate at the given context length) by a substantial factor equal to the block size (e.g., 50 in their experiments).
- Compatibility with advanced data structures that can further decrease the resource requirements for operating the model with very large context lengths.

**Figure 1** (p. 2): "An illustration comparing *standard attention* and our *attention with landmarks*. The example shows the (causal) attention given by a current token e to previous ones, illustrating our mechanism with block-size â„“_block = 2. The attention scores rely on the similarity of query vector with the key vector, and in our case also with the landmark vector corresponding to the block. This is why the same token b can have a high (green) attention score when being part of one block and a low (red) attention score when being in other one, despite having the same representative vector in both cases. Landmark tokens (same as regular tokens) have the same vector representation at the first layer. However, this changes as they are updated though depth, leading to the illustrated behavior of attention at the intermediate layers."

The figure shows two diagrams side by side. On the left: standard attention where token "e" attends to all previous tokens [a, b, c, b] with uniform access. On the right: attention with landmarks where tokens are grouped into blocks of size 2 ([a, b], [c, b]), each followed by a landmark token. Token "e" attends to all tokens including landmarks, with the landmark gating the attention to each block -- token b gets high (green) attention in one block but low (red) attention in another block depending on the landmark score.
