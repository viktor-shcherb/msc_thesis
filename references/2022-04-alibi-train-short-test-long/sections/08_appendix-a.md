# A Appendix [p. 15-16]

## A.1 Introduction [p. 15]

[p. 15]

The training speed of transformer LMs gets slower as the input subsequence length $L$ increases. Figure 7 visualizes this.

### Figure 7

**Figure 7** (p. 15): "Training speed of our model and the sinusoidal baseline trained on different amounts of input subsequence tokens $L$."

The figure shows a line plot titled "Training Speed":
- x-axis: Input Tokens ($L$), values: 64, 128, 256, 512, 1024, 1536, 2048, 3072
- y-axis: Words Per Second, values: 0 to 40000
- Two lines: Sinusoidal (orange dots) and ALiBi (blue x markers, dashed line)
- Both lines decrease roughly linearly (on a log-scale x-axis) as input tokens increase
- Sinusoidal and ALiBi have nearly identical training speeds at every input length, with both starting around 35000 WPS at 64 tokens and decreasing to approximately 10000 WPS at 3072 tokens

Table 1 contains the runtimes and memory use statistics for models using the various position methods discussed in this work. [p. 15]

### Table 1

**Table 1** (p. 15): "The speed (during training and evaluation, in words per second) and memory usage (during training) of the rotary, T5 bias, and ALiBi models compared to the sinusoidal baseline on WikiText-103. Training and inference are batched, and speeds are shown for one V100 GPU."

| Position Method | Train Length | Speed (Train) | Speed (Eval.) | Memory |
|---|---|---|---|---|
| Sinusoidal | 512 | 28.5k | 82.1k | 15.3 GB |
| Sinusoidal | 1024 | 26.0k | 77.8k | 19.2 GB |
| Sinusoidal | 3072 | 15.3k | 42.4k | 15.1 GB |
| Rotary | 512 | 20.0k | 43.4k | 17.8 GB |
| Rotary | 1024 | 17.7k | 39.4k | 22.8 GB |
| Rotary | 3072 | 11.5k | 29.5k | 17.8 GB |
| T5 Bias | 512 | 14.4k | 21.8k | 16.9 GB |
| T5 Bias | 1024 | 13.0k | 20.2k | 20.9 GB |
| T5 Bias | 3072 | 4.3k | 4.9k | 15.9 GB |
| ALiBi | 512 | 28.3k | 85.8k | 15.3 GB |
| ALiBi | 1024 | 25.8k | 76.4k | 19.3 GB |
| ALiBi | 3072 | 15.5k | 42.2k | 15.2 GB |

Note: Speed is measured in words per second; higher is better (indicated by up arrow). Memory is measured during training; lower is better (indicated by down arrow). Memory usage is lower at 3072 tokens compared to 1024 because batches are broken into multiple updates. [p. 15]

Tables 2, 3, and 4 show the perplexity and runtime of models using the sinusoidal, rotary, T5 bias, and ALiBi position methods when extrapolating to sequences longer than the ones they were trained on. The models used in these tables were trained on $L = 512$, 1024 and 3072 tokens. [p. 15]

### Table 2

**Table 2** (p. 16): "The sinusoidal, rotary, T5 bias and ALiBi models trained on $L$ = **512** on WikiText-103 and evaluated with different values of $L_{valid}$ on the validation set. **Bold** shows the best score for each model. Inference speeds (in words per second) are from inference on a GPU with batch size of one."

| Inputs | Sinusoidal PPL | Sinusoidal WPS | Rotary PPL | Rotary WPS | T5 Bias PPL | T5 Bias WPS | ALiBi PPL | ALiBi WPS |
|---|---|---|---|---|---|---|---|---|
| 512 | 20.05 | 15046 | 20.07 | 10839 | 19.65 | 11724 | 19.73 | 14726 |
| 513 | 19.98 | 14925 | 20.01 | 10806 | 19.57 | 10491 | 19.62 | 14965 |
| 522 | 19.93 | 15116 | 20.02 | 11295 | 19.57 | 9970 | 19.64 | 15316 |
| 532 | **19.91** | 15358 | 19.98 | 10854 | 19.53 | 10382 | 19.61 | 15383 |
| 542 | **19.91** | 15076 | 19.94 | 10795 | 19.47 | 12270 | 19.57 | 15301 |
| 552 | **19.91** | 16394 | 19.93 | 12267 | 19.47 | 13000 | 19.54 | 16540 |
| 562 | **19.91** | 16646 | 19.87 | 12481 | 19.39 | 12201 | 19.49 | 16385 |
| 572 | 19.95 | 16934 | 19.83 | 12668 | 19.36 | 12851 | 19.46 | 16881 |
| 582 | 20.13 | 16961 | 19.88 | 12594 | 19.41 | 13904 | 19.48 | 17064 |
| 592 | 20.18 | 17243 | 19.84 | 13007 | 19.36 | 13706 | 19.43 | 17289 |
| 602 | 20.40 | 17502 | 19.81 | 12788 | 19.33 | 14102 | 19.38 | 17141 |
| 612 | 20.59 | 17637 | 19.81 | 12601 | 19.27 | 14573 | 19.38 | 17661 |
| 712 | 24.86 | 15614 | **19.79** | 12676 | 19.10 | 13818 | 19.14 | 15637 |
| 812 | 30.82 | 17151 | 20.17 | 13954 | 18.94 | 14377 | 18.99 | 17210 |
| 912 | 37.42 | 17200 | 20.73 | 13887 | 18.86 | 15345 | 18.88 | 17619 |
| 1012 | 43.54 | 16304 | 21.37 | 13759 | 18.79 | 14240 | 18.73 | 16059 |
| 1112 | 50.36 | 16424 | 22.01 | 13891 | **18.77** | 14014 | 18.68 | 16659 |
| 1212 | 58.01 | 17294 | 23.02 | 15245 | 18.87 | 14589 | 18.67 | 17372 |
| 1312 | 63.62 | 15314 | 23.93 | 13698 | 18.84 | 13138 | 18.60 | 15698 |
| 1412 | 70.75 | 15663 | 24.81 | 13928 | 18.87 | 12857 | 18.59 | 15860 |
| 1512 | 76.23 | 15812 | 25.99 | 14248 | 18.91 | 13752 | 18.52 | 16225 |
| 2512 | 132.41 | 15254 | 31.58 | 13456 | 20.41 | 9948 | 18.41 | 15204 |
| 3512 | 178.97 | 13293 | 35.54 | 11850 | 22.91 | 7847 | 18.40 | 13329 |
| 4512 | 209.37 | 11767 | 39.15 | 10485 | 25.91 | 6146 | 18.41 | 11738 |
| 5512 | 240.44 | 10168 | 43.14 | 9020 | 29.54 | 5309 | 18.36 | 9986 |
| 6512 | 271.40 | 9052 | 47.81 | 8108 | 34.48 | 4680 | 18.35 | 9022 |
| 7512 | 293.02 | 8315 | 51.12 | 7483 | 39.29 | 4102 | 18.33 | 8324 |
| 8512 | 305.65 | 7259 | 54.98 | 6718 | 43.08 | 3660 | 18.34 | 7366 |
| 9512 | 336.02 | 6672 | 57.85 | 6211 | 48.90 | 3370 | 18.34 | 6555 |
| 10512 | 341.53 | 6126 | 60.77 | 5575 | 52.95 | 3010 | 18.32 | 6030 |
| 11512 | 362.74 | 5994 | 66.62 | 5445 | 61.38 | 2873 | 18.32 | 5882 |
| 12512 | 373.17 | 5421 | 69.70 | 4988 | 64.94 | 2602 | **18.31** | 5287 |
| 13512 | 382.91 | 5174 | 73.27 | 4692 | OOM | - | **18.31** | 4962 |
| 14512 | 399.98 | 4351 | 75.52 | 4103 | OOM | - | **18.31** | 4352 |
| 15512 | 406.01 | 4291 | 79.25 | 3969 | OOM | - | **18.31** | 4289 |

---
[p. 17 continued]

### Table 3

**Table 3** (p. 17): "The sinusoidal, rotary, T5 bias and ALiBi models trained on $L$ = **1024** on WikiText-103 and evaluated with different values of $L_{valid}$ on the validation set. **Bold** shows the best score for each model. Inference speeds (in words per second) are from inference on a GPU with batch size of one."

| Inputs | Sinusoidal PPL | Sinusoidal WPS | Rotary PPL | Rotary WPS | T5 Bias PPL | T5 Bias WPS | ALiBi PPL | ALiBi WPS |
|---|---|---|---|---|---|---|---|---|
| 1024 | 19.34 | 17002 | 19.33 | 14690 | 18.80 | 14973 | 18.66 | 16951 |
| 1025 | 19.33 | 16630 | 19.34 | 14423 | 18.82 | 14635 | 18.67 | 16690 |
| 1034 | 19.27 | 16589 | 19.28 | 14351 | 18.74 | 14435 | 18.60 | 16707 |
| 1044 | 19.26 | 16760 | 19.27 | 14491 | 18.72 | 14644 | 18.60 | 16667 |
| 1054 | 19.23 | 16747 | 19.26 | 14503 | 18.71 | 14800 | 18.58 | 16833 |
| 1064 | 19.21 | 16676 | 19.22 | 14623 | 18.70 | 14498 | 18.55 | 16941 |
| 1074 | **19.19** | 16879 | 19.19 | 14464 | 18.65 | 14670 | 18.49 | 16936 |
| 1084 | 19.22 | 16942 | 19.23 | 14650 | 18.70 | 14607 | 18.56 | 17090 |
| 1094 | 19.24 | 16771 | 19.22 | 14629 | 18.69 | 14517 | 18.54 | 16880 |
| 1104 | 19.28 | 16870 | 19.27 | 14837 | 18.69 | 14635 | 18.52 | 17009 |
| 1114 | 19.29 | 16795 | 19.27 | 14879 | 18.69 | 14540 | 18.52 | 17050 |
| 1124 | 19.26 | 17312 | **19.18** | 15121 | 18.62 | 14480 | 18.46 | 17571 |
| 1224 | 20.54 | 17901 | 19.38 | 15584 | 18.58 | 14956 | 18.40 | 18013 |
| 1324 | 23.13 | 16308 | 19.96 | 14386 | 18.52 | 13726 | 18.33 | 16422 |
| 1424 | 26.45 | 16217 | 21.27 | 14385 | 18.48 | 13516 | 18.28 | 16121 |
| 1524 | 29.82 | 16377 | 22.59 | 14693 | 18.42 | 13587 | 18.22 | 16659 |
| 1624 | 34.27 | 15928 | 24.34 | 14228 | 18.40 | 12979 | 18.17 | 16053 |
| 1724 | 38.24 | 16640 | 25.66 | 14686 | 18.35 | 12976 | 18.15 | 16607 |
| 1824 | 42.23 | 16840 | 27.63 | 14918 | **18.30** | 13071 | 18.08 | 16846 |
| 1924 | 46.46 | 15071 | 29.64 | 13452 | 18.31 | 11843 | 18.08 | 15118 |
| 2024 | 51.09 | 15591 | 31.17 | 13706 | 18.34 | 11906 | 18.05 | 15557 |
| 3024 | 96.46 | 13639 | 35.67 | 12256 | 18.62 | 8480 | **17.92** | 13668 |
| 4024 | 144.00 | 12441 | 44.30 | 11203 | 19.44 | 7443 | 17.95 | 12402 |
| 5024 | 182.31 | 11431 | 48.31 | 10324 | 20.47 | 6384 | **17.92** | 11394 |
| 6024 | 214.02 | 10238 | 54.78 | 9117 | 21.76 | 5577 | 18.01 | 10119 |
| 7024 | 261.86 | 8785 | 62.83 | 7950 | 23.64 | 4867 | 17.93 | 8779 |
| 8024 | 284.88 | 8132 | 64.91 | 7355 | 25.79 | 4377 | 17.96 | 8086 |
| 9024 | 310.04 | 7045 | 71.91 | 6380 | 27.54 | 3787 | 17.98 | 7001 |
| 10024 | 337.48 | 6633 | 77.70 | 6016 | 29.54 | 3582 | 17.97 | 6583 |
| 11024 | 358.43 | 5722 | 81.15 | 5219 | 31.94 | 3170 | 18.02 | 5641 |
| 12024 | 375.95 | 5560 | 87.51 | 5072 | 33.35 | 2940 | 18.01 | 5294 |
| 13024 | 393.57 | 4691 | 94.74 | 4383 | OOM | - | 17.98 | 4621 |
| 14024 | 403.52 | 4905 | 96.10 | 4546 | OOM | - | 18.01 | 4827 |
| 15024 | 431.66 | 4518 | 99.78 | 4170 | OOM | - | 17.96 | 4447 |
| 16024 | 453.32 | 4239 | 106.99 | 3878 | OOM | - | 17.98 | 4153 |

---
[p. 18 continued]

### Table 4

**Table 4** (p. 18): "The sinusoidal, rotary, T5 bias and ALiBi models trained on $L$ = **3072** on WikiText-103 and evaluated with different values of $L_{valid}$ on the validation set. **Bold** shows the best score for each model. Inference speeds (in words per second) are from inference on a GPU with batch size of one."

| Inputs | Sinusoidal PPL | Sinusoidal WPS | Rotary PPL | Rotary WPS | T5 Bias PPL | T5 Bias WPS | ALiBi PPL | ALiBi WPS |
|---|---|---|---|---|---|---|---|---|
| 3072 | 18.67 | 13380 | 18.57 | 12548 | 18.01 | 8828 | 17.60 | 13866 |
| 3073 | 18.67 | 13773 | 18.57 | 12474 | 18.01 | 8483 | 17.59 | 13793 |
| 3082 | 18.62 | 13741 | 18.54 | 12388 | 17.95 | 8698 | 17.59 | 13778 |
| 3092 | **18.60** | 13742 | 18.48 | 12458 | 17.92 | 8361 | 17.55 | 13783 |
| 3102 | 18.65 | 13701 | 18.52 | 12365 | 17.94 | 8764 | 17.59 | 13747 |
| 3112 | 18.64 | 13809 | 18.51 | 12449 | 17.96 | 8665 | 17.59 | 13827 |
| 3122 | 18.68 | 13722 | 18.52 | 12432 | 17.98 | 8437 | 17.58 | 13795 |
| 3132 | 18.67 | 13825 | 18.54 | 12490 | 17.97 | 8653 | 17.58 | 13784 |
| 3142 | 18.69 | 13543 | 18.52 | 12230 | 17.97 | 8282 | 17.61 | 13572 |
| 3152 | 18.66 | 13520 | 18.56 | 12240 | 17.98 | 8608 | 17.59 | 13523 |
| 3162 | 18.71 | 13501 | 18.56 | 12253 | 18.04 | 8589 | 17.62 | 13598 |
| 3172 | 18.72 | 13563 | 18.55 | 12297 | 17.99 | 8583 | 17.59 | 13625 |
| 3272 | 18.87 | 13453 | 18.55 | 12148 | 17.93 | 8144 | 17.59 | 13482 |
| 3372 | 19.46 | 13533 | 18.50 | 12254 | 17.88 | 8442 | 17.52 | 13565 |
| 3472 | 20.55 | 13047 | 18.52 | 11868 | 17.95 | 7857 | 17.54 | 13107 |
| 3572 | 21.84 | 13128 | 18.50 | 11882 | 17.86 | 7814 | 17.50 | 13170 |
| 3672 | 23.04 | 13106 | 18.49 | 11859 | 17.87 | 7719 | 17.48 | 13196 |
| 3772 | 24.47 | 13287 | 18.54 | 11942 | 17.85 | 7579 | 17.49 | 13312 |
| 3872 | 25.85 | 12621 | **18.40** | 11272 | 17.82 | 7581 | 17.41 | 12566 |
| 3972 | 27.21 | 12379 | 18.48 | 11151 | 17.84 | 7483 | 17.41 | 12324 |
| 4072 | 28.59 | 12178 | 18.59 | 11019 | 17.88 | 6974 | 17.48 | 12212 |
| 5072 | 45.53 | 11076 | 18.80 | 9887 | 17.76 | 6230 | 17.33 | 10938 |
| 6072 | 65.01 | 10114 | 19.50 | 9049 | **17.68** | 5554 | 17.26 | 10133 |
| 7072 | 85.96 | 8647 | 20.60 | 7861 | 17.83 | 4820 | 17.22 | 8670 |
| 8072 | 102.74 | 7755 | 21.60 | 6991 | 18.06 | 4281 | 17.30 | 7729 |
| 9072 | 125.99 | 6953 | 22.14 | 6360 | 18.12 | 3823 | 17.26 | 6939 |
| 10072 | 133.68 | 6646 | 23.21 | 6068 | 18.37 | 3579 | 17.28 | 6597 |
| 11072 | 161.29 | 5663 | 24.39 | 5158 | 18.64 | 3119 | 17.26 | 5585 |
| 12072 | 169.55 | 5567 | 26.70 | 5111 | 18.93 | 2920 | 17.24 | 5397 |
| 13072 | 189.43 | 5044 | 29.33 | 4658 | 19.10 | 2735 | **17.15** | 4809 |
| 14072 | 203.86 | 4915 | 32.21 | 4616 | OOM | - | 17.22 | 4866 |
| 15072 | 221.14 | 4561 | 33.47 | 4292 | OOM | - | 17.23 | 4491 |
| 16072 | 231.29 | 4382 | 34.51 | 4099 | OOM | - | 17.22 | 4312 |
