# 5 Related Work [p. 9]

[p. 9]

In parallel with this work, Wennberg & Henter (2021) introduce a relative position method that, like ALiBi, adds a bias to attention scores that is a function of the distance between the key and query elements. Unlike ALiBi, which uses a non-learned linear function, their method uses a radial-basis function, with multiple trainable parameters (in the authors' experiments, this led to a slight decrease in runtime). In addition, they present experiments on text classification, not on language modeling. They do not explore extrapolation. [p. 9]

The Distance Aware Transformer (Wu et al., 2021) multiplies attention scores by a bias that is a function of the distance between the key and query. This function uses a different, learned parameter in every head. They show results only on text classification. In the authors' experiments (not presented), multiplying attention scores by the bias (instead of adding, as in ALiBi) degraded performance. [p. 9]

Transformer-XL (Dai et al., 2019) presented a language model that uses a cache and can attend to more tokens during inference than it was trained on (by increasing the length of the cache). However, this work presents results only where output length is limited to the $L$ (the training length), and their relative position method is very slow (Press et al., 2021). The Longformer (Beltagy et al., 2020) adapts models trained on shorter sequences to document-level tasks. However, to achieve this they had to partially train their models on longer sequences. ALiBi enables extrapolation without any additional training on longer sequences. [p. 9]

To the authors' knowledge, extrapolation has not been previously explored in transformer language modeling, but it has been investigated previously and concurrently with transformers on other tasks, such as machine translation (Rosendahl et al., 2019; Neishi & Yoshinaga, 2019; Newman et al., 2020; Kiyono et al., 2021), sequence-to-sequence models trained on an artificial dataset (Hupkes et al., 2020), pretrained sequence-to-sequence models tested on arithmetic tasks (Nogueira et al., 2021, Appendix C), models trained with reinforcement learning (Lampinen et al., 2021), image, speech recognition, and machine translation models (Likhomanenko et al., 2021), and protein structure prediction (Jumper et al., 2021, Appendix 1.5). [p. 9]
