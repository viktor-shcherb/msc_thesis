# Model Evaluation [p. 3-4]

## Length Generalization Setup

[p. 3] Following Anil et al. (2022), the authors focus on algorithmic tasks such as copying, addition, etc. For each task, they train on a finite number of examples of up to a certain length and test them on both seen and unseen lengths at inference. Problems are presented as sequence-to-sequence tasks, where the input sequence is the problem instance and the output sequence is the solution.

Formally, let $\mathcal{D} = \{(\mathbf{x}_i, \mathbf{y}_i)\}$ denote a dataset of such task where $\mathbf{x}_i$ is the input and $\mathbf{y}_i$ is the output sequence. For each task a function $\lambda : \mathcal{D} \to \mathbb{N}$ can be defined that returns the length bucket of a task instance $d \in \mathcal{D}$. This can be the number of tokens or any general notion of length/depth of reasoning. Using this function and a threshold $L$, they employ samples where $\lambda \leq L$ for learning the task and samples where $\lambda > L$ for evaluating generalization. The performance on each instance is reported as the exact-match accuracy of its answer with the ground truth.

## Architecture

[p. 3] A conventional decoder-only Transformer architecture is used as a base for all experiments, with five different approaches for encoding positions: **Absolute Position Embedding (APE)**, **ALiBi**, **Rotary**, and **T5's Relative Bias**. Removing the positional encoding entirely (**NoPE**) is also considered. APE uses sinusoidal functions (Vaswani et al., 2017) as the learnable variant cannot produce embeddings for unseen positions. Given the absence of publicly available Transformer-based LMs trained with the aforementioned PEs on the same pretraining data, they train models from scratch for each task on its training data with the autoregressive language modeling objective:

$$\log p_\theta(\mathbf{y}|\mathbf{x}) = \sum_{i=1}^{T} \log p_\theta(y_i | \mathbf{x}, \mathbf{y}_{1:i-1})$$

The same hyperparameters are used for all PEs, employing the "**base**" model size configuration, popular in HuggingFace library (Wolf et al., 2020), resulting in ~107M trainable weights (list of all hyperparameters in Appendix D.2).

## Tasks

[p. 4] The study is concentrated on downstream tasks. The models are evaluated on three categories (Table 1) of synthetic tasks that have been widely used in the literature to investigate length generalization:

1. **Primitive tasks** such as Copying and Reversing (Ontanon et al., 2022)
2. **Mathematical and reasoning tasks** such as Addition (Nye et al., 2021), Polynomial Evaluation, Sorting, Summation (Saxton et al., 2019), Parity (Anil et al., 2022), LEGO (Zhang et al., 2023)
3. **Classical length generalization datasets** such as SCAN (Lake and Baroni, 2018) and PCFG (Hupkes et al., 2020)

These tasks provide complete control over the train-test distribution, while also requiring reasoning and compositionality skills, and serve as fundamental building blocks for more complex tasks. For the first two categories, datasets are generated by first sampling the length of the task instance from the uniform distribution $\mathcal{U}(1, L)$, and then sampling the input and output sequences according to the task's generative process. For the test set, the same procedure is followed but the sample length is drawn from $\mathcal{U}(1, 2L)$ to include both seen and unseen lengths. Throughout the paper, unless otherwise stated, $L = 20$. For the third category of tasks, length generalization splits from the corresponding datasets are used. Table 1 provides an example of each task (more examples in Appendix D.1).

Empirical evaluation is over ten tasks and three seeds per dataset-PE pair.

**Table 1** (p. 4): "Examples of the input and output of the tasks."

| Task | Input Example | Output Example |
|---|---|---|
| **Primitive Tasks** | | |
| Copy | `Copy the following words: <w1> <w2> <w3> <w4> <w5>` | `<w1> <w2> <w3> <w4> <w5>` |
| Reverse | `Reverse the following words: <w1> <w2> <w3> <w4> <w5>` | `<w5> <w4> <w3> <w2> <w1>` |
| **Mathematical and Algorithmic Tasks** | | |
| Addition | `Compute: 5 3 7 2 6 + 1 9 1 7 ?` | `The answer is 5 5 6 4 3.` |
| Polynomial Eval. | `Evaluate x = 3 in ( 3 x ** 0 + 1 x ** 1 + 1 x ** 2 ) % 10 ?` | `The answer is 5.` |
| Sorting | `Sort the following numbers: 3 1 4 1 5 ?` | `The answer is 1 1 3 4 5.` |
| Summation | `Compute: ( 1 + 2 + 3 + 4 + 7 ) % 10 ?` | `The answer is 7.` |
| Parity | `Is the number of 1's even in [ 1 0 0 1 1] ?` | `The answer is No.` |
| LEGO | `If a = -1; b = -a; c = +b; d = +c. Then what is c?` | `The answer is +1.` |
| **Classical Length Generalization Datasets** | | |
| SCAN | `jump twice and run left` | `JUMP JUMP TURN_LEFT RUN` |
| PCFG | `shift prepend K10 R1 K12 , E12 F16` | `F16 K10 R1 K12 E12` |

**Figure 2** (p. 4): "Aggregate ranking of positional encoding methods on length extrapolation across three different groups of tasks. No PE and T5's Relative Bias outperform other encoding methods in these categories."

The figure shows three grouped bar charts (Mean Rank, higher is better, y-axis 0 to ~0.75) for: Primitive Tasks, Mathematical & Reasoning, and Classic Len. Gen. Datasets. In each group, five bars represent NoPE, T5, ALiBi, Rotary, APE. NoPE and T5 consistently rank highest across all three groups.
