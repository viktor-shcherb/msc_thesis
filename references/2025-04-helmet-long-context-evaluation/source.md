# HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly

**Authors:** Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, Danqi Chen
**Affiliations:** Howard Yen, Tianyu Gao, Danqi Chen (Princeton Language and Intelligence, Princeton University); Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat (Intel)

## Publication Status

- **arXiv preprint:** October 2024, arXiv:2410.02694
- **Peer-reviewed:** Yes
- **Conference:** The Thirteenth International Conference on Learning Representations (ICLR 2025), April 24--28, 2025, Singapore
- **Status:** Published conference paper

## Preferred Citation

Cite the ICLR 2025 version:

> Yen, H., Gao, T., Hou, M., Ding, K., Fleischer, D., Izsak, P., Wasserblat, M., & Chen, D. (2025). HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly. In International Conference on Learning Representations (ICLR 2025).

## Links

- arXiv: https://arxiv.org/abs/2410.02694
- OpenReview: https://openreview.net/forum?id=293V3bJbmE
- Code: https://github.com/princeton-nlp/HELMET
- Dataset: https://huggingface.co/datasets/princeton-nlp/HELMET
- Project page: https://princeton-nlp.github.io/HELMET/
- Blog: https://huggingface.co/blog/helmet
