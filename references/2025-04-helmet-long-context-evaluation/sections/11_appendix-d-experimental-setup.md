# Appendix D: Experimental Setup [p. 34]

As previously described, the authors are able to evaluate models across different input lengths [p. 34]. Thus, they evaluate all models at L âˆˆ {8192, 16384, 32768, 65536, 131072} [p. 34]. For the proprietary models, GPT, Gemini, and Claude, they rely on the provider's API [p. 34]. For all open-source models, they evaluate on a H100 GPUs with 80GB of memory [p. 34]. They use the HuggingFace Transformers (Wolf et al., 2020) to load and generate model outputs [p. 34]. They apply instruction-tuned models' chat template whenever applicable [p. 34]. They use FlashAttention2 (Dao, 2023) and BF16 for faster inference [p. 34]. Their compute is limited to 8 H100 GPUs; thus, they are not able to run some of the larger models, such as Command-R or Jamba-1.5-Large, at 128K tokens [p. 34]. They evaluate on 600 examples for JSON KV, NQ, PopQA, and TQA, 300 examples for the MSMARCO and HotpotQA, 500 examples for ICL, and 100 examples for the remaining datasets [p. 34].
