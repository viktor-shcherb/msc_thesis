# Conclusion [p. 10]

In this work, they first identify the shortcomings of long-context evaluation settings and existing benchmarksâ€”over-reliance on synthetic tasks, limited coverage of realistic applications, and unreliable metrics among others [p. 10]. They seek to address these issues by constructing HELMET, an application-centric benchmark with diverse domains and reliable evaluation settings [p. 10]. They then present a comprehensive evaluation of 59 frontier LCLMs across multiple dimensions, including different tasks, input lengths, and model types [p. 10]. Their analysis shows that synthetic tasks poorly predict downstream performance and that different categories exhibit distinct capabilities and trends [p. 10]. Thus, evaluating models on a diverse set of real-world tasks is essential [p. 10]. Furthermore, open-source models still lag behind closed-source models on complex tasks at longer lengths [p. 10]. Finally, they hope that their benchmark and comprehensive evaluation serve as a valuable resource for future research in long-context language modeling [p. 10].
