# References

This file contains bibliographic information for references cited in the section notes.

## Abdin et al. (2024)
Marah Abdin et al. Phi-3 technical report: A highly capable language model locally on your phone, 2024. URL https://arxiv.org/abs/2404.14219.
- Cited in: 03_analysis.md as Phi model family

## Agarwal et al. (2024)
Rishabh Agarwal et al. Many-shot in-context learning. 2024. URL https://arxiv.org/abs/2404.11018.
- Cited in: 04_related-works.md as work on in-context learning

## AI et al. (2024)
01 AI et al. Yi: Open foundation models by 01.ai, 2024.
- Cited in: 04_related-works.md as open-source long-context model effort

## An et al. (2024)
Chenxin An et al. L-eval: Instituting standardized evaluation for long context language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14388-14411, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.776.
- Cited in: 01_introduction.md as existing long-context benchmark
- Cited in: 02_our-benchmark-helmet.md for their length-instruction-enhanced evaluation and reference-free pairwise win-rates with LLMs

## Arora et al. (2023)
Simran Arora et al. Zoology: Measuring and improving recall in efficient language models. 2023.
- Cited in: 04_related-works.md as work on procedural benchmarks for LCLMs

## Asai et al. (2024a)
Akari Asai et al. Reliable, adaptable, and attributable language models with retrieval. 2024b. URL https://arxiv.org/abs/2403.03187.
- Cited in: 02_our-benchmark-helmet.md for SubEM metric

## Bajaj et al. (2018)
Payal Bajaj et al. Ms marco: A human generated machine reading comprehension dataset, 2018. URL https://arxiv.org/abs/1611.09268.
- Cited in: 02_our-benchmark-helmet.md as source of MS MARCO dataset

## Bai et al. (2024)
Yushi Bai et al. LongBench: A bilingual, multitask benchmark for long context understanding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1–20, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.172.
- Cited in: 01_introduction.md as existing long-context benchmark

## Beltagy et al. (2020)
Iz Beltagy et al. Longformer: The long-document Transformer. arXiv:2004.05150, 2020. URL https://arxiv.org/abs/2004.05150.
- Cited in: 03_analysis.md as sliding-window attention architecture
- Cited in: 04_related-works.md as efficient architecture

## Bertsch et al. (2023)
Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R. Gormley. Unlimiformer: Long-range transformers with unlimited length input. Neural Information Processing Systems (NeurIPS), 2023. URL https://openreview.net/forum?id=lNUGJWLCJo.
- Cited in: 04_related-works.md as efficient architecture

## Bertsch et al. (2024)
Amanda Bertsch et al. In-context learning with long-context models: An in-depth exploration. In First Workshop on Long-Context Foundation Models @ ICML 2024, 2024. URL https://openreview.net/forum?id=4KAmcJvUbq.
- Cited in: 02_our-benchmark-helmet.md for many-shot ICL methodology
- Cited in: 04_related-works.md as work on in-context learning

## Bohnet et al. (2022)
Bernd Bohnet et al. Attributed question answering: Evaluation and modeling for attributed large language models. arXiv preprint arXiv:2212.08037, 2022. URL https://arxiv.org/pdf/2212.08037.pdf.
- Cited in: 02_our-benchmark-helmet.md for generation with citations

## Brown et al. (2020)
Tom B Brown et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS), 2020. URL https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.
- Cited in: 02_our-benchmark-helmet.md for in-context learning

## Casanueva et al. (2020)
Iñigo Casanueva et al. Efficient intent detection with dual sentence encoders. In Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, pp. 38–45, 2020. URL https://aclanthology.org/2020.nlp4convai-1.5.
- Cited in: 02_our-benchmark-helmet.md as source of BANKING77 dataset

## Chang et al. (2024)
Yapei Chang et al. Booookscore: A systematic exploration of book-length summarization in the era of LLMs. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=7Ttk3RzDeu.
- Cited in: 01_introduction.md as work on summarization
- Cited in: 04_related-works.md as work on summarization

## Chen et al. (2017)
Danqi Chen et al. Reading Wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1870–1879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https://aclanthology.org/P17-1171.
- Cited in: 02_our-benchmark-helmet.md for open-domain question answering (ODQA)

## Chen et al. (2023)
Shouyuan Chen et al. Extending context window of large language models via positional interpolation, 2023.
- Cited in: 01_introduction.md and 04_related-works.md for position extrapolation techniques

## Dao & Gu (2024)
Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. 2024. URL https://arxiv.org/abs/2405.21060.
- Cited in: 03_analysis.md and 04_related-works.md for hybrid models with SSM modules

## Deutsch et al. (2022)
Daniel Deutsch et al. Re-examining system-level correlations of automatic summarization evaluation metrics. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 6038–6052, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.442. URL https://aclanthology.org/2022.naacl-main.442.
- Cited in: 01_introduction.md for unreliable ROUGE metrics

## Ding et al. (2024)
Yiran Ding et al. Longrope: Extending llm context window beyond 2 million tokens. 2024. URL https://arxiv.org/abs/2402.13753.
- Cited in: 03_analysis.md and 04_related-works.md for context extrapolation training

## Dong et al. (2024)
Zican Dong et al. BAMBOO: A comprehensive benchmark for evaluating long text modeling capacities of large language models. In Nicoletta Calzolari et al. (eds.), Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), May 2024. URL https://aclanthology.org/2024.lrec-main.188.
- Cited in: 04_related-works.md as existing benchmark with limited context lengths

## Dubey et al. (2024)
Abhimanyu Dubey et al. The llama 3 herd of models. 2024. URL https://arxiv.org/abs/2407.21783.
- Cited in: 01_introduction.md and 03_analysis.md as Llama model family
- Cited in: 04_related-works.md as open-source long-context model effort

## Fu et al. (2024)
Yao Fu et al. Data engineering for scaling language models to 128k context. 2024.
- Cited in: 01_introduction.md for long-context evaluation developments
- Cited in: 02_our-benchmark-helmet.md for models without instruction tuning
- Cited in: 04_related-works.md as open-source long-context model effort

## Gao et al. (2023)
Tianyu Gao et al. Enabling large language models to generate text with citations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 6465–6488, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.398. URL https://aclanthology.org/2023.emnlp-main.398.
- Cited in: 01_introduction.md and 02_our-benchmark-helmet.md for generation with citations (ALCE benchmark)

## Goldman et al. (2024)
Omer Goldman et al. Is it really long context if all you need is retrieval? towards genuinely difficult long context nlp. 2024. URL https://arxiv.org/abs/2407.00402.
- Cited in: 04_related-works.md as work on procedural benchmarks for LCLMs

## Goyal et al. (2023)
Tanya Goyal et al. News Summarization and Evaluation in the Era of GPT-3. 2023. URL https://arxiv.org/abs/2209.12356.
- Cited in: 01_introduction.md and 02_our-benchmark-helmet.md for unreliable ROUGE metrics

## Gu & Dao (2024)
Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=tEYskwlYY2.
- Cited in: 04_related-works.md as efficient architecture

## Guo et al. (2024)
[Note: Citation appears in 04_related-works.md for open-source long-context model efforts, but full bibliographic entry not visible in pages 11-26 of the PDF]

## Hsieh et al. (2024)
Cheng-Ping Hsieh et al. RULER: What's the real context size of your long-context language models? In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=kIo6bc765y.
- Cited in: 01_introduction.md for NIAH and RULER
- Cited in: 02_our-benchmark-helmet.md as source of synthetic tasks
- Cited in: 04_related-works.md as procedural benchmark

## Jiang et al. (2023)
Albert Q. Jiang et al. Mistral 7b, 2023.
- Cited in: 03_analysis.md as Mistral model family

## Joshi et al. (2017)
Mandar Joshi et al. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601–1617, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147.
- Cited in: 02_our-benchmark-helmet.md as source of TriviaQA dataset

## Kamoi et al. (2023)
Ryo Kamoi et al. WiCE: Real-World Entailment for Claims in Wikipedia. arXiv preprint arXiv:2303.01432, 2023. URL https://arxiv.org/abs/2303.01432.
- Cited in: 02_our-benchmark-helmet.md for summarization evaluation methodology

## Kamradt (2024)
Garrett Kamradt. Needle in a haystack - pressure testing llms. 2024. URL https://github.com/gkamradt/LLMTest_NeedleInAHaystack.
- Cited in: 01_introduction.md and 04_related-works.md for NIAH task

## Karpinska et al. (2024)
Marzena Karpinska et al. One thousand and one paths: A "novel" challenge for long-context language models. 2024. URL https://arxiv.org/abs/2406.16264.
- Cited in: 03_analysis.md and 04_related-works.md for evaluating long-context models on specific domains

## Kim et al. (2024)
Yekyung Kim et al. FABLES: Evaluating faithfulness and content selection in book-length summarization. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=IHhxQSoaWU.
- Cited in: 04_related-works.md as work on summarization

## Kočiský et al. (2018)
Tomáš Kočiský et al. The NarrativeQA reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317–328, 2018. doi: 10.1162/tacl a 00023. URL https://aclanthology.org/Q18-1023.
- Cited in: 02_our-benchmark-helmet.md as source of NarrativeQA dataset

## Krishna et al. (2023)
Kalpesh Krishna et al. LongEval: Guidelines for human evaluation of faithfulness in long-form summarization. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 1650–1669, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-main.121. URL https://aclanthology.org/2023.eacl-main.121/.
- Cited in: 02_our-benchmark-helmet.md for unreliable ROUGE metrics

## Kwan et al. (2024)
Wai-Chung Kwan et al. M4LE: A multi-ability multi-range multi-task multi-domain long-context evaluation benchmark for large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 15360–15406, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.832. URL https://aclanthology.org/2024.acl-long.832.
- Cited in: 04_related-works.md as existing benchmark with limited context lengths

## Kwiatkowski et al. (2019)
Tom Kwiatkowski et al. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452–466, 2019. doi: 10.1162/tacl a 00276. URL https://aclanthology.org/Q19-1026.
- Cited in: 02_our-benchmark-helmet.md as source of Natural Questions dataset

## Laban et al. (2024)
Philippe Laban et al. Summary of a haystack: A challenge to long-context llms and rag systems. 2024. URL https://arxiv.org/abs/2407.01370.
- Cited in: 04_related-works.md as work on procedural benchmarks for LCLMs

## Larson et al. (2019)
Stefan Larson et al. An evaluation dataset for intent classification and out-of-scope prediction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 1311–1316, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1131. URL https://aclanthology.org/D19-1131.
- Cited in: 02_our-benchmark-helmet.md as source of CLINC150 dataset

## Lee et al. (2024)
Haejun Lee et al. Can long-context language models subsume retrieval, rag, sql, and more?. 2024. URL https://arxiv.org/abs/2406.13121.
- Cited in: 01_introduction.md and 02_our-benchmark-helmet.md for RAG work
- Cited in: 04_related-works.md as work on RAG

## Levy et al. (2024)
Mosh Levy, Alon Jacoby, and Yoav Goldberg. Same task, more tokens: the impact of input length on the reasoning performance of large language models. In Association for Computational Linguistics (ACL), 2024.
- Cited in: 04_related-works.md as work on procedural benchmarks for LCLMs

## Li & Roth (2002)
Xin Li and Dan Roth. Learning question classifiers. In Proceedings of the 19th international conference on Computational linguistics (COLING), pp. 1–7, 2002.
- Cited in: 02_our-benchmark-helmet.md as source of TREC dataset

## Li et al. (2024a)
Jiaqi Li et al. LooGLE: Can long-context language models understand long contexts? In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16304–16335, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.859.
- Cited in: 04_related-works.md as existing benchmark with limited context lengths

## Li et al. (2024b)
Mo Li et al. Needlebench: Can llms do retrieval and reasoning in 1 million context window?. 2024b. URL https://arxiv.org/abs/2407.11963.
- Cited in: 04_related-works.md as work on procedural benchmarks for LCLMs

## Li et al. (2024c)
Tianle Li et al. Long-context llms struggle with long in-context learning. CoRR, abs/2404.02060, 2024c. URL https://doi.org/10.48550/arXiv.2404.02060.
- Cited in: 01_introduction.md and 02_our-benchmark-helmet.md for many-shot ICL
- Cited in: 04_related-works.md as work on in-context learning

## Lieber et al. (2024)
Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor Zusman, and Yoav Shoham. Jamba: A hybrid transformer-mamba language model, 2024. URL https://arxiv.org/abs/2403.19887.
- Cited in: 03_analysis.md and 04_related-works.md for hybrid models

## Lin (2004)
Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013/.
- Cited in: 01_introduction.md and 02_our-benchmark-helmet.md for ROUGE metric

## Liu et al. (2019)
Xingkun Liu et al. Benchmarking natural language understanding services for building conversational agents. In Proceedings of the Ninth International Workshop on Spoken Dialogue Systems (IWSDS), 2019.
- Cited in: 02_our-benchmark-helmet.md as source of NLU dataset

## Liu et al. (2023)
Nelson F. Liu et al. Lost in the middle: How language models use long contexts. 2023.
- Cited in: 02_our-benchmark-helmet.md for gold passage positioning
- Cited in: 04_related-works.md for synthetic tasks

## Mallen et al. (2023)
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and nonparametric memories. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 9802–9822, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.546. URL https://aclanthology.org/2023.acl-long.546.
- Cited in: 04_related-works.md as work on parametric and non-parametric memories

## Min et al. (2022)
Sewon Min et al. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 11048–11064, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.759. URL https://aclanthology.org/2022.emnlp-main.759.
- Cited in: 02_our-benchmark-helmet.md for in-context learning methodology

## OpenAI (2023)
OpenAI. GPT-4 technical report. 2023. URL https://arxiv.org/abs/2303.12865.
- Cited in: 01_introduction.md and 04_related-works.md for GPT-4 as frontier LCLM

## Pan et al. (2024)
Jane Pan, Tianyu Gao, Howard Chen, and Danqi Chen. What in-context learning "learns" in-context: Disentangling task recognition and task learning. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 8298-8319, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.527. URL https://aclanthology.org/2023.findings-acl.527.
- Cited in: 02_our-benchmark-helmet.md for in-context learning methodology

## Peng et al. (2024)
Bowen Peng et al. YaRN: Efficient context window extension of large language models. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=wHBfxhZu1u.
- Cited in: 01_introduction.md and 03_analysis.md for context extrapolation training
- Cited in: 04_related-works.md for position extrapolation techniques

## Petroni et al. (2021)
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. KILT: a benchmark for knowledge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2523–2544, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.200. URL https://aclanthology.org/2021.naacl-main.200.
- Cited in: 02_our-benchmark-helmet.md as source of KILT benchmark

## Qwen et al. (2025)
Qwen: An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Minghao Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyu Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115.
- Cited in: 01_introduction.md and 03_analysis.md as Qwen model family

## Ratner et al. (2023)
Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel context windows for large language models. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 6383–6402, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.352. URL https://aclanthology.org/2023.acl-long.352.
- Cited in: 04_related-works.md for parallel context windows technique

## Robertson & Zaragoza (2009)
Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333–389, apr 2009. ISSN 1554-0669. doi: 10.1561/1500000019. URL https://doi.org/10.1561/1500000019.
- Cited in: 02_our-benchmark-helmet.md for BM25 retrieval algorithm

## Rubin et al. (2022)
Samuel Joseph Amouyal Ohad Rubin, Ori Yoran, Tomer Wolfson, Jonathan Herzig, and Jonathan Berant. QAMPARI: An Open-domain Question Answering Benchmark for Questions with Many Answers from Multiple Paragraphs. arXiv preprint arXiv:2205.12665, 2022. URL https://arxiv.org/abs/2205.12665.
- Cited in: 02_our-benchmark-helmet.md as source of QAMPARI dataset

## Shaham et al. (2023)
Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized CompaRison over long language sequences. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 12007–12021, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.823.
- Cited in: 04_related-works.md as existing benchmark with limited context lengths

## Shaham et al. (2023) - ZeroSCROLLS
Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. ZeroSCROLLS: A zero-shot benchmark for long text understanding. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 9977–9989, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.536. URL https://aclanthology.org/2023.findings-emnlp.536.
- Cited in: 04_related-works.md as existing benchmark

## Shen et al. (2022)
Zejiang Shen, Kyle Lo, Lauren Yu, Nathan Dahlberg, Margo Schlanger, and Doug Downey. Multi-lexsum: Real-world summaries of civil rights lawsuits at multiple granularities. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=z1d8fU1S8Cr.
- Cited in: 02_our-benchmark-helmet.md as source of Multi-LexSum dataset

## Stelmakh et al. (2022)
Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. ASQA: Factoid questions meet long-form answers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 8273–8288, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.566. URL https://aclanthology.org/2022.emnlp-main.566.
- Cited in: 02_our-benchmark-helmet.md as source of ASQA dataset

## Su et al. (2021)
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2021. URL https://arxiv.org/abs/2104.09864.
- Cited in: 01_introduction.md and 04_related-works.md for RoPE position embeddings

## Sun et al. (2023)
Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. Is ChatGPT good at search? investigating large language models as re-ranking agents. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 14918–14937, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.923. URL https://aclanthology.org/2023.emnlp-main.923.
- Cited in: 02_our-benchmark-helmet.md for re-ranking evaluation

## Tay et al. (2021)
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=qVyeW-grC2k.
- Cited in: 04_related-works.md as benchmark for efficient transformers

## Gemini Team et al. (2024)
Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Lespiau, Katie Millican, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. 2024. URL https://arxiv.org/abs/2403.05530.
- Cited in: 01_introduction.md and 03_analysis.md as Gemini model family with extended context
- Cited in: 04_related-works.md as frontier LCLM

## Gemini Team et al. (2024a)
Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, Soroosh Mariooryad, Yifan Ding, Xinyang Geng, Fred Alcober, Roy Frostig, Mark Omernick, Lexi Walker, Cosmin Paduraru, Christina Sorokin, Andrea Tacchetti, Colin Gaffney, Samira Daruki, Olcan Sercinoglu, Zach Gleicher, Juliette Love, Paul Voigtlaender, Rohan Jain, Gabriela Surita, Kareem Mohamed, Rory Blevins, Junwhan Ahn, Tao Zhu, Kornraphop Kawintiranon, Orhan Firat, Yiming Gu, Yujing Zhang, Matthew Rahtz, Manaal Faruqui, Natalie Clay, Justin Gilmer, JD Co-Reyes, Ivo Penchev, Rui Zhu, Nobuyuki Morioka, Kevin Hui, Krishna Haridasan, Victor Campos, Mahdis Mahdieh, Mandy Guo, Samer Hassan, Kevin Kilgour, Arpi Vezer, Heng-Tze Cheng, Raoul de Liedekerke, Siddharth Goyal, Paul Barham, DJ Strouse, Seb Noury, Jonas Adler, Mukund Sundararajan, Sharad Vikram, Dmitry Lepikhin, Michela Paganini, Xavier Garcia, Fan Yang, Dasha Valter, Kiran Vodrahalli, Chulayuth Asawaroengchai, Roman Ring, Norbert Kalb, Livio Baldini Soares, Siddhartha Brahma, David Steiner, Tianhe Yu, Fabian Mentzer, Antoine Toisoul, Lucas Gonzalez, Bibo Xu, Raphael Lopez Kaufman, Laurent El Shafey, Junhyuk Oh, Tom Hennigan, George van den Driessche, Seth Odoom, Mario Lucic, Becca Roelofs, Sid Lall, Amit Marathe, Betty Chan, Santiago Ontanon, Luheng He, Sebastian Riedel, Karel Lenc, Chih-Kuan Yeh, Aakanksha Chowdhery, Yang Xu, Mehran Kazemi, Ehsan Amid, Anastasia Petrushkina, Piyush Patil, Steven Hansen, Dave Orr, Sebastien M. R. Arnold, Jordan Grimstad, Albin Cassirer, Shubham Agrawal, Ondrej Glembek, Yadav, Xi Chen, Elena Gribovskaya, Jacob Austin, Jeffrey Zhao, Kaushal Patel, Paul Komarek, Sophia Austin, Sebastian Borgeaud, Linda Friso, Abhimanyu Goyal, Ben Caine, Kris Cao, Da-Woon Chung, Matthew Lamm, Gabe Barth-Maron, Thais Kagohara, Kate Olszewska, Mia Chen, Kaushik Shivakumar, Rishabh Agarwal, Antoine Briukhov, Rayhan Asat, Mugdha Doshi, Woon Chung, Yuan Zhang, Bat-Orgil Batsaikhan, Mateo Wirth, James Qin, Ivo Damholdt, Tulsee Doshi, Martin Chadwick, Jilin Chen, Sanil Jain, Quoc Le, Arjun Kar, Madhu Gurumurthy, Adam Bloniarz, Jaehoon Lee, Pedram Pejman, Paul Michel, Stephen Spencer, Vladimir Feinberg, Xuehan Xiong, Nikolay Savinov, Charlotte Smith, Shaken Shakeri, Dustin Tran, Mary Chesus, Bernd Bohnet, George Tucker, Tamara von Glehn, Carrie Muir, Yiran Mao, Hideto Kazawa, Ambrose Slone, Kedar Soparkar, Disha Shrivastava, James Manyika, Vishakh Nair, Koray Kavukcuoglu, Demis Hassabis, Oriol Vinyals, Jeff Dean, and Koray Kavukcuoglu. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024a. URL https://arxiv.org/abs/2403.05530.
- Cited in: 01_introduction.md and 03_analysis.md as Gemini model family with extended context
- Cited in: 04_related-works.md as frontier LCLM
- Note: This appears to be the same reference as "Gemini Team et al. (2024)" listed earlier, with full author list now visible on pages 21-22

## Gemini Team et al. (2024b)
Gemini Team: Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Lespiau, Katie Millican, et al. (multiple authors listed on p. 22). Reference to Gemini models with extended context windows.
- Cited in: 04_related-works.md as frontier model with extended context beyond 100K tokens
- Note: Likely refers to an earlier Gemini technical report; full citation details visible on p. 22

## Jamba Team (2024c)
Jamba Team, Barak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, Daniel Gissin, Daniel Jannai, Dor Muhlgay, Dor Zimberg, Edden M Gerber, Elad Dolev, Eran Krakovsky, Erez Safahi, Eyal Schwartz, Gal Cohen, Gal Shachaf, Haim Rozenblum, Hofit Bata, Ido Blass, Inbal Magar, Itay Dalmedigos, Jhonathan Osin, Julie Fadlon, Maria Rozman, Matan Danos, Michael Gokhman, Mor Zusman, Naama Gidron, Nir Ratner, Noam Gat, Noam Rozen, Oded Fried, Ohad Leshno, Omer Antverg, Omri Abend, Opher Lieber, Or Dagan, Orit Cohavi, Raz Alon, Ro'i Belson, Roi Cohen, Rom Gilad, Roman Glozman, Shahar Lev, Shahar Mikuli, Shahar Shiboli, Shelly Gur Arie, Shir Levkovitz, Tomer Asida, Tom Ben Gal, Tom Braude, Uriya Pumerantz, Yehoshua Cohen, Yonatan Belinkov, Yuval Globerson, Yuval Peleg Levy, and Yoav Levine. Jamba-1.5: Hybrid transformer-mamba models at scale, 2024c. URL https://arxiv.org/abs/2408.12570.
- Cited in: 01_introduction.md as Jamba-1.5 model

## Touvron et al. (2023)
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.
- Cited in: 03_analysis.md for Llama 2 tokenizer used to measure input lengths

## Vaswani et al. (2017)
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems (NIPS), 30, 2017. URL https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.
- Cited in: 03_analysis.md for full dense transformer architecture

## Wang et al. (2024a)
Chonghua Wang, Haodong Duan, Songyang Zhang, Dahua Lin, and Kai Chen. Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 3712-3724, Mexico City, Mexico, June 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.205. URL https://aclanthology.org/2024.naacl-long.205.
- Cited in: 04_related-works.md as existing benchmark for long-context QA

## Wang et al. (2024b)
Cunxiang Wang, Ruoxi Ning, Boqi Pan, Tonghui Wu, Qipeng Guo, Cheng Deng, Guangsheng Bao, Xiangkun Hu, Zheng Zhang, Qian Wang, and Yue Zhang. Novelqa: Benchmarking question answering on documents exceeding 200k tokens, 2024b. URL https://arxiv.org/abs/2403.12766.
- Cited in: 04_related-works.md as existing benchmark for long-context QA

## Wang et al. (2024c)
Minzheng Wang, Longze Chen, Fu Cheng, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, Yunshui Li, Min Yang, Fei Huang, and Yongbin Li. Leave no document behind: Benchmarking long-context LLMs with extended multi-doc QA. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 5627-5646, Miami, Florida, USA, November 2024c. Association for Computational Linguistics.
- Cited in: 04_related-works.md as existing benchmark for long-context QA

## Wei et al. (2023)
Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinying Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. Larger language models do in-context learning differently, 2023. URL https://arxiv.org/abs/2303.03846.
- Cited in: 02_our-benchmark-helmet.md for testing task learning vs pre-trained priors in ICL

## Wolf et al. (2020)
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing, 2020. URL https://arxiv.org/abs/1910.03771.
- Cited in: References section as Transformers library

## Xiong et al. (2023)
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma. Effective long-context scaling of foundation models, 2023. URL https://arxiv.org/abs/2309.16039.
- Cited in: 01_introduction.md and 04_related-works.md for position extrapolation techniques

## Xu et al. (2024)
Xiaoyue Xu, Qinyuan Ye, and Xiang Ren. Stress-testing long-context language models with lifelong ICL and task haystack. In First Workshop on Long-Context Foundation Models @ ICML 2024, 2024. URL https://openreview.net/forum?id=5ltgRXnZt.
- Cited in: 02_our-benchmark-helmet.md and 04_related-works.md for many-shot ICL work

## Yang et al. (2018)
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2369–2380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259.
- Cited in: 02_our-benchmark-helmet.md as source of HotpotQA dataset
- Cited in: 09_appendix-b-datasets.md as source of HotpotQA dataset

## Yen et al. (2024)
Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, Danqi Chen. HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly. Published as a conference paper at ICLR 2025. arXiv:2410.02694v3 [cs.CL] 6 Mar 2025.
- This is the current paper
- Cited in: 04_related-works.md for hybrid architecture models

## Yen et al. (2024) - Parallel Context
Howard Yen, Tianyu Gao, and Danqi Chen. Long-context language modeling with parallel context encoding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2588–2610, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.142. URL https://aclanthology.org/2024.acl-long.142.
- Cited in: 04_related-works.md for parallel context encoding

## Yuan et al. (2024)
Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang. Lv-eval: A balanced long-context benchmark with 5 length levels up to 256k, 2024. URL https://arxiv.org/abs/2402.05136.
- Cited in: 04_related-works.md and 08_appendix-a-comparison-with-other-benchmarks.md for long-context benchmarks and ICL work

## Zhang & Bansal (2021)
Shiyue Zhang and Mohit Bansal. Finding a balanced degree of automation for summary evaluation. In Empirical Methods in Natural Language Processing (EMNLP), pp. 6617–6632, 2021. doi: 10.18653/v1/2021.emnlp-main.531. URL https://aclanthology.org/2021.emnlp-main.531.
- Cited in: 02_our-benchmark-helmet.md for summarization evaluation methodology (decomposing summaries into atomic claims)

## Zhang et al. (2024a)
Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan Lin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min Zhang. mgte: Generalized long-context text representation and ranking models for multilingual text retrieval, 2024a. URL https://arxiv.org/abs/2407.19669.
- Cited in: 02_our-benchmark-helmet.md footnote 3 as Alibaba-NLP/gte-large-en-v1.5 retrieval model

## Zhang et al. (2024b)
Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Sun. ∞bench: Extending long context evaluation beyond 100k tokens, 2024b. URL https://arxiv.org/abs/2402.13718.
- Cited in: 01_introduction.md as existing long-context benchmark
- Cited in: 02_our-benchmark-helmet.md for ∞BENCH benchmark
- Cited in: 03_analysis.md for ∞BENCH benchmark
- Cited in: 04_related-works.md as existing benchmark
- Cited in: 08_appendix-a-comparison-with-other-benchmarks.md extensively for ∞BENCH comparison

## Zheng et al. (2023)
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id=uccHPGDlao.
- Cited in: References section for MT-bench and LLM-as-a-judge methodology

---

**Note:** The references section spans pages 11-28. All pages have now been processed (pages 11-15 processed in earlier window, pages 16-26 processed in previous window, pages 26-28 processed in this window). Full bibliographic details have been added for all citations that appear in the section notes.
