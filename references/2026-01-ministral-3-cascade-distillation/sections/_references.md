# References

## [1]
Pravesh Agrawal et al. *Pixtral 12B.* arXiv:2410.07073, 2024.
- Cited in `02_model-architecture.md` for the vision encoder architecture lineage.

## [2]
Joshua Ainslie et al. *GQA: Training generalized multi-query transformer models from multi-head checkpoints.* arXiv:2305.13245, 2023.
- Cited in `02_model-architecture.md` for grouped-query attention.

## [3]
Jacob Austin et al. *Program synthesis with large language models.* arXiv:2108.07732, 2021.
- Cited in `04_results.md` as the MBPP benchmark source.

## [4]
Shuai Bai et al. *Qwen3-VL technical report.* arXiv:2511.21631, 2025.
- Cited in `01_introduction.md` and `04_results.md` as Qwen3-VL comparison baseline.

## [5]
Peter Clark et al. *Think you have solved question answering? Try ARC, the AI2 reasoning challenge.* arXiv:1803.05457, 2018.
- Cited in `04_results.md` for ARC-Challenge.

## [6]
DeepSeek-AI et al. *DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning.* arXiv:2501.12948, 2025.
- Cited in `03_training-recipe.md` for GRPO recipe context.

## [7]
Abhimanyu Dubey et al. *The Llama 3 herd of models.* arXiv:2407.21783, 2024.
- Cited in `01_introduction.md` for token-budget comparison.

## [8]
Shangmin Guo et al. *Direct language model alignment from online AI feedback.* arXiv:2402.04792, 2024.
- Cited in `03_training-recipe.md` for ODPO method.

## [9]
Dan Hendrycks et al. *Measuring massive multitask language understanding.* arXiv:2009.03300, 2020.
- Cited in `04_results.md` for MMLU/MMLU-Redux context.

## [10]
Dan Hendrycks et al. *Measuring mathematical problem solving with the MATH dataset.* arXiv:2103.03874, 2021.
- Cited in `04_results.md` for MATH benchmark.

## [11]
Naman Jain et al. *LiveCodeBench: Holistic and contamination free evaluation of large language models for code.* arXiv:2403.07974, 2024.
- Cited in `04_results.md` for LiveCodeBench.

## [12]
Mandar Joshi et al. *TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension.* arXiv:1705.03551, 2017.
- Cited in `04_results.md` for TriviaQA.

## [13]
Aishwarya Kamath et al. *Gemma 3 technical report.* arXiv:2503.19786, 2025.
- Cited in `01_introduction.md` and `04_results.md` as a model family baseline.

## [14]
Tom Kwiatkowski et al. *Natural Questions: A benchmark for question answering research.* TACL, 2019.
- Cited in `04_results.md` for NaturalQS/Natural Questions.

## [15]
Guokun Lai et al. *RACE: Large-scale reading comprehension dataset from examinations.* EMNLP, 2017.
- Cited in `04_results.md` for RACE High.

## [16]
Tianle Li et al. *From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline.* arXiv:2406.11939, 2024.
- Cited in `04_results.md` for Arena Hard.

## [17]
Bill Yuchen Lin et al. *WildBench: Benchmarking LLMs with challenging tasks from real users in the wild.* arXiv:2406.04770, 2024.
- Cited in `04_results.md` for WildBench.

## [18]
Pan Lu et al. *MathVista: Evaluating mathematical reasoning of foundation models in visual contexts.* ICLR, 2024.
- Cited in `04_results.md` for MathVista.

## [19]
MetaAI. *The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation.* 2025.
- Cited in `02_model-architecture.md` and `03_training-recipe.md` for position-based temperature scaling context.

## [20]
Saurav Muralidharan et al. *Compact language models via pruning and knowledge distillation.* arXiv:2407.14679, 2024.
- Cited in `03_training-recipe.md` as pruning/distillation prior work.

## [21]
Ken M. Nakanishi. *Scalable-softmax is superior for attention.* arXiv:2501.19399, 2025.
- Cited in `02_model-architecture.md` and `03_training-recipe.md` for softmax temperature scaling.

## [22]
Long Ouyang et al. *Training language models to follow instructions with human feedback.* NeurIPS, 2022.
- Cited in `03_training-recipe.md` for instruction-following setup.

## [23]
Bowen Peng et al. *YaRN: Efficient context window extension of large language models.* arXiv:2309.00071, 2023.
- Cited in `02_model-architecture.md` and `03_training-recipe.md` for long-context extension.

## [24]
Aryo Perez et al. *Are we done with MMLU?* arXiv:2406.04127, 2024.
- Cited in `04_results.md` for MMLU-Redux context.

## [25]
Rafael Rafailov et al. *Direct preference optimization: Your language model is secretly a reward model.* arXiv:2305.18290, 2023.
- Cited in `03_training-recipe.md` for DPO formulation context.

## [26]
Abhinav Rastogi et al. *Magistral.* arXiv:2506.10910, 2025.
- Cited in `03_training-recipe.md` for 3B distillation teacher and GRPO pipeline details.

## [27]
David Rein et al. *GPQA: A graduate-level Google-proof Q&A benchmark.* First Conference on Language Modeling, 2024.
- Cited in `04_results.md` for GPQA Diamond.

## [28]
Zhihong Shao et al. *DeepSeekMath: Pushing the limits of mathematical reasoning in open language models.* arXiv:2402.03300, 2024.
- Cited in `03_training-recipe.md` for GRPO citation in Figure 1 caption.

## [29]
Noam Shazeer. *GLU variants improve transformer.* arXiv:2002.05202, 2020.
- Cited in `02_model-architecture.md` and `03_training-recipe.md` for SwiGLU.

## [30]
Sharath T. Sreenivas et al. *LLM pruning and distillation in practice: The Minitron approach.* arXiv:2408.11796, 2024.
- Cited in `03_training-recipe.md` as a pruning baseline.

## [31]
Jianlin Su et al. *RoFormer: Enhanced transformer with rotary position embedding.* arXiv:2104.09864, 2021.
- Cited in `02_model-architecture.md` for RoPE.

## [32]
Mingjie Sun et al. *A simple and effective pruning approach for large language models.* arXiv:2306.11695, 2023.
- Cited in `03_training-recipe.md` for pruning lineage (Wanda).

## [33]
Ashish Vaswani et al. *Attention is all you need.* NeurIPS, 2017.
- Cited in `02_model-architecture.md` for the decoder-only Transformer backbone.

## [34]
An Yang et al. *Qwen3 technical report.* arXiv:2505.09388, 2025.
- Cited in `01_introduction.md` and `04_results.md` for training-budget and model-baseline comparisons.

## [35]
Xiang Yue et al. *MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI.* CVPR, 2024.
- Cited in `04_results.md` for MMMU benchmark.

## [36]
Biao Zhang and Rico Sennrich. *Root mean square layer normalization.* NeurIPS, 2019.
- Cited in `02_model-architecture.md` for RMSNorm.

## [37]
Wanjun Zhong et al. *AGIEval: A human-centric benchmark for evaluating foundation models.* arXiv:2304.06364, 2023.
- Cited in `04_results.md` for AGIEval.
