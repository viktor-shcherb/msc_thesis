# References cited in section notes

*Only references that appear in the section note files are listed here.*

---

- **[1]** A. Abboud, V. V. Williams, and O. Weimann. Consequences of faster alignment of sequences. In *International Colloquium on Automata, Languages, and Programming*, pages 39-51. Springer, 2014. — Cited in 03_theoretical-results.md and 09_appendix-c-limitations.md for the Orthogonal Vector Conjecture (OVC) hardness results.

- **[2]** A. Abboud, A. Backurs, and V. V. Williams. Tight hardness results for lcs and other sequence similarity measures. In *2015 IEEE 56th Annual Symposium on Foundations of Computer Science*, pages 59-78. IEEE, 2015. — Cited in 03_theoretical-results.md and 09_appendix-c-limitations.md for the Orthogonal Vector Conjecture (OVC) hardness results.

- **[4]** J. Ainslie, S. Ontanon, C. Alberti, P. Pham, A. Ravula, and S. Sanghai. Etc: Encoding long and structured data in transformers. *arXiv preprint arXiv:2004.08483*, 2020. — Cited in 01_introduction.md as closely related work (Extended Transformers Construction) that uses global tokens; cited in 11_appendix-e-nlp-experiments-details.md for differences between Longformer and BIGBIRD-etc.

- **[6]** J. Alt, R. Ducatez, and A. Knowles. Extremal eigenvalues of critical Erdos-Renyi graphs. *arXiv preprint arXiv:1905.03243*, 2019. — Cited in 02_bigbird-architecture.md for spectral properties of random graphs (second eigenvalue).

- **[7]** A. Backurs and P. Indyk. Edit distance cannot be computed in strongly subquadratic time (unless seth is false). In *Proceedings of the forty-seventh annual ACM symposium on Theory of computing*, pages 51-58, 2015. — Cited in 03_theoretical-results.md and 09_appendix-c-limitations.md for the Orthogonal Vector Conjecture (OVC) hardness results.

- **[8]** I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. *arXiv preprint arXiv:2004.05150*, 2020. — Cited in 01_introduction.md as related work (localized sliding window mask with global tokens); cited in 04_experiments-nlp.md as baseline in QA (Tab. 3) and for BPC comparison; cited in 11_appendix-e-nlp-experiments-details.md for WikiHop setup following Beltagy et al. and E.3 comparison with Longformer.

- **[9]** F. Benaych-Georges, C. Bordenave, A. Knowles, et al. Largest eigenvalues of sparse inhomogeneous Erdos-Renyi graphs. *Annals of Probability*, 47(3):1653-1676, 2019. — Cited in 02_bigbird-architecture.md for spectral properties of random graphs.

- **[10]** F. Benaych-Georges, C. Bordenave, A. Knowles, et al. Spectral radii of sparse random matrices. In *Annales de l'Institut Henri Poincare, Probabilites et Statistiques*, volume 56, pages 2141-2161. Institut Henri Poincare, 2020. — Cited in 02_bigbird-architecture.md for spectral properties of random graphs.

- **[11]** R. Bharanikumar, K. A. R. Premkumar, and A. Palaniappan. Promoterpredict: sequence-based modelling of escherichia coli sigma70 promoter strength yields logarithmic dependence between promoter strength and sequence. *PeerJ*, 6:e5862, 2018. — Cited in 05_experiments-genomics.md for promoter site prediction methods.

- **[12]** S. Buldyrev, A. Goldberger, S. Havlin, R. Mantegna, M. Matsa, C.-K. Peng, M. Simons, and H. Stanley. Long-range correlation properties of coding and noncoding dna sequences: Genbank analysis. *Physical Review E*, 51(5):5084, 1995. — Cited in 05_experiments-genomics.md; many functional effects in DNA are highly non-local.

- **[13]** A. Busia, G. E. Dahl, C. Fannjiang, D. H. Alexander, E. Dorfman, R. Poplin, C. Y. McLean, P.-C. Chang, and M. DePristo. A deep learning approach to pattern recognition for short dna sequences. *BioRxiv*, page 353474, 2019. — Cited in 05_experiments-genomics.md for deep learning for genomics data.

- **[14]** J. Chen, S.-t. Lin, and G. Durrett. Multi-hop question answering via reasoning chains. *arXiv preprint arXiv:1910.02610*, 2019. — Cited in 04_experiments-nlp.md as baseline MultiHop in QA test results (Tab. 3).

- **[15]** Y.-C. Chen, Z. Gan, Y. Cheng, J. Liu, and J. Liu. Distilling the knowledge of bert for text generation. *arXiv preprint arXiv:1911.03829*, 2019. — Cited in 01_introduction.md; Transformers used for generation.

- **[16]** R. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences with sparse transformers. *arXiv preprint arXiv:1904.10509*, 2019. — Cited in 01_introduction.md; proposed a sparse model reducing complexity to O(n sqrt(n)). Also cited in 11_appendix-e-nlp-experiments-details.md in E.3 comparison with Longformer.

- **[17]** F. Chung and L. Lu. The average distances in random graphs with given expected degrees. *Proceedings of the National Academy of Sciences*, 99(25):15879-15882, 2002. — Cited in 02_bigbird-architecture.md for logarithmic shortest path in Erdos-Renyi random graphs.

- **[19]** K. Clark, U. Khandelwal, O. Levy, and C. D. Manning. What does bert look at? an analysis of bert's attention. *arXiv preprint arXiv:1906.04341*, 2019. — Cited in 02_bigbird-architecture.md; neighboring inner-products are extremely important.

- **[20]** A. Cohan, F. Dernoncourt, D. S. Kim, T. Bui, S. Kim, W. Chang, and N. Goharian. A discourse-aware attention model for abstractive summarization of long documents. *arXiv preprint arXiv:1804.05685*, 2018. — Cited in 04_experiments-nlp.md as baseline Long-Doc-Seq2Seq in summarization (Tab. 4); cited in 11_appendix-e-nlp-experiments-details.md for Arxiv and PubMed summarization datasets (Tab. 18).

- **[21]** Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. *arXiv:1901.02860*, 2019. — Cited in 01_introduction.md; proposed auto-regressive models for left-to-right LM.

- **[22]** J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. *arXiv preprint arXiv:1810.04805*, 2018. — Cited in 01_introduction.md and 04_experiments-nlp.md as foundational pretrained transformer model (BERT); MLM objective reference. Also cited in 12_appendix-f-genomics-experiments-details.md for MLM and NSP pretraining tasks in F.1.

- **[24]** R. Dreos, G. Ambrosini, R. Cavin Perier, and P. Bucher. Epd and epdnew, high-quality promoter resources in the next-generation sequencing era. *Nucleic acids research*, 41(D1):D157-D164, 2013. — Cited in 05_experiments-genomics.md; Eukaryotic Promoter Database (EPDnew) used for promoter region prediction. Also cited in 12_appendix-f-genomics-experiments-details.md for EPDnew dataset in F.2.

- **[25]** G. Erkan and D. R. Radev. Lexrank: Graph-based lexical centrality as salience in text summarization. *Journal of artificial intelligence research*, 22:457-479, 2004. — Cited in 04_experiments-nlp.md as baseline LexRank in summarization (Tab. 4).

- **[26]** Y. Fang, S. Sun, Z. Gan, R. Pillai, S. Wang, and J. Liu. Hierarchical graph network for multi-hop question answering. *arXiv preprint arXiv:1911.03631*, 2019. — Cited in 04_experiments-nlp.md as baseline HGN in QA test results (Tab. 3).

- **[27]** L. A. Gates, C. E. Foulds, and B. W. O'Malley. Histone marks in the 'driver's seat': functional roles in steering the transcription cycle. *Trends in biochemical sciences*, 42(12):977-989, 2017. — Cited in 05_experiments-genomics.md; HM task known to have longer-range correlations.

- **[30]** M. Ghandi, D. Lee, M. Mohammad-Noori, and M. A. Beer. Enhanced regulatory sequence prediction using gapped k-mer features. *PLoS computational biology*, 10(7), 2014. — Cited in 05_experiments-genomics.md as baseline gkm-SVM in chromatin-profile prediction (Tab. 7).

- **[31]** A. Gidiotis and G. Tsoumakas. A divide-and-conquer approach to the summarization of academic articles. *arXiv preprint arXiv:2004.06190*, 2020. — Cited in 04_experiments-nlp.md as baseline Dancer in summarization (Tab. 4).

- **[32]** M. Gong. *ReflectionNet*, 2020 (accessed June 3, 2020). URL https://www.microsoft.com/en-us/research/people/migon/. — Cited in 04_experiments-nlp.md as baseline ReflectionNet in QA test results (Tab. 3).

- **[33]** S. Gray, A. Radford, and D. P. Kingma. Gpu kernels for block-sparse weights. *arXiv preprint arXiv:1711.09224*, 3, 2017. — Cited in 10_appendix-d-implementation-details.md; sparse multiplications cannot be efficiently implemented in GPUs.

- **[34]** K. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang. Realm: Retrieval-augmented language model pre-training. *arXiv preprint arXiv:2002.08909*, 2020. — Cited in 01_introduction.md as a method achieving strong performance by selecting relevant contexts (REALM); cited in 11_appendix-e-nlp-experiments-details.md as CC-News pretraining dataset (Tab. 9).

- **[37]** S. Hochreiter and J. Schmidhuber. Long short-term memory. *Neural computation*, 9(8):1735-1780, 1997. — Cited in 01_introduction.md; Transformers are known to vastly outperform LSTM.

- **[38]** S. Hoory, N. Linial, and A. Wigderson. Expander graphs and their applications. *Bulletin of the American Mathematical Society*, 43(4):439-561, 2006. — Cited in 02_bigbird-architecture.md for spectral properties of random graphs as expanders.

- **[39]** G. Izacard and E. Grave. Leveraging passage retrieval with generative models for open domain question answering. *arXiv preprint arXiv:2007.01282*, 2020. — Cited in 04_experiments-nlp.md as baseline Fusion-in-Decoder in QA test results (Tab. 3).

- **[41]** M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In *Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics*, Vancouver, Canada, July 2017. Association for Computational Linguistics. — Cited in 04_experiments-nlp.md and 11_appendix-e-nlp-experiments-details.md; TriviaQA-wiki dataset (Tab. 11).

- **[42]** M. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and O. Levy. Spanbert: Improving pre-training by representing and predicting spans. *Transactions of the Association for Computational Linguistics*, 8:64-77, 2020. — Cited in 01_introduction.md as a method selecting relevant contexts (SpanBERT); cited in 04_experiments-nlp.md as baseline in QA test results (Tab. 3).

- **[43]** E. Katzav, O. Biham, and A. K. Hartmann. Distribution of shortest path lengths in subcritical Erdos-Renyi networks. *Physical Review E*, 98(1):012301, 2018. — Cited in 02_bigbird-architecture.md for logarithmic shortest path in Erdos-Renyi random graphs.

- **[46]** E. Khurana, Y. Fu, D. Chakravarty, F. Demichelis, M. A. Rubin, and M. Gerstein. Role of non-coding sequence variants in cancer. *Nature Reviews Genetics*, 17(2):93, 2016. — Cited in 05_experiments-genomics.md; diseases correlated to non-coding genomic variations.

- **[49]** N. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In *International Conference on Learning Representations*, 2019. — Cited in 01_introduction.md; reduced complexity to O(n log(n)) using LSH.

- **[50]** T. Kudo and J. Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. *arXiv preprint arXiv:1808.06226*, 2018. — Cited in 05_experiments-genomics.md; byte-pair encoding for DNA tokenization. Also cited in 12_appendix-f-genomics-experiments-details.md for sentencepiece tokenization in F.1.

- **[51]** V. Kumar, A. Choudhary, and E. Cho. Data augmentation using pre-trained transformer models. *arXiv preprint arXiv:2003.02245*, 2020. — Cited in 01_introduction.md; pretraining improvement in low data regime.

- **[52]** T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, et al. Natural questions: a benchmark for question answering research. *Transactions of the Association for Computational Linguistics*, 7:453-466, 2019. — Cited in 04_experiments-nlp.md and 11_appendix-e-nlp-experiments-details.md; Natural Questions dataset (Tab. 11).

- **[54]** K. Lee, M.-W. Chang, and K. Toutanova. Latent retrieval for weakly supervised open domain question answering. *arXiv preprint arXiv:1906.00300*, 2019. — Cited in 01_introduction.md as a method selecting relevant contexts (ORQA).

- **[55]** J. J. Levy, A. J. Titus, C. L. Petersen, Y. Chen, L. A. Salas, and B. C. Christensen. Methylnet: an automated and modular deep learning approach for dna methylation analysis. *BMC bioinformatics*, 21(1):1-15, 2020. — Cited in 05_experiments-genomics.md; methylation analysis.

- **[57]** P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Kuttler, M. Lewis, W.-t. Yih, T. Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. *arXiv preprint arXiv:2005.11401*, 2020. — Cited in 01_introduction.md as a method selecting relevant contexts (RAG).

- **[58]** W. Liang. Segmenting dna sequence into words based on statistical language model. *Nature Precedings*, pages 1-1, 2012. — Cited in 05_experiments-genomics.md; DNA segmentation into tokens and SRILM baseline (Tab. 5).

- **[59]** H. Lin, Z.-Y. Liang, H. Tang, and W. Chen. Identifying sigma70 promoters with novel pseudo nucleotide composition. *IEEE/ACM transactions on computational biology and bioinformatics*, 2017. — Cited in 05_experiments-genomics.md; promoter region identification methods.

- **[60]** J. Lin, D. Quan, V. Sinha, K. Bakshi, D. Huynh, B. Katz, and D. R. Karger. What makes a good answer? the role of context in question answering. In *Proceedings of the Ninth IFIP TC13 International Conference on Human-Computer Interaction (INTERACT 2003)*, pages 25-32, 2003. — Cited in 01_introduction.md; QA as a task requiring larger context.

- **[61]** D. Liu, Y. Gong, J. Fu, Y. Yan, J. Chen, D. Jiang, J. Lv, and N. Duan. Rikinet: Reading wikipedia pages for natural question answering. *arXiv preprint arXiv:2004.14560*, 2020. — Cited in 04_experiments-nlp.md as baseline RikiNet-v2 in QA test results (Tab. 3).

- **[63]** Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692*, 2019. — Cited in 01_introduction.md alongside BERT; cited in 04_experiments-nlp.md for pretraining (RoBERTa). Also cited in 11_appendix-e-nlp-experiments-details.md for warm starting encoder-decoder in E.5.

- **[65]** L. Martin, B. Muller, P. J. O. Suarez, Y. Dupont, L. Romary, E. V. de la Clergerie, D. Seddah, and B. Sagot. Camembert: a tasty french language model. *arXiv preprint arXiv:1911.03894*, 2019. — Cited in 01_introduction.md; Transformers used for POS tagging.

- **[66]** D. Miller. Leveraging bert for extractive text summarization on lectures. *arXiv preprint arXiv:1906.04165*, 2019. — Cited in 01_introduction.md; Transformers used for summarization.

- **[68]** A. Nenkova and L. Vanderwende. The impact of frequency on summarization. *Microsoft Research, Redmond, Washington, Tech. Rep. MSR-TR-2005*, 101, 2005. — Cited in 04_experiments-nlp.md as baseline SumBasic in summarization (Tab. 4).

- **[71]** M. Oubounyt, Z. Louadi, H. Tayara, and K. T. Chong. Deepromoter: Robust promoter predictor using deep learning. *Frontiers in genetics*, 10, 2019. — Cited in 05_experiments-genomics.md; promoter site prediction and dataset compilation. Baseline DeePromoter in Tab. 6. Also cited in 12_appendix-f-genomics-experiments-details.md for EPDnew dataset and negative example construction in F.2.

- **[72]** J. Perez, J. Marinkovic, and P. Barcelo. On the turing completeness of modern neural network architectures. *arXiv preprint arXiv:1901.03429*, 2019. — Cited in 01_introduction.md, 03_theoretical-results.md, 07_appendix-a-universal-approximators.md, and 08_appendix-b-turing-completeness.md; showed the full transformer is Turing Complete. Key reference for the Turing completeness proof. Notation in App. A follows Perez et al. App. B extends results to the Perez et al. setting, reusing their proof structure with amendments for sparse attention.

- **[73]** J. Qiu, H. Ma, O. Levy, S. W.-t. Yih, S. Wang, and J. Tang. Blockwise self-attention for long document understanding. *arXiv preprint arXiv:1911.02972*, 2019. — Cited in 01_introduction.md; reduced complexity by using block sparsity.

- **[74]** J. W. Rae, A. Potapenko, S. M. Jayakumar, and T. P. Lillicrap. Compressive transformers for long-range sequence modelling. *arXiv preprint arXiv:1911.05507*, 2019. — Cited in 01_introduction.md; proposed auto-regressive models for left-to-right LM.

- **[75]** C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. *arXiv preprint arXiv:1910.10683*, 2019. — Cited in 01_introduction.md as a foundational pretrained transformer model (T5).

- **[76]** S. Rothe, S. Narayan, and A. Severyn. Leveraging pre-trained checkpoints for sequence generation tasks. *arXiv preprint arXiv:1907.12461*, 2019. — Cited in 04_experiments-nlp.md; pretraining helps in generative tasks. Baseline + RoBERTa in summarization (Tab. 4). Also cited in 11_appendix-e-nlp-experiments-details.md for warm starting encoder-decoder and as baseline in shorter summarization (Tab. 20).

- **[77]** A. See, P. J. Liu, and C. D. Manning. Get to the point: Summarization with pointer-generator networks. *arXiv preprint arXiv:1704.04368*, 2017. — Cited in 04_experiments-nlp.md as baseline Pntr-Gen-Seq2Seq in summarization (Tab. 4). Also cited in 11_appendix-e-nlp-experiments-details.md as baseline PtGen in shorter summarization (Tab. 20).

- **[78]** E. Sharma, C. Li, and L. Wang. Bigpatent: A large-scale dataset for abstractive and coherent summarization. *arXiv preprint arXiv:1906.03741*, 2019. — Cited in 04_experiments-nlp.md; BigPatents dataset where salient content is evenly distributed. Also cited in 11_appendix-e-nlp-experiments-details.md for Tab. 18 summarization dataset statistics.

- **[80]** D. A. Spielman and S.-H. Teng. Spectral sparsification of graphs. *SIAM Journal on Computing*, 40(4):981-1025, 2011. — Cited in 02_bigbird-architecture.md for random graphs as expanders approximating complete graphs (spectral sparsification).

- **[81]** S. Subramanian, R. Li, J. Pilault, and C. Pal. On extractive and abstractive neural document summarization with transformer language models. *arXiv preprint arXiv:1909.03186*, 2019. — Cited in 04_experiments-nlp.md as baselines Sent-CLF, Sent-PTR, Extr-Abst-TLM in summarization (Tab. 4).

- **[82]** S. Sukhbaatar, E. Grave, P. Bojanowski, and A. Joulin. Adaptive attention span in transformers. *arXiv preprint arXiv:1905.07799*, 2019. — Cited in 01_introduction.md; proposed auto-regressive models for left-to-right LM.

- **[83]** C. Sun, L. Huang, and X. Qiu. Utilizing bert for aspect-based sentiment analysis via constructing auxiliary sentence. *arXiv preprint arXiv:1903.09588*, 2019. — Cited in 01_introduction.md; Transformers used for sentiment analysis.

- **[84]** D. Sussman. *Lecture Notes for Boston University MA 882 Spring 2017*, 2017 (accessed June 3, 2020). URL http://math.bu.edu/people/sussman/MA882_2017/2017-01-26-Lecture-2.html. — Cited in 02_bigbird-architecture.md; Erdos-Renyi random graphs do not have a high clustering coefficient.

- **[85]** I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In *Advances in neural information processing systems*, pages 3104-3112, 2014. — Cited in 04_experiments-nlp.md as baseline Attn-Seq2Seq in summarization (Tab. 4).

- **[86]** A. Tampuu, Z. Bzhalava, J. Dillner, and R. Vicente. Viraminer: Deep learning on raw dna sequences for identifying viral genomes in human samples. *PloS one*, 14(9), 2019. — Cited in 05_experiments-genomics.md; deep learning for genomics data.

- **[87]** Z. Tang, Y. Shen, X. Ma, W. Xu, J. Yu, and W. Lu. Multi-hop reading comprehension across documents with path-based graph convolutional network. *arXiv:2006.06478*, 2020. — Cited in 04_experiments-nlp.md as baseline MRC-GCN in QA test results (Tab. 3).

- **[90]** R. K. Umarov and V. V. Solovyev. Recognition of prokaryotic and eukaryotic promoters using convolutional deep learning neural networks. *PloS one*, 12(2), 2017. — Cited in 05_experiments-genomics.md as baseline CNNProm in promoter region prediction (Tab. 6).

- **[91]** A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In *Advances in neural information processing systems*, pages 5998-6008, 2017. — Cited in 01_introduction.md, 02_bigbird-architecture.md, and 03_theoretical-results.md; the original Transformer paper. Also cited in 11_appendix-e-nlp-experiments-details.md as baseline Transformer in shorter summarization (Tab. 20).

- **[93]** Z. Wang, P. Ng, X. Ma, R. Nallapati, and B. Xiang. Multi-passage bert: A globally normalized bert model for open-domain question answering. *arXiv preprint arXiv:1908.08167*, 2019. — Cited in 01_introduction.md; Transformers used for machine reading comprehension; simplest sliding window methods.

- **[94]** D. J. Watts and S. H. Strogatz. Collective dynamics of 'small-world' networks. *nature*, 393(6684):440-442, 1998. — Cited in 02_bigbird-architecture.md; small-world graph model balancing shortest path and locality.

- **[95]** J. Welbl, P. Stenetorp, and S. Riedel. Constructing datasets for multi-hop reading comprehension across documents. *Transactions of the Association for Computational Linguistics*, 6:287-302, 2018. — Cited in 04_experiments-nlp.md and 11_appendix-e-nlp-experiments-details.md; WikiHop dataset (Tab. 11).

- **[96]** R. Williams. A new algorithm for optimal 2-constraint satisfaction and its implications. *Theoretical Computer Science*, 348(2-3):357-365, 2005. — Cited in 03_theoretical-results.md and 09_appendix-c-limitations.md for the Orthogonal Vector Conjecture (OVC) hardness results.

- **[97]** S. Wiseman, S. M. Shieber, and A. M. Rush. Challenges in data-to-document generation. *arXiv preprint arXiv:1707.08052*, 2017. — Cited in 04_experiments-nlp.md as baseline LSA in summarization (Tab. 4).

- **[98]** X. Xiao, Z.-C. Xu, W.-R. Qiu, P. Wang, H.-T. Ge, and K.-C. Chou. ipsw (2l)-pseknc: A two-layer predictor for identifying promoters and their strength by hybrid features via pseudo k-tuple nucleotide composition. *Genomics*, 111(6):1785-1793, 2019. — Cited in 05_experiments-genomics.md; promoter region identification methods.

- **[99]** Y. Yang, R. Zhang, S. Singh, and J. Ma. Exploiting sequence-based features for predicting enhancer-promoter interactions. *Bioinformatics*, 33(14):i252-i260, 2017. — Cited in 05_experiments-genomics.md; promoter region identification methods.

- **[100]** Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov, and C. D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. *arXiv preprint arXiv:1809.09600*, 2018. — Cited in 04_experiments-nlp.md and 11_appendix-e-nlp-experiments-details.md; HotpotQA-distractor dataset (Tab. 11).

- **[101]** Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le. Xlnet: Generalized autoregressive pretraining for language understanding. In *Advances in neural information processing systems*, pages 5754-5764, 2019. — Cited in 01_introduction.md; pretraining improvement on tasks with sufficient data. Also cited in 11_appendix-e-nlp-experiments-details.md for GLUE comparison (Tab. 16).

- **[102]** Z. Yao, S. Cao, W. Xiao, C. Zhang, and L. Nie. Balanced sparsity for efficient dnn inference on gpu. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 33, pages 5676-5683, 2019. — Cited in 10_appendix-d-implementation-details.md; sparse multiplications cannot be efficiently implemented in GPUs.

- **[103]** Z. Ye, Q. Guo, Q. Gan, X. Qiu, and Z. Zhang. Bp-transformer: Modelling long-range context via binary partitioning. *arXiv preprint arXiv:1911.04070*, 2019. — Cited in 01_introduction.md; proposed binary partitions of the data.

- **[104]** C. Yun, S. Bhojanapalli, A. S. Rawat, S. J. Reddi, and S. Kumar. Are transformers universal approximators of sequence-to-sequence functions? *arXiv preprint arXiv:1912.10077*, 2019. — Cited in 01_introduction.md, 03_theoretical-results.md, and 07_appendix-a-universal-approximators.md; showed transformers capture all continuous seq-to-seq functions. Key reference for universal approximation proof. Lemmas 1, 4, 5 and Definition 2 in App. A restate results from this paper.

- **[105]** C. Yun, Y.-W. Chang, S. Bhojanapalli, A. S. Rawat, S. J. Reddi, and S. Kumar. o(n) connections are expressive enough: Universal approximability of sparse transformers. In *Advances in Neural Information Processing Systems*, 2020. — Cited in 03_theoretical-results.md and 07_appendix-a-universal-approximators.md; also explored universal approximation but did not show Turing completeness for sparse models. Noted as contemporary work in App. A.

- **[106]** H. Zhang, C.-L. Hung, M. Liu, X. Hu, and Y.-Y. Lin. Ncnet: Deep learning network models for predicting function of non-coding dna. *Frontiers in genetics*, 10, 2019. — Cited in 05_experiments-genomics.md; deep learning for genomics data.

- **[107]** J. Zhang, Y. Zhao, M. Saleh, and P. J. Liu. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. *arXiv preprint arXiv:1912.08777*, 2019. — Cited in 04_experiments-nlp.md; state-of-the-art summarization specific pretraining (Pegasus). Baseline in summarization (Tab. 4). Also cited in 11_appendix-e-nlp-experiments-details.md as baseline in shorter summarization (Tab. 20) and for warm starting large model in E.5.

- **[109]** J. Zhou and O. G. Troyanskaya. Predicting effects of noncoding variants with deep learning-based sequence model. *Nature methods*, 12(10):931-934, 2015. — Cited in 05_experiments-genomics.md; predicting functional effects of non-coding variants (DeepSea), compiled 919 chromatin-profiles. Baseline in Tab. 7. Also cited in 12_appendix-f-genomics-experiments-details.md for chromatin profile dataset and training/testing splits in F.3.

- **[3]** J. Abreu, L. Fred, D. Macedo, and C. Zanchettin. Hierarchical attentional hybrid neural networks for document classification. In *International Conference on Artificial Neural Networks*, pages 396-402. Springer, 2019. — Cited in 11_appendix-e-nlp-experiments-details.md as SoTA baseline for Yelp-5 classification (Tab. 15, score 73.28).

- **[5]** C. Alberti, K. Lee, and M. Collins. A bert baseline for the natural questions. *arXiv preprint arXiv:1901.08634*, 2019. — Cited in 11_appendix-e-nlp-experiments-details.md; Natural Questions setup largely follows [5].

- **[18]** C. Clark and M. Gardner. Simple and effective multi-paragraph reading comprehension. *arXiv preprint arXiv:1710.10723*, 2017. — Cited in 11_appendix-e-nlp-experiments-details.md; TriviaQA uses noisy spans loss following Clark and Gardner [18] (Tab. 12).

- **[35]** J. He, L. Wang, L. Liu, J. Feng, and H. Wu. Long document classification from local word glimpses via recurrent attention learning. *IEEE Access*, 7:40707-40718, 2019. — Cited in 11_appendix-e-nlp-experiments-details.md; Arxiv topic assignment dataset (Tab. 15).

- **[40]** Y. Jiang, J. Petrak, X. Song, K. Bontcheva, and D. Maynard. Team bertha von suttner at semeval-2019 task 4: Hyperpartisan news detection using elmo sentence representation convolutional network. In *Proceedings of the 13th International Workshop on Semantic Evaluation*, pages 840-844, 2019. — Cited in 11_appendix-e-nlp-experiments-details.md as SoTA baseline for Hyperpartisan classification (Tab. 15, score 90.6).

- **[47]** J. Kiesel, M. Mestre, R. Shukla, E. Vincent, P. Adineh, D. Corney, B. Stein, and M. Potthast. Semeval-2019 task 4: Hyperpartisan news detection. In *Proceedings of the 13th International Workshop on Semantic Evaluation*, pages 829-839, 2019. — Cited in 11_appendix-e-nlp-experiments-details.md; Hyperpartisan classification dataset (Tab. 15).

- **[53]** J.-S. Lee and J. Hsiang. Patent classification by fine-tuning bert language model. *World Patent Information*, 61:101965, 2020. — Cited in 11_appendix-e-nlp-experiments-details.md; Patents classification dataset (Tab. 15).

- **[64]** A. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning word vectors for sentiment analysis. In *Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies*, pages 142-150, 2011. — Cited in 11_appendix-e-nlp-experiments-details.md; IMDb classification dataset (Tab. 15).

- **[69]** M. L. Olson, L. Zhang, and C.-N. Yu. Adapting pretrained language models for long document classification. *OpenReview*, 2019. — Cited in 11_appendix-e-nlp-experiments-details.md as SoTA baseline for Arxiv and Patents classification (Tab. 15).

- **[70]** A. v. d. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding. *arXiv preprint arXiv:1807.03748*, 2018. — Cited in 11_appendix-e-nlp-experiments-details.md; variant of contrastive predictive coding used as dual encoder model for QA regularization.

- **[79]** P. Shaw, J. Uszkoreit, and A. Vaswani. Self-attention with relative position representations. *arXiv preprint arXiv:1803.02155*, 2018. — Cited in 11_appendix-e-nlp-experiments-details.md; relative position encoding used for ETC.

- **[88]** T. Thongtan and T. Phienthrakul. Sentiment classification using document embeddings trained with cosine similarity. In *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop*, pages 407-414, 2019. — Cited in 11_appendix-e-nlp-experiments-details.md as SoTA baseline for IMDb classification (Tab. 15, score 97.4).

- **[89]** T. H. Trinh and Q. V. Le. A simple method for commonsense reasoning. *arXiv preprint arXiv:1806.02847*, 2018. — Cited in 11_appendix-e-nlp-experiments-details.md; Stories pretraining dataset (Tab. 9).

- **[108]** X. Zhang, J. Zhao, and Y. LeCun. Character-level convolutional networks for text classification. In *Advances in neural information processing systems*, pages 649-657, 2015. — Cited in 11_appendix-e-nlp-experiments-details.md; Yelp-5 classification dataset (Tab. 15).

- **[23]** L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and H.-W. Hon. Unified language model pre-training for natural language understanding and generation. In *Advances in Neural Information Processing Systems*, pages 13042-13054, 2019. — Cited in 11_appendix-e-nlp-experiments-details.md as baseline UniLM in shorter summarization (Tab. 20).

- **[28]** J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin. Convolutional sequence to sequence learning. In *Proceedings of the 34th International Conference on Machine Learning-Volume 70*, pages 1243-1252. JMLR.org, 2017. — Cited in 11_appendix-e-nlp-experiments-details.md as baseline ConvS2S in shorter summarization (Tab. 20).

- **[29]** S. Gehrmann, Y. Deng, and A. M. Rush. Bottom-up abstractive summarization. *arXiv preprint arXiv:1808.10792*, 2018. — Cited in 11_appendix-e-nlp-experiments-details.md as baseline Bottom-Up in shorter summarization (Tab. 20).

- **[36]** K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom. Teaching machines to read and comprehend. In *Advances in neural information processing systems*, pages 1693-1701, 2015. — Cited in 11_appendix-e-nlp-experiments-details.md; CNN/DailyMail dataset (Tab. 19).

- **[44]** W. J. Kent, C. W. Sugnet, T. S. Furey, K. M. Roskin, T. H. Pringle, A. M. Zahler, and D. Haussler. The human genome browser at ucsc. *Genome research*, 12(6):996-1006, 2002. — Cited in 12_appendix-f-genomics-experiments-details.md; LiftOver used to convert GRCh38 to GRCh37 coordinates.

- **[45]** U. Khandelwal, K. Clark, D. Jurafsky, and L. Kaiser. Sample efficient text summarization using a single pre-trained transformer. *arXiv preprint arXiv:1905.08836*, 2019. — Cited in 11_appendix-e-nlp-experiments-details.md as baseline TransLM in shorter summarization (Tab. 20).

- **[48]** B. Kim, H. Kim, and G. Kim. Abstractive summarization of reddit posts with multi-level memory networks. *arXiv preprint arXiv:1811.00783*, 2018. — Cited in 11_appendix-e-nlp-experiments-details.md as baseline MMN in shorter summarization (Tab. 20).

- **[56]** M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. *arXiv preprint arXiv:1910.13461*, 2019. — Cited in 11_appendix-e-nlp-experiments-details.md as baseline BART in shorter summarization (Tab. 20).

- **[62]** Y. Liu and M. Lapata. Text summarization with pretrained encoders. *arXiv preprint arXiv:1908.08345*, 2019. — Cited in 11_appendix-e-nlp-experiments-details.md as baseline Extr-Abst-BERT in shorter summarization (Tab. 20).

- **[67]** S. Narayan, S. B. Cohen, and M. Lapata. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. *arXiv preprint arXiv:1808.08745*, 2018. — Cited in 11_appendix-e-nlp-experiments-details.md; BBC XSum dataset (Tab. 19).

- **[92]** A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. *arXiv preprint arXiv:1804.07461*, 2018. — Cited in 11_appendix-e-nlp-experiments-details.md; GLUE benchmark (Tab. 16).

- **[110]** Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In *IEEE international conference on computer vision*, pages 19-27, 2015. — Cited in 11_appendix-e-nlp-experiments-details.md; Books pretraining dataset (Tab. 9).
