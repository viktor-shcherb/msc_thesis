# 6 Conclusion [p. 10]

[p. 10] The authors propose BIGBIRD: a sparse attention mechanism that is linear in the number of tokens. BIGBIRD satisfies a number of theoretical results: it is a universal approximator of sequence to sequence functions and is also Turing complete. Theoretically, they use the power of extra global tokens to preserve the expressive powers of the model. They complement these results by showing that moving to sparse attention mechanism does incur a cost. Empirically, BIGBIRD gives *state-of-the-art* performance on a number of NLP tasks such as question answering and long document classification. They further introduce attention based contextual language model for DNA and fine-tune it for downstream tasks such as promoter region prediction and predicting effects of non-coding variants. [p. 10]
