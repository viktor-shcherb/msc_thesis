# Big Bird: Transformers for Longer Sequences

**Authors:** Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed
**Affiliation:** Google Research

## Publication Status

- **arXiv preprint:** July 2020, arXiv:2007.14062 (v2: January 2021)
- **Peer-reviewed:** Yes
- **Conference:** 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada (held virtually), December 6--12, 2020, Volume 33, pp. 17283--17297
- **Status:** Published conference paper

## Preferred Citation

Cite the NeurIPS 2020 version:

> Zaheer, M., Guruganesh, G., Dubey, A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., & Ahmed, A. (2020). Big Bird: Transformers for Longer Sequences. In Advances in Neural Information Processing Systems 33 (NeurIPS 2020), pp. 17283--17297.

## Links

- arXiv: https://arxiv.org/abs/2007.14062
- NeurIPS proceedings: https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html
- Code: https://github.com/google-research/bigbird
- Blog: https://research.google/blog/constructing-transformers-for-longer-sequences-with-sparse-attention-methods/
