# Retentive Network: A Successor to Transformer for Large Language Models

**Authors:** Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei
**Affiliations:** Microsoft Research, Tsinghua University

## Publication Status

- **arXiv preprint:** July 2023, arXiv:2307.08621v4
- **Peer-reviewed:** No
- **Conference/Journal:** Submitted to ICLR 2024 (rejected) and NeurIPS 2024 (rejected).
- **Status:** Preprint

## Notes

Despite significant attention and citations, this paper was rejected at both ICLR 2024 and NeurIPS 2024. Reviewer concerns included limited scale of experiments (up to 6.7B parameters, 100B tokens) and strong claims ("successor to Transformer") not fully supported by the evidence.

## Preferred Citation

Cite as arXiv preprint:

> Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., & Wei, F. (2023). Retentive Network: A Successor to Transformer for Large Language Models. arXiv:2307.08621.

## Links

- arXiv: https://arxiv.org/abs/2307.08621
- Code: https://aka.ms/retnet (Microsoft/unilm)
