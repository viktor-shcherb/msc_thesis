# 6 Evaluations [p. 9–11]

The evaluation demonstrates that the Yi model family achieves inspiring performance on a wide range of tasks and delivers close to GPT-3.5 user preference rate. Base model performance on standard benchmarks is reported first, then chat model performance and its user preference rate. [p. 9]

## 6.1 Base Model Performance

### 6.1.1 Main Results [p. 9–10]

Results are presented for base models across standard academic benchmarks. While benchmarking open-source models, a disparity was observed between the results generated by their pipeline and those reported in public sources. Upon conducting a more in-depth investigation, this difference is mostly because different models use different prompts, post-processing strategies, and sampling techniques, which may potentially induce significant variations in the outcomes. The prompt and post-processing strategy remains consistent with the default settings of the original benchmarks [2, 4, 7, 8, 10-12, 27, 28, 42, 50, 61-63, 72, 74, 75, 89, 90]. Greedy decoding is used without any post-processing for the generated content. For scores that were not reported publicly (or scores reported with different settings), they try to get results with their pipeline. For scores that can be found publicly, the existing numbers are directly reported. The following benchmarks are used, largely following the practice of LLaMA 2 [77]: [p. 9]

**Commonsense Reasoning:** PIQA [4], SIQA [63], HellaSwag [89], WinoGrande [62], ARC [11], OpenBookQA (OBQA) [50], and CommonsenseQA (CSQA) [75] to assess common sense reasoning. CSQA was exclusively tested using a 7-shot setup, while all other tests were conducted with a 0-shot configuration. [p. 9]

**Reading Comprehension:** The 0-shot average on SQuAD [61], QuAC [8], and BoolQ [10]. [p. 9]

**Math:** The average of the GSM8K [12] (8 shot), and MATH [28] (4 shot) benchmarks with pass@1 accuracy without any specific prompting strategy (e.g. Chain-of-Thought prompting) and other ensemble technique (e.g., majority voting). [p. 9]

**Code:** The average pass@1 scores on HumanEval [7] (Chen et al., 2021) and MBPP [2] (Austin et al., 2021). [p. 10]

**Popular Aggregated Benchmark:** The overall results for MMLU [27] (5-shot), CMMLU [42] (5-shot), Gaokao-Bench [90] (5-shot), and BigBench [72] Hard (BBH [74]) (3-shot). [p. 10]

**Table 2** (p. 8): "Overall performance on grouped academic benchmarks compared to open-source base models. **CR** stands for Commonsense Reasoning. **RC** stands for Reading Comprehension."

| Models | Size | MMLU | BBH | C-Eval | CMMLU | Gaokao | CR | RC | Code | Math |
|--------|------|------|-----|--------|-------|--------|------|------|------|------|
| GPT-4 | - | 83.0 | **86.7** | 69.9 | 71.0 | 72.3 | **89.3** | - | **65.3** | **66.1** |
| GPT-3.5 | - | 69.1 | 70.1 | 52.5 | 55.5 | 51.1 | 83.1 | - | 54.8 | 35.6 |
| Qwen | 14B | 66.7 | 53.4 | 72.1 | 71.0 | 62.5 | 74.2 | 72.5 | 40.6 | 43.1 |
| Llama2 | 34B | 62.6 | 44.1 | - | 50.1 | - | 71.1 | 68.9 | 27.8 | 24.2 |
| Llama2 | 70B | 69.7 | 64.9 | - | 53.3 | 23.3 | 72.7 | 72.3 | 38.4 | 35.2 |
| Baichuan-2 | 13B | 55.0 | 49.0 | 59.0 | 61.97 | 45.6 | 66.3 | 62.4 | 23.4 | 16.1 |
| InternLM | 20B | 62.1 | 52.5 | 58.8 | 59.0 | 45.5 | 78.3 | - | 34.8 | 30.26 |
| Skywork | 13B | 62.1 | 41.7 | 60.6 | 61.8 | 68.1 | 72.4 | 61.4 | 64.9 | 18.1 |
| Falcon | 180B | 70.4 | 54.0 | 57.8 | 58.0 | 59.0 | 74.4 | - | - | - |
| Yi | 6B | 63.2 | 42.8 | 72.0 | 75.5 | 72.2 | 72.2 | 68.7 | 21.1 | 18.6 |
| Yi | 34B | 76.3 | 54.3 | **81.4** | **83.7** | **82.8** | 80.7 | **76.5** | 32.1 | 40.8 |

By training on a significantly larger number of tokens (3.1T) compared to prior work (usually <= 2T), a substantial performance gain is observed across benchmarks, as shown in Table 2. However, it is important to note that there are still discernible disparities between Yi and existing open-source and close-source models, particularly in tasks related to mathematics and coding. As performance in these domains can be significantly improved by continual pretraining and instruction fine-tuning, the authors refrained from incorporating extensive mathematical and coding content in the pretraining corpus when making the initial design choices. They plan to release models with enhanced math and coding capabilities in the future. [p. 10]

**Table 3** (p. 10): "Comparison of models on GSM8k, MATH, Human-Eval, and MBPP."

| Model | Size | GSM8k | MATH | Human-Eval pass@1 | MBPP pass@1 |
|-------|------|-------|------|--------------------|-------------|
| GPT-3.5 | - | 57.1 | 14.0 | 48.1 | 61.4 |
| GPT-4 | - | **92.0** | **40.2** | **67.0** | **63.6** |
| Falcon | 180B | 54.4 | - | 0.61 | 47.0 |
| Qwen | 7B | 51.7 | 11.6 | 29.9 | 34.0 |
| Qwen | 14B | 61.3 | 24.8 | 32.3 | 48.9 |
| Baichuan 2 | 7B | 24.5 | 5.6 | 18.3 | 28.3 |
| Baichuan 2 | 13B | 22.1 | 10.1 | 20.7 | 26.1 |
| LLaMA 2 | 7B | 16.7 | 3.3 | 12.8 | 14.8 |
| LLaMA 2 | 34B | 42.2 | 6.2 | 22.6 | 33.0 |
| LLaMA 2 | 70B | 56.8 | 13.5 | 31.7 | 45.0 |
| Mistral | 7B | 47.5 | 11.3 | 30.5 | 47.5 |
| InternLM | 20B | 62.9 | 10.9 | 28.1 | 41.4 |
| Skywork | 7B | 55.8 | 7.8 | 13.4 | 22.8 |
| Yi | 6B | 32.5 | 4.6 | 15.9 | 26.3 |
| Yi | 34B | 67.2 | 14.4 | 23.2 | 41.0 |

### 6.1.2 Discussions [p. 10]

**Gain from Model Scale.** Yi-34B has substantial performance improvement compared to Yi-6B, though they utilized the same pretrain corpora. Larger model size leads to higher performance gain on Code and Math benchmarks, referring to Tab. 3, compared to benchmarks focusing on Commonsense Reasoning, Reading Comprehension, or Knowledge.

**Data Quality.** Smaller models of higher quality pretrain data, like Yi-34B or Qwen-14B, usually demonstrate better performance than models of larger size but (presumably) lower quality data, such as Falcon-180B (though the focus of Falcon-180B might be more on the scaling side, which is definitely of important value on its own).

**Gap between GPT-4 and Open-source LLMs.** Based on Tab. 2, open-source LLMs still lag behind the performance of GPT-4 and GPT-3.5 on various benchmarks. Yet representative bilingual LLMs, e.g. Qwen-14B and Yi-34B, can match or even surpass the performance of GPT-4 on Chinese knowledge related benchmarks, including C-Eval [31], CMMLU [42], and Gaokao [90]. However, there is still a huge gap between GPT-4 and open-source models on reasoning-related benchmarks like BBH [72], code (HumanEval), and math (MATH). [p. 10]

### 6.1.3 In-Context Learning Study [p. 11]

The in-context learning capability is further investigated, i.e., the capability of inferring the underlying function given the few-shot input-output demonstrations. The task considered is inferring the linear coefficient of a weighted sum. Specifically, define y = w_1 x_1 + w_2 x_2 + ... + w_n x_n, where the few-shot demonstration is x_1, x_2, ..., x_n, y, and the model is asked to (implicitly) infer w_1, w_2, ..., w_n by predicting the y given a new set of input x. Two measures are used: [p. 11]

- (a) the absolute difference between model prediction y and the ground truth y*, i.e., |y - y*| as a continuous measure
- (b) the exact match y == y* as a discontinuous measure

The authors further note that most of the models perform reasonably well on addition and subtraction, so the ability to do arithmetic, as a confounding factor, can be ruled out. [p. 11]

The results are shown in Figure 3. When setting the linear coefficients to [1, -1], Yi-34B and LLaMA-2 70B perform the best in terms of answer exact match. If the number of linear coefficients is increased to [1, 1, 1, 1, 1], the emergent behavior is observed that only large models (LLaMA-2 70B and Mixtral) can achieve good scores on exact match, although the differences to target is more continuous. These observations give side evidence for Yi-34B's performance on in-context learning and indicate that further scaling may allow the model to infer more complicated functions by in-context learning. [p. 11]

**Figure 3** (p. 11): "Evaluating language model's in-context learning capability by inferring the linear coefficients of a weighted sum. Considering the discussions of whether emergent ability is an artifact of measurement [65], we use difference to the target (target number - model prediction) as a continuous measure, and exact match (target number == model prediction) as a discontinuous measure. A: when there is two linear coefficients, Yi-34B performs the best when measuring by the difference to the target number. B: increasing the number of linear coefficients to 5, only models that are large enough (LLaMA2 70B and Mixtral 8x7B) can achieve meaningful exact match, showing that in-context learning complex functions is an emergent ability."

The figure has four subplots arranged in a 2x2 grid:
- **A1** (top-left): "In-context inferring the linear coefficient [1, -1]" -- Difference to target (y-axis, 0-50) vs. #Shots (x-axis, 10-40). Shows decreasing difference as shots increase. Models compared: Mistral-7B-v0.1, Yi-34B, Llama-2-70b-hf, Mixtral-8x7B-v0.1, Yi-6B, Llama-2-7b-hf, Llama-2-13b-hf. Yi-34B shows among the lowest difference to target.
- **A2** (top-right): Exact match (y-axis, 0-0.5) vs. #Shots (x-axis, 10-40). Yi-34B and Llama-2-70b-hf achieve highest exact match rates (around 0.4-0.5 at 40 shots).
- **B1** (bottom-left): "In-context inferring the linear coefficient [1, 1, 1, 1, 1]" -- Difference to target (y-axis, 0-160) vs. #Shots (x-axis, 10-40). Higher differences overall compared to A1; decreasing trend for larger models.
- **B2** (bottom-right): Exact match (y-axis, 0-0.8) vs. #Shots (x-axis, 10-40). Only LLaMA-2-70b-hf and Mixtral-8x7B-v0.1 achieve meaningful exact match (above 0.5 at higher shot counts), demonstrating emergence. Smaller models remain near zero.

---
[p. 12–13 continued]

## 6.2 Chat Model Performance

[p. 12] The automatic and human preference evaluation of the Chat Model is reported. Greedy decoding is used to generate responses. For automatic evaluation benchmarks, answers are extracted from the model's generated outputs and accuracy is calculated. During the evaluation process, the authors observed that different prompts have varying influence on results. Therefore, for the same set of questions, identical prompts are used to evaluate all models, aiming to ensure as fair and unbiased results as possible. [p. 12]

### 6.2.1 Automatic Evaluations [p. 12]

For automatic evaluation, the same benchmarks are used as for the base model (detailed in Sec. 6.1.1). Both zero-shot and few-shot methods are used but generally zero-shot is more suitable for chat models. The evaluation involves generating responses while following instructions explicitly or implicitly (such as the format in the few-shot examples). Relevant answers are then isolated from the generated text. Unlike the base model, for the zero-shot evaluations on the GSM8K and BBH datasets, the Chain-of-Thought (CoT) approach is employed to guide the model in deliberation before reaching an answer. [p. 12]

The results in Tab. 4 demonstrate the effectiveness of the chat models in understanding human instructions and generating appropriate instruction-following responses. The 4-bit quantization results are particularly highlighted, as 4-bit quantization substantially reduces the memory requirement while the model performance nearly does not drop. This observation serves as the foundation of serving the model on consumer-grade devices. [p. 12]

In line with Goodhart's principle, when a measurement metric becomes the target of pursuit, it ceases to serve as a reliable standard of assessment. Consequently, the outcomes of evaluations on benchmarks are exclusively employed for ensuring that alignment training does not detrimentally impact the foundational knowledge and capabilities of the base model. The authors do not engage in targeted optimization of the chat model with the objective of enhancing benchmark performance. [p. 12]

To further evaluate the generalizability of the model's capabilities, assessments of its mathematical computation proficiency were conducted by subjecting it to the 2023 Hungarian high school mathematics final exam questions, first proposed by the xAI Grok team then reproduced by Paster [55]. This evaluation was undertaken with the aim of determining whether the model exhibited signs of overfitting to training datasets that are mathematically oriented. The results in Fig. 4 show that Yi-34B-Chat performs inspiringly on both the GSM8K and the Hungarian mathematics exam. However, Yi-6B-Chat does not exhibit strong mathematical capabilities (on both GSM8K and the Hungarian mathematics exam). The authors speculate that smaller models may require more data to activate their corresponding abilities during the SFT stage. [p. 12]

**Table 4** (p. 13): "Overall performance on automatic benchmarks compared to open-source chat models. We highlight the 4-bit quantization results, as 4-bit quantization substantially reduces the memory requirement while the model performance nearly does not drop. This observation serve as the foundation of serving the model on consumer-grade devices, e.g., RTX4090."

| Model | Size | MMLU | CMMLU | C-Eval(val) | TruthfulQA | BBH | GSM8K |
|-------|------|------|-------|-------------|------------|-----|-------|
| | | 0-shot / 5-shot | 0-shot / 5-shot | 0-shot / 5-shot | 0-shot | 0-shot / 3-shot | 0-shot / 4-shot |
| LLaMA2-Chat | 13B | 50.9 / 47.3 | 27.5 / 35.1 | 27.9 / 35.9 | 36.8 | 32.9 / 58.2 | 36.9 / 2.7 |
| LLaMA2-Chat | 70B | 59.4 / 59.9 | 36.1 / 41.0 | 35.0 / 41.3 | 54.0 | 42.4 / 58.5 | 47.1 / 58.7 |
| Baichuan2-Chat | 13B | 55.1 / 50.1 | 58.6 / 59.5 | 56.0 / 54.8 | 49.0 | 38.8 / 47.2 | 45.7 / 23.3 |
| Qwen-Chat | 14B | 64.0 / 65.0 | 67.7 / 70.6 | 66.1 / 70.1 | 52.5 | 49.7 / 55.0 | 59.5 / 61.2 |
| InternLM-Chat | 20B | 55.6 / 57.4 | 53.6 / 53.8 | 51.2 / 53.6 | 51.8 | 42.4 / 36.7 | 15.7 / 43.4 |
| AquilaChat2 | 34B | 65.2 / 66.7 | 67.5 / 70.0 | **83.0 / 89.4** | **64.3** | 20.1 / 34.3 | 11.5 / 48.5 |
| Yi-Chat | 6B | 58.2 / 61.0 | 69.4 / 74.7 | 68.8 / 74.2 | 50.6 | 39.7 / 47.2 | 38.4 / 44.9 |
| Yi-Chat-8bits(GPTQ) | 6B | 58.3 / 61.0 | 69.2 / 74.7 | 69.2 / 73.9 | 49.9 | 40.4 / 47.3 | 39.4 / 44.9 |
| Yi-Chat-4bits(AWQ) | 6B | 56.8 / 59.9 | 67.7 / 73.3 | 67.5 / 72.3 | 50.3 | 37.7 / 43.6 | 35.7 / 38.4 |
| Yi-Chat | 34B | **67.6** / 73.5 | **79.1 / 81.3** | 77.0 / 78.5 | 62.4 | 51.4 / **71.7** | **71.7 / 76.0** |
| Yi-Chat-8bits(GPTQ) | 34B | 66.2 / **73.7** | 79.1 / 81.2 | 76.8 / 79.0 | 61.8 | **52.1** / 71.0 | 70.7 / 75.7 |
| Yi-Chat-4bits(AWQ) | 34B | 65.8 / 72.4 | 78.2 / 80.5 | 75.7 / 77.3 | 61.8 | 48.3 / 69.4 | 70.5 / 74.0 |

**Figure 4** (p. 13): "Yi's result of Hungarian mathematics exam."

Scatter plot with x-axis "Exam Score (%)" (range 10-70) and y-axis "GSM8k Score (%)" (range 10-90). Models plotted include:
- GPT-4: ~(68, 92) — top-right, highest on both axes
- Claude 2: ~(55, 88)
- Yi-34B-chat: ~(58, 73) — performs well on both axes
- Grok-1: ~(63, 65)
- GPT-3.5 Turbo: ~(45, 57)
- Mixtral-8x7B: ~(35, 60)
- Yi-34B (base): ~(38, 67)
- OpenChat 3.5: ~(42, 78)
- MetaMath Mistral 7B: ~(25, 80)
- MetaMath 7B: ~(22, 70)
- Llemma 34B: ~(47, 48)
- Grok-0 (33B): ~(33, 55)
- Qwen 7B: ~(25, 53)
- MAmmoTH 7B: ~(23, 48)
- Yi-6B-chat: ~(18, 40) — highlighted in cyan
- Mistral 7B: ~(28, 40)
- Yi-6B: ~(22, 35) — highlighted in cyan
- Llemma 7B: ~(18, 35)
- Code Llama 34B: ~(23, 28)
- Code Llama 7B: ~(13, 10)

The figure shows that Yi-34B-Chat performs inspiringly on both the GSM8K and the Hungarian math exam, while Yi-6B-Chat does not exhibit strong mathematical capabilities.

### 6.2.2 Human Evaluations [p. 12–13]

[p. 12] An assessment of the model's conversational abilities was conducted, considering aspects to ensure its effectiveness and safety. A collection of open-source evaluation datasets from the community was compiled, such as alpaca-eval [21], Belle-eval [88], and MT-bench [93]. Additionally, the authors established their own helpful and harmless evaluation dataset by gathering and constructing data of varying difficulty levels, for the purpose of comprehensively assessing the conversational abilities of chat models. [p. 12]

However, whether it is a public evaluation set or a self-built evaluation set, the evaluation results are strongly influenced by the assessment criteria and the design of the prompt. Internal evaluation results may be unfair to other models, making it difficult to accurately represent the true capability level. Therefore, only external evaluation results are presented to demonstrate the current conversational abilities of the chat model. The following evaluations are considered: [p. 12]

1. **AlpacaEval** [44]: designed to assess the English conversation capabilities of models by comparing the responses of a specified model to reference replies from Davinci003 [21] in order to calculate a win-rate.
2. **LMSys Chatbot Arena** [93]: showcases the responses of different models through a dialogue platform, then asks users to make selections based on their preferences, then computes the Elo score.
3. **SuperClue**: a leaderboard aimed at comprehensively evaluating the Chinese language capabilities of models.

Footnotes: AlpacaEval URL: https://tatsu-lab.github.io/alpaca_eval/; LMSys Chatbot Arena URL: https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard; SuperClue URL: https://www.superclueai.com/ [p. 12–13]

**Table 5** (p. 14): "Human evaluation comparison with other open-source chat models."

| Model | Size | AlpacaEval | LMSys Chatbot Arena | SuperClue |
|-------|------|------------|---------------------|-----------|
| GPT-4-Turbo | - | **97.7** | **1243** | **89.79** |
| GPT-3.5-Turbo | - | 89.37 | 1117 | 59.39 |
| LLaMA2-Chat | 70B | 92.66 | 1077 | - |
| Yi-Chat | 34B | 94.08 | 1110 | 71.87 |

[p. 13] Tab. 5 presents the performance results of Yi-34B-Chat in the three third-party evaluations, with the cutoff date for the results being December 21, 2023. The data demonstrates that, although there is still a gap compared to GPT-4, the model exhibits proficient bilingual (Chinese and English) dialogue capabilities and aligns well with user preferences. Additional comparative results of various models are accessible for review on the official website. [p. 13]

**Figure 5** (p. 14): "SFT data scaling curve. Compared with UltraChat and its cleaned version UltraChat 200K, our SFT data demonstrates clear scaling advantages. We attribute its steep slope to the data quality."

Two line plots side by side:
- **Left plot (Alpaca Eval 2.0):** x-axis is data size (log scale, 10^4 to 10^6), y-axis is score (roughly 3-22). Three lines for Yi Data (green), UltraChat 200K (blue), and UltraChat Original (red). Yi Data shows the steepest scaling curve, reaching ~20 at the highest data point. GPT-4 is marked at ~21.5 and GPT-3.5 at ~10.5 as horizontal reference points.
- **Right plot (MT Bench):** x-axis is data size (log scale, 10^4 to 10^6), y-axis is score (roughly 6.5-9). Three lines for Yi Data (green), UltraChat 200K (blue), and UltraChat Original (red). Yi Data again shows the steepest slope, approaching ~8 at the highest data point. GPT-4 is marked at ~9 and GPT-3.5 at ~8.5 as reference.

The authors further demonstrate the data quality by comparing the speed of preference increase during data scaling. When compared with UltraChat [19] and its cleaned version UltraChat 200K, there is a clear tendency of performance improvements when scaling up the Yi data. [p. 14]
