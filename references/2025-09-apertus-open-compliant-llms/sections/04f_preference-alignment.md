# 4.3 Preference Alignment [p. 32â€“33]

[p. 32] After SFT has encouraged the model to follow instructions, the alignment pipeline shapes the model's behavior according to helpfulness, honesty, safety, and refusal. In addition, alignment training data includes precise instruction-following, general reasoning, and question answering tasks.

## Two Major Approaches to LLM Alignment [p. 32]

[p. 32] There are two major approaches to aligning LLMs:

1. **Optimizing a reward signal** that proxies human preferences via reinforcement learning with KL regularization (*e.g.*, RLHF; Ouyang et al., 2022)
2. **Applying direct alignment algorithms (DAA)** (Rafailov et al., 2024) such as DPO (Rafailov et al., 2023), which optimize directly on human preference pairs without the need for explicit reward modeling or online RL.

The former typically relies on online RL methods like PPO (Schulman et al., 2017) or GRPO (Shao et al., 2024), which require careful hyperparameter tuning and are computationally intensive due to their online nature. Practitioners often prefer direct alignment methods, which are more stable and efficient in practice. However, these methods have limitations: they rely on relative preference signals (*i.e.*, "chosen" vs. "rejected" completions), which are less informative than absolute feedback, and they often exhibit undesirable behavior (for instance, reducing the probabilities of both completions, resulting in a shift of probability mass toward out-of-distribution samples; Pal et al., 2024). [p. 32]

## QRPO Algorithm [p. 32]

[p. 32] To address the limitations of both online RL and direct alignment methods, the recently-proposed Quantile Reward Policy Optimization algorithm (QRPO, Matrenok et al., 2025) is adopted. QRPO enables optimization of an absolute reward signal while retaining the advantages of DAA methods: training stability, offline learning capability, and significantly reduced computational demands compared to online RL.

An advantage of QRPO is that it takes as input a reward ranking over a set of reference completions. Hence, unlike traditional RL approaches, QRPO naturally supports not just reward model scores but also human preference rankings and LLM-as-a-judge preference annotations. The alignment pipeline adapts both regimes: first, using a pretrained reward model for standard preference alignment (Section 4.3.1), and second, aligning the model to constitutional values using an LLM-as-judge setup (Section 4.3.2). [p. 32]

**QRPO algorithm.** Quantile Reward Policy Optimization (QRPO) optimizes an absolute reward signal by minimizing the following loss:

$$\mathcal{L}_{QRPO} = \mathbb{E}_{x,y} \left[ \left( \mathcal{R}_q(x, y) - \beta_{KL} \log Z_q(x) - \beta_{KL} \log \frac{\pi_\theta(y \mid x)}{\pi_{ref}(y \mid x)} \right)^2 \right],$$

where $\mathcal{R}_q(x, y)$ is the quantile reward, representing the percentile rank of a candidate completion $y$ among a set of reference completions (sampled from a reference policy $\pi_{ref}$), and $Z_q(x)$ is the corresponding partition function:

$$\mathcal{R}_q(x, y) = \Pr_{y' \sim \pi_{ref}(\cdot \mid x)} \left\{ \mathcal{R}(x, y') \leq \mathcal{R}(x, y) \right\},$$

$$Z_q(x) = \beta_{KL} \left( \exp(1 / \beta_{KL}) - 1 \right).$$

[p. 33] The model is trained using a dataset $\mathcal{D} = (x_i, y_i)$ composed of both offline completions (generated by other LLMs) and off-policy completions (generated by the reference model $\pi_{ref}$). For each prompt $x_i$, a set of $n = 10$ reference completions $y_{i,j} \sim \pi_{ref}(\cdot \mid x_i)$ are generated, which are used both for training and to estimate the quantile reward. Each reference completion is annotated with a reward to construct the reference reward set:

$$\mathcal{S}_{ref,i} = \left\{ \mathcal{R}(x_i, y_{i,j}) \right\}_{j=1}^{n}.$$

The quantile reward $\mathcal{R}_q(x_i, y_i)$ is then computed as the empirical cumulative distribution function (CDF) of the reward over this reference set:

$$\mathcal{R}_q(x_i, y_i) = \frac{1}{|\mathcal{S}_{ref,i}|} \sum_{\mathcal{R}(x_i, y_{i,j}) \in \mathcal{S}_{ref,i}} \mathbf{1} \left\{ \mathcal{R}(x_i, y_{i,j}) \leq \mathcal{R}(x_i, y_i) \right\}.$$

When using LLM-as-judge preference annotations, rewards can be provided by assigning absolute scores to single completions or through pairwise ranks (see Section 4.3.2 for further details). [p. 33]

## Length-normalized QRPO [p. 33]

[p. 33] Inspired by the Tulu 3 family of models, a length-normalized variant of QRPO is adopted, in which the KL regularization coefficient $\beta_{KL}$ is normalized by the length of the completion $|y|$. The loss thus becomes:

$$\mathcal{L}_{QRPO-norm} = \mathbb{E}_{x,y} \left[ \left( \mathcal{R}_q(x, y) - \frac{\beta_{KL}}{|y|} \log Z_{q-norm}(x) - \frac{\beta_{KL}}{|y|} \log \frac{\pi_\theta(y \mid x)}{\pi_{ref}(y \mid x)} \right)^2 \right],$$

$$Z_{q-norm}(x) = \frac{\beta_{KL}}{|y|} \left( \exp\left( \frac{1}{\beta_{KL} / |y|} \right) - 1 \right).$$

Such normalization is typically motivated by the need to normalize log-probabilities with respect to sequence length. In QRPO, $\beta_{KL}$ is divided by the completion length in all components of the loss to ensure correctness and consistency of the partition function $Z_q$. [p. 33]

Experiments comparing both QRPO and DPO in their standard and length-normalized forms in ablation studies show that length normalization consistently improves downstream performance for both algorithms. QRPO and DPO achieve similar results for the 8B model, while QRPO outperforms DPO in the 70B model. Based on these findings, length-normalized QRPO is adopted as the preferred alignment method. [p. 33]

## QRPO Training Hyperparameters [p. 33]

- **$\beta_{KL}$:** 5
- **Length normalization:** applied (yielding an average value of $\beta_{KL} / |y| \approx 0.03$)
- **Optimizer:** AdEMAMix (Pagliardini et al., 2025) with $\beta_3 = 0.99$, $\alpha = 8.0$, and both $t_{\beta_3}$ and $t_\alpha$ set to the total number of training steps
- **Default values:** $\beta_1 = 0.9$ and $\beta_2 = 0.999$
- **Learning rate:** $5 \times 10^{-7}$ for the 8B model and $1 \times 10^{-7}$ for the 70B model
