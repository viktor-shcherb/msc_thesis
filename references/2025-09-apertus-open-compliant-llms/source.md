# Democratizing Open and Compliant LLMs for Global Language Environments: Apertus v1 Technical Report

**Authors:** Project Apertus

**Core Team:** Alejandro Hernandez-Cano, Alexander Hagele, Allen Hao Huang, Angelika Romanou, Antoni-Joan Solergibert, Barna Pasztor, Bettina Messmer, Dhia Garbaya, Eduard Frank Durech, Ido Hakimi, Juan Garcia Giraldo, Mete Ismayilzada, Negar Foroutan, Skander Moalla, Tiancheng Chen, Vinko Sabolcec, Yixuan Xu

**Leads:** Antoine Bosselut, Martin Jaggi, Imanol Schlag

**Affiliations:**
- 1: EPFL
- 2: ETH Zurich
- 3: CSCS (Swiss National Supercomputing Centre)
- 4: HES-SO Valais-Wallis
- 5: HSLU
- 6: IST Austria
- 7: ZHAW
- 8: University of Zurich
- 9: University of Bern
- 10: Vischer

Core Team affiliations: Alejandro Hernandez-Cano (EPFL), Alexander Hagele (EPFL), Allen Hao Huang (EPFL), Angelika Romanou (EPFL), Antoni-Joan Solergibert (EPFL / ETH Zurich), Barna Pasztor (ETH Zurich), Bettina Messmer (EPFL), Dhia Garbaya (EPFL), Eduard Frank Durech (EPFL / ETH Zurich), Ido Hakimi (ETH Zurich), Juan Garcia Giraldo (EPFL), Mete Ismayilzada (EPFL), Negar Foroutan (EPFL), Skander Moalla (EPFL), Tiancheng Chen (ETH Zurich), Vinko Sabolcec (EPFL), Yixuan Xu (EPFL / ETH Zurich)

Leads affiliations: Antoine Bosselut (EPFL), Martin Jaggi (EPFL), Imanol Schlag (ETH Zurich)

## Publication Status

- **arXiv preprint:** September 2025, arXiv:2509.14233
- **Peer-reviewed:** No
- **Status:** Preprint

## Preferred Citation

Cite the arXiv version:

> Project Apertus. (2025). Democratizing Open and Compliant LLMs for Global Language Environments. arXiv:2509.14233.

## Links

- arXiv: https://arxiv.org/abs/2509.14233
- PDF: https://arxiv.org/pdf/2509.14233
- Code (pretraining): https://github.com/swiss-ai/pretrain-code
- Code (pretraining data): https://github.com/swiss-ai/pretrain-data
- Code (post-training): https://github.com/swiss-ai/posttraining
- Code (finetuning recipes): https://github.com/swiss-ai/apertus-finetuning-recipes
- Code (memorization analysis): https://github.com/swiss-ai/apertus-memorization
- Code (tech report): https://github.com/swiss-ai/apertus-tech-report
- Models: https://huggingface.co/collections/swiss-ai/apertus-llm
- Project page: https://www.swiss-ai.org/apertus
