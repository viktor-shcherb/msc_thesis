# 5 Conclusion [p. 9]

[p. 9] The authors proposed Long Range Arena (LRA), a new benchmark for evaluating progress on efficient Transformer research. The new benchmark is challenging and probes at model capabilities in dealing with diverse data types and structures such as text, mathematics, and visual data. The benchmark comprises of tasks ranging from 1K to 16K tokens. For the first time, the authors conduct an extensive side-by-side comparison of ten recently proposed efficient Transformer models. The experimental results show that these tasks are very challenging even for long-range Transformer models. The overall results show that there is no one-size-fits-all solution and trade-offs have to be made in terms of model quality and speed/memory. The authors plan to open source their code and benchmarks to facilitate future benchmarking, research and model development.
