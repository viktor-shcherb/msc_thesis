# References

This file contains only references that are cited in the section notes. The paper uses author-year citation format.

## Abnar & Zuidema, 2020
- Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*, 2020.
- **Cited in:** 07_appendix-a-lra-tasks.md (attention-rollout visualization method for Pathfinder attention maps)

## Ainslie et al., 2020
- Joshua Ainslie, Santiago Ontanon, Chris Alberti, Philip Pham, Anirudh Ravula, and Sumit Sanghai. Etc: Encoding long and structured data in transformers. *arXiv preprint arXiv:2004.08483*, 2020.
- **Cited in:** 01_introduction.md (conflating inductive bias with pretraining benefits)

## Beltagy et al., 2020
- Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. *arXiv preprint arXiv:2004.05150*, 2020.
- **Cited in:** 01_introduction.md, 03_experimental-results.md, 04_related-work.md (Longformer model)

## Child et al., 2019
- Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. *arXiv preprint arXiv:1904.10509*, 2019.
- **Cited in:** 01_introduction.md, 02_long-range-arena.md, 03_experimental-results.md, 04_related-work.md (Sparse Transformers)

## Choromanski et al., 2020a
- Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Jared Davis, Tamas Sarlos, David Belanger, Lucy Colwell, and Adrian Weller. Masked language modeling for proteins via linearly scalable long-context transformers. *arXiv preprint arXiv:2006.03555*, 2020a.
- **Cited in:** 03_experimental-results.md, 04_related-work.md (Performers; kernel-based model)

## Choromanski et al., 2020b
- Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. *arXiv preprint arXiv:2009.14794*, 2020b.
- **Cited in:** 01_introduction.md (Performers, listed among ten evaluated models), 08_appendix-b-models-implementation.md (FAVOR+ implementation for Performers)

## Dai et al., 2019
- Zihang Dai, Zhilin Yang, Yiming Yang, William W. Cohen, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. In *Proceedings of ACL 2019*, 2019.
- **Cited in:** 04_related-work.md (Transformer-XL, sensitive to context of ~900 tokens)

## Daniluk et al., 2017
- Michal Daniluk, Tim Rockt, Johannes Welbl, and Sebastian Riedel. Frustratingly Short Attention Spans in Neural Language Modeling. In *Proceedings of ICLR 2017*, 2017.
- **Cited in:** 02_long-range-arena.md (footnote 4, required attention span), 04_related-work.md (LSTM language models rarely attend beyond seven preceding words)

## Devlin et al., 2018
- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. *arXiv preprint arXiv:1810.04805*, 2018.
- **Cited in:** 01_introduction.md (Transformers applied to language)

## Guo et al., 2016
- Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. A deep relevance matching model for ad-hoc retrieval. In *Proceedings of the 25th ACM International on Conference on Information and Knowledge Management*, pp. 55-64, 2016.
- **Cited in:** 02_long-range-arena.md (similarity score learning applications)

## Guu et al., 2020
- Kelvin Guu, Kenton Lee, Zora Tung, and Panupong Pasupat. REALM: Retrieval-Augmented Language Model Pre-Training. In *Proceedings of ICML 2020*, 2020.
- **Cited in:** 04_related-work.md (QA: pre-training and separate retrieval model approach)

## Hooker, 2020
- Sara Hooker. The hardware lottery. *arXiv preprint arXiv:2009.06489*, 2020.
- **Cited in:** 08_appendix-b-models-implementation.md (hardware lottery concept; models need hardware support to succeed)

## Ho et al., 2019
- Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidimensional transformers. *arXiv preprint arXiv:1912.12180*, 2019.
- **Cited in:** 04_related-work.md (combined attention patterns)

## Houtkamp & Roelfsema, 2010
- R. Houtkamp and P. R. Roelfsema. Parallel and serial grouping of image elements in visual perception. *J Exp Psychol Hum Percept Perform.*, 2010.
- **Cited in:** 02_long-range-arena.md (Pathfinder task motivated by cognitive psychology)

## Howard & Ruder, 2018
- Jeremy Howard and Sebastian Ruder. Universal Language Model Fine-tuning for Text Classification. In *Proceedings of ACL 2018*, 2018.
- **Cited in:** 02_long-range-arena.md (text classification applications)

## Jiang et al., 2019
- Jyun-Yu Jiang, Mingyang Zhang, Cheng Li, Michael Bendersky, Nadav Golbandi, and Marc Najork. Semantic text matching for long-form documents. In *The World Wide Web Conference*, pp. 795-806, 2019.
- **Cited in:** 02_long-range-arena.md (long-form document matching setup)

## Joshi et al., 2017
- Mandar Joshi, Eunsol Choi, Daniel S Weld, Luke Zettlemoyer, and Paul G Allen. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In *Proceedings of ACL 2017*, 2017.
- **Cited in:** 04_related-work.md (QA on long contexts, Wikipedia documents)

## Katharopoulos et al., 2020
- Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. *arXiv preprint arXiv:2006.16236*, 2020.
- **Cited in:** 01_introduction.md, 02_long-range-arena.md, 03_experimental-results.md, 04_related-work.md (Linear Transformers)

## Kawakami et al., 2019
- Kazuya Kawakami, Chris Dyer, and Phil Blunsom. Learning to discover, ground and use words with segmental neural language models. In *Proceedings of ACL 2019*, pp. 6429-6441, 2019.
- **Cited in:** 02_long-range-arena.md (character boundary learning as a challenging problem)

## Kim* et al., 2020
- Junkyung Kim*, Drew Linsley*, Kalpit Thakkar, and Thomas Serre. Disentangling neural mechanisms for perceptual grouping. In *International Conference on Learning Representations*, 2020.
- **Cited in:** 02_long-range-arena.md (Pathfinder challenge), 07_appendix-a-lra-tasks.md (Pathfinder task difficulty for CNN models)

## Kitaev et al., 2020
- Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In *International Conference on Learning Representations*, 2020.
- **Cited in:** 01_introduction.md, 02_long-range-arena.md, 03_experimental-results.md, 04_related-work.md (Reformer)

## Kocisky et al., 2018
- Tomas Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. The NarrativeQA Reading Comprehension Challenge. *Transactions of the Association for Computational Linguistics*, 2018.
- **Cited in:** 04_related-work.md (QA on books)

## Krizhevsky, 2009
- Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
- **Cited in:** 02_long-range-arena.md (CIFAR-10 dataset for image classification task)

## Kwiatkowski et al., 2019
- Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Question Answering Research. In *Transactions of the ACL*, 2019.
- **Cited in:** 04_related-work.md (QA on Wikipedia documents)

## Lee et al., 2019
- Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent Retrieval for Weakly Supervised Open Domain Question Answering. In *Proceedings of ACL 2019*, 2019.
- **Cited in:** 04_related-work.md (QA: pre-training and separate retrieval model approach)

## Linsley et al., 2018
- Drew Linsley, Junkyung Kim, Vijay Veerabadran, Charles Windolf, and Thomas Serre. Learning long-range spatial dependencies with horizontal gated recurrent units. In *Advances in neural information processing systems*, pp. 152-164, 2018.
- **Cited in:** 02_long-range-arena.md (Pathfinder challenge, first introduced for learning long-range spatial dependencies), 07_appendix-a-lra-tasks.md (Pathfinder task difficulty for CNN models)

## Liu et al., 2018
- Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. *arXiv preprint arXiv:1801.10198*, 2018.
- **Cited in:** 01_introduction.md, 04_related-work.md (efficient Transformer models, fixed local window approach)

## Lu et al., 2019
- Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In *Advances in Neural Information Processing Systems*, pp. 13-23, 2019.
- **Cited in:** 01_introduction.md (Transformers applied to images)

## Maas et al., 2011
- Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In *Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies*, pp. 142-150, Portland, Oregon, USA, June 2011.
- **Cited in:** 02_long-range-arena.md (IMDb reviews dataset for text classification), 07_appendix-a-lra-tasks.md (IMDb dataset for byte-level document classification)

## Nangia & Bowman, 2018
- Nikita Nangia and Samuel R Bowman. Listops: A diagnostic dataset for latent tree learning. *arXiv preprint arXiv:1804.06028*, 2018.
- **Cited in:** 02_long-range-arena.md (standard ListOps task, parsing ability), 07_appendix-a-lra-tasks.md (ListOps generation steps)

## Paperno et al., 2016
- Denis Paperno, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern. The LAMBADA dataset: Word prediction requiring a broad discourse context. In *Proceedings of ACL 2016*, 2016.
- **Cited in:** 04_related-work.md (footnote 5, LAMBADA dataset tests context understanding)

## Parmar et al., 2018
- Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. *arXiv preprint arXiv:1802.05751*, 2018.
- **Cited in:** 04_related-work.md (early fixed pattern / local window approach)

## Radford et al., 2019
- Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language Models are Unsupervised Multitask Learners. 2019.
- **Cited in:** 04_related-work.md (large-scale models maintain consistent theme over long sequences)

## Rae & Razavi, 2020
- Jack W Rae and Ali Razavi. Do Transformers Need Deep Long-Range Memory? In *Proceedings of ACL 2020*, pp. 7524-7529, 2020.
- **Cited in:** 02_long-range-arena.md (footnote 4, required attention span), 04_related-work.md (debated how much long-range signal language modeling encodes; improving models by limiting attention range)

## Radev et al., 2013
- Dragomir R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. The acl anthology network corpus. *Language Resources and Evaluation*, 47(4):919-944, 2013.
- **Cited in:** 02_long-range-arena.md (ACL Anthology Network dataset)

## Raffel et al., 2019
- Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. *arXiv preprint arXiv:1910.10683*, 2019.
- **Cited in:** 01_introduction.md (Transformers applied to language)

## Rives et al., 2019
- Alexander Rives, Siddharth Goyal, Joshua Meier, Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry Ma, and Rob Fergus. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. *bioRxiv*, pp. 622803, 2019.
- **Cited in:** 01_introduction.md (Transformers applied to protein sequences)

## Roy et al., 2020
- Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. *arXiv preprint arXiv:2003.05997*, 2020.
- **Cited in:** 04_related-work.md (learned attention patterns; generative modeling evaluation)

## Ruder et al., 2019
- Sebastian Ruder, Matthew E Peters, Swabha Swayamdipta, and Thomas Wolf. Transfer learning in natural language processing. In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials*, pp. 15-18, 2019.
- **Cited in:** 04_related-work.md (long-range coreference resolution, reasoning with events, discourse understanding)

## Socher et al., 2013
- Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In *Proceedings of EMNLP 2013*, pp. 1631-1642. Citeseer, 2013.
- **Cited in:** 04_related-work.md (SST dataset for NLU)

## Tan & Bansal, 2019
- Hao Tan and Mohit Bansal. LXMERT: Learning Cross-Modality Encoder Representations from Transformers. In *Proceedings of EMNLP 2019*, 2019.
- **Cited in:** 01_introduction.md (Transformers applied to images)

## Tay et al., 2020a
- Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Rethinking self-attention in transformer models. *arXiv preprint arXiv:2005.00743*, 2020a.
- **Cited in:** 01_introduction.md, 03_experimental-results.md (Synthesizers)

## Tay et al., 2020b
- Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. *arXiv preprint arXiv:2002.11296*, 2020b.
- **Cited in:** 01_introduction.md, 02_long-range-arena.md, 03_experimental-results.md (Sinkhorn Transformers; also cited in footnote 3 regarding IMDb char-level performance)

## Tay et al., 2020c
- Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. *arXiv preprint arXiv:2009.06732*, 2020c.
- **Cited in:** 01_introduction.md, 04_related-work.md (survey/overview of efficient Transformer research area)

## Vaswani et al., 2017
- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In *Advances in neural information processing systems*, pp. 5998-6008, 2017.
- **Cited in:** 01_introduction.md, 03_experimental-results.md (original Transformer model)

## Wang et al., 2020
- Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. *arXiv preprint arXiv:2006.04768*, 2020.
- **Cited in:** 01_introduction.md, 02_long-range-arena.md, 03_experimental-results.md, 04_related-work.md (Linformer; also cited for conflating inductive bias with pretraining, autoregressive decoding limitation, and NLU evaluation)

## Welbl et al., 2018
- Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. Constructing Datasets for Multi-hop Reading Comprehension Across Documents. In *Transactions of the Association for Computational Linguistics*, 2018.
- **Cited in:** 04_related-work.md (datasets requiring multiple hops of reasoning)

## Williams et al., 2018
- Adina Williams, Nikita Nangia, and Samuel R. Bowman. A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference. In *Proceedings of NAACL-HLT 2018*, 2018. URL http://arxiv.org/abs/1704.05426.
- **Cited in:** 04_related-work.md (MultiNLI dataset for NLU)

## Yang et al., 2018
- Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In *Proceedings of EMNLP 2018*, 2018.
- **Cited in:** 04_related-work.md (datasets requiring multiple hops of reasoning)

## Yang et al., 2020
- Liu Yang, Mingyang Zhang, Cheng Li, Michael Bendersky, and Marc Najork. Beyond 512 tokens: Siamese multi-depth transformer-based hierarchical encoder for document matching. *CoRR*, abs/2004.12297, 2020. URL https://arxiv.org/abs/2004.12297.
- **Cited in:** 02_long-range-arena.md (long-form document matching setup)

## Zaheer et al., 2020
- Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. *arXiv preprint arXiv:2007.14062*, 2020.
- **Cited in:** 01_introduction.md, 02_long-range-arena.md, 03_experimental-results.md, 04_related-work.md (BigBird; also cited for conflating inductive bias with pretraining, and QA evaluation)
