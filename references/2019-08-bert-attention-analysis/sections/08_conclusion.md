# 8 Conclusion [p. 10]

The authors have proposed a series of analysis methods for understanding the attention mechanisms of models and applied them to BERT. While most recent work on model analysis for NLP has focused on probing vector representations or model outputs, the authors have shown that a substantial amount of linguistic knowledge can be found not only in the hidden states, but also in the attention maps. They argue that probing attention maps complements other model analysis techniques, and should be part of the toolkit used by researchers to understand what neural networks learn about language. [p. 10]

## Acknowledgements [p. 10]

The authors thank the anonymous reviews for their thoughtful comments and suggestions. Kevin is supported by a Google PhD Fellowship. [p. 10]
