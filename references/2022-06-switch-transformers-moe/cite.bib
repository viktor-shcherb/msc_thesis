@article{2022-06-switch-transformers-moe,
  author       = {William Fedus and Barret Zoph and Noam Shazeer},
  title        = {{Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity}},
  journal      = JMLR,
  year         = {2022},
  month        = apr,
  volume       = {23},
  number       = {120},
  pages        = {1--39},
  url          = {http://jmlr.org/papers/v23/21-0998.html},
  eprint       = {2101.03961},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  keywords     = {mixture-of-experts, natural language processing, sparsity, large-scale machine learning, distributed computing},
}
