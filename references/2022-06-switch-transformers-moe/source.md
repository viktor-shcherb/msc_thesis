# Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity

**Authors:** William Fedus*, Barret Zoph*, Noam Shazeer
**Affiliation:** Google, Mountain View, CA 94043, USA

*Equal contribution.

## Publication Status

- **arXiv preprint:** January 2021, arXiv:2101.03961
- **Peer-reviewed:** Yes
- **Journal:** Journal of Machine Learning Research, Volume 23, Number 120, Pages 1-40, Published April 2022
- **Status:** Published journal paper

## Preferred Citation

Cite the JMLR 2022 version:

> Fedus, W., Zoph, B., & Shazeer, N. (2022). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. Journal of Machine Learning Research, 23(120):1-39.

## Links

- arXiv: https://arxiv.org/abs/2101.03961
- JMLR: https://jmlr.org/papers/v23/21-0998.html
- Code (JAX/T5X): https://github.com/google-research/t5x
- Code (TensorFlow): https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py

## Notes

- **Slug date discrepancy:** The directory uses `2022-06` (arXiv v3 revision date of 16 Jun 2022), but the JMLR publication date is April 2022 ("Published 4/22" on the paper header). The correct slug would be `2022-04-switch-transformers-moe`.
