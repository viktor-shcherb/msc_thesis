# B. Preventing Token Dropping with *No-Token-Left-Behind* [p. 29-30]

[p. 29]

Due to software constraints on TPU accelerators, the shapes of the Tensors must be statically sized. As a result, each expert has a finite and fixed capacity to process token representations. This presents an issue for the model which dynamically routes tokens at run-time that may result in an uneven distribution over experts. If the number of tokens sent to an expert is less than the expert capacity, then the computation may simply be padded -- an inefficient use of the hardware, but mathematically correct. However, when the number of tokens sent to an expert is larger than its capacity (expert overflow), a protocol is needed to handle this. Lepikhin et al. (2020) adapts a Mixture-of-Expert model and addresses expert overflow by passing its representation to the next layer through a residual connection without processing, which the authors also follow.

The authors suspected that having no computation applied to tokens could be very wasteful, especially since if there is overflow on one expert, that means another expert will have extra capacity. With this intuition they create *No-Token-Left-Behind*, which iteratively reroutes any tokens that are at first routed to an expert that is overflowing. Figure 11 shows a graphical description of this method, which will allow guaranteeing almost no tokens will be dropped during training and inference. They hypothesised that this could improve performance and further stabilize training, but found no empirical benefits. They suspect that once the network learns associations between different tokens and experts, if this association is changed (e.g. sending a token to its second highest expert) then performance could be degraded.

[p. 30]

**Figure 11** (p. 30): "Diagram of the *No-Token-Left-Behind Routing*. Stage 1 is equivalent to Switch routing where tokens are routed to the expert with the highest probability from the router. In Stage 2 we look at all tokens that have overflowed and route them to the expert with which has the second highest probability. Tokens can still be overflowed if their second highest expert has too many tokens, but this allows most of the tokens to be routed. This process can be iterated to guarantee virtually no tokens are dropped at all."

The figure shows a two-stage routing process with 3 experts. In Stage 1, tokens are routed to the expert with the highest probability from the Router Probabilities matrix (shown as a 3x7 grid of values: row 1: 0.1, 0.7, 0.5, 0.8, 0.3, 0.7; row 2: 0.7, 0.2, 0.3, 0.1, 0.1, 0.1; row 3: 0.2, 0.1, 0.2, 0.1, 0.6, 0.2). Solid arrows show primary routing. In Stage 2, overflowed tokens are rerouted to the expert with the second highest probability (shown with dashed red arrows).
