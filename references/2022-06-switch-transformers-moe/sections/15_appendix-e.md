# E. Relation of Upstream to Downstream Model Performance [p. 32]

[p. 32]

There is no guarantee that a model's quality on a pre-training objective will translate to downstream task results. Figure 13 presents the correlation of the upstream model quality, for both dense and Switch models, on the C4 pre-training task with two downstream task measures: average SuperGLUE performance and TriviaQA score. These two tasks are chosen as one probes the model's reasoning and the other factual knowledge.

**Figure 13** (p. 32): "Upstream pre-trained quality to downstream model quality. We correlate the upstream performance with downstream quality on both SuperGLUE and TriviaQA (SOTA recorded without SSM), reasoning and knowledge-heavy benchmarks, respectively (validation sets). We find that, as with the baseline, the Switch model scales with improvements in the upstream pre-training task. For SuperGLUE, we find a loosely linear relation between negative log perplexity and the average SuperGLUE score. However, the dense model often performs better for a fixed perplexity, particularly in the large-scale regime. Conversely, on the knowledge-heavy task, TriviaQA, we find that the Switch Transformer may follow an improved scaling relationship -- for a given upstream perplexity, it does better than a dense counterpart. Further statistics (expensive to collect and left to future work) would be necessary to confirm these observations."

- Left plot: x-axis is "C4 Neg. Log Perplexity" (ranging from approximately -1.7 to -1.0), y-axis is "SuperGLUE Score" (ranging from approximately 65 to 90). Three series are plotted: SOTA (gray line), Dense (green dots), and Switch (blue dots). Both Dense and Switch models show a loosely linear relationship between upstream perplexity and downstream SuperGLUE score. The dense model often performs better for a fixed perplexity, particularly in the large-scale regime (rightmost points near -1.0 to -1.1 perplexity).
- Right plot: x-axis is "C4 Neg. Log Perplexity" (ranging from approximately -1.7 to -1.0), y-axis is "TriviaQA Score" (ranging from approximately 10 to 50). Three series are plotted: SOTA (gray line), Dense (green dots), and Switch (blue dots). For the knowledge-heavy TriviaQA task, Switch Transformer models appear to follow an improved scaling relationship -- for a given upstream perplexity, the Switch model does better than a dense counterpart.

The authors find a consistent correlation, indicating that for both baseline and Switch models, improved pre-training leads to better downstream results. Additionally, for a fixed upstream perplexity, both Switch and dense models perform similarly in the small to medium model size regime. However, in the largest model regime (T5-11B/T5-XXL) the largest Switch models, as mentioned in Section 5.6, do not always translate their upstream perplexity well to downstream fine-tuning on the SuperGLUE task. This warrants future investigation and study to fully realize the potential of sparse models. Understanding the fine-tuning dynamics with expert-models is described as very complicated and dependent on regularization, load-balancing, and fine-tuning hyper-parameters.
