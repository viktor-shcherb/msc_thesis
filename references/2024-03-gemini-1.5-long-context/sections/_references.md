# References

Only references actually cited in the section notes are included below. Full bibliographic details are sourced from pages 75-91 of the PDF.

---

## A

### Agarwal et al., 2024a
Rishabh Agarwal, Avi Singh, Lei M. Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand, Zaheer Abbas, Azade Nova, John D. Co-Reyes, Eric Chu, Feryal Behbahani, Aleksandra Faust, and Hugo Larochelle. Many-shot in-context learning. *CoRR*, abs/2404.11018, 2024a.
- Cited in 05c_realistic-long-context-evaluations.md (MTOB ASR many-shot learning reference), 05d_realistic-long-context-continued.md (many-shot ICL regime, in-context planning shots approach)

### Agarwal et al., 2024b
Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos Garea, Matthieu Geist, and Olivier Bachem. On-policy distillation of language models: Learning from self-generated mistakes. In *The Twelfth International Conference on Learning Representations*, 2024b.
- Cited in 03_model-architecture.md (Flash online distillation reference), 12_appendix.md (Model Card: Flash distillation)

### Ainslie et al., 2023
Joshua Ainslie, Tao Lei, Michiel de Jong, Santiago Ontanon, Siddhartha Brahma, Yury Zemlyanskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, et al. Colt5: Faster long-range transformers with conditional computation. *arXiv preprint arXiv:2303.09752*, 2023.
- Cited in 05a_long-context-evaluations.md (novel architectural approaches to long context)

### Andriushchenko et al., 2024
Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. Jailbreaking leading safety-aligned llms with simple adaptive attacks. *arXiv preprint arXiv:2404.02151*, 2024.
- Cited in 09_safety-security-responsibility.md (blackbox jailbreak attack template)

### Anil et al., 2018
Rohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Ormandi, George E. Dahl, and Geoffrey E. Hinton. Large scale distributed neural network training through online distillation, 2018.
- Cited in 03_model-architecture.md (Flash online distillation reference), 12_appendix.md (Model Card: Flash distillation)

### Anil et al., 2023a
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. *arXiv preprint arXiv:2305.10403*, 2023a.
- Cited in 09_safety-security-responsibility.md (memorization rates reporting)

### Anil et al., 2023b
Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PaLM 2 Technical Report, 2023b.
- Cited in 03_model-architecture.md (language model research), 09_safety-security-responsibility.md (conditional pre-training control tags)

### Anthropic, 2023a
Anthropic. Model Card and Evaluations for Claude Models, 2023a.
- Cited in 01_introduction.md (context window of Claude), 03_model-architecture.md (language model research), 05a_long-context-evaluations.md (expanding context window)

### Anthropic, 2023b
Anthropic. Long context prompting for Claude 2.1, 2023b.
- Cited in 05b_diagnostic-long-context-continued.md (Claude 2.1 prompting guidelines for retrieval), 12g_blink-realworldqa-video-prompts.md (MRCR prompt workaround for Claude 2.1 refusals)

### Ardila et al., 2019
Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M Tyers, and Gregor Weber. Common voice: A massively-multilingual speech corpus. *arXiv preprint arXiv:1912.06670*, 2019.
- Cited in 09_safety-security-responsibility.md (Mozilla Common Voice dataset for gender evaluation)

### Bradbury et al., 2018
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.
- Cited in 12_appendix.md (Model Card: Hardware & Software -- training framework)

## B

### Bai et al., 2022
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: Harmlessness from AI feedback. *arXiv preprint arXiv:2212.08073*, 2022.
- Cited in 09_safety-security-responsibility.md (Constitutional AI inspiration for SFT data generation)

### Balazevic et al., 2024
Ivana Balazevic, Yuge Shi, Pinelopi Papalampidi, Rahma Chaabouni, Skanda Koppula, and Olivier J Henaff. Memory consolidation enables long-context video understanding. *arXiv preprint arXiv:2402.05861*, 2024.
- Cited in 05d_realistic-long-context-continued.md (GPT-4V EgoSchema result comparison)

### Becker and LeCun, 1989
Suzanna Becker and Yann LeCun. Improving the convergence of back-propagation learning with second-order methods. 1989.
- Cited in 03_model-architecture.md (higher-order preconditioned methods for Flash training)

### Bengio et al., 2013
Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation, 2013.
- Cited in 03_model-architecture.md (conditional computation reference)

### Bertsch et al., 2023
Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R Gormley. Unlimiformer: Long-range transformers with unlimited length input. *arXiv preprint arXiv:2305.01625*, 2023.
- Cited in 05a_long-context-evaluations.md (post-training modifications for long context)

### Bertsch et al., 2024
Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew R Gormley, and Graham Neubig. In-context learning with long-context models: An in-depth exploration. *arXiv preprint arXiv:2405.00200*, 2024.
- Cited in 05d_realistic-long-context-continued.md (many-shot ICL regime)

### Beyer et al., 2021
Lucas Beyer, Xiaohua Zhai, Amelie Royer, Larisa Markeeva, Rohan Anil, and Alexander Kolesnikov. Knowledge distillation: A good teacher is patient and consistent, 2021.
- Cited in 03_model-architecture.md (Flash online distillation reference), 12_appendix.md (Model Card: Flash distillation)

### Biderman et al., 2024
Stella Biderman, USVSN PRASHANTH, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raff. Emergent and predictable memorization in large language models. *Advances in Neural Information Processing Systems*, 36, 2024.
- Cited in 09_safety-security-responsibility.md (verbatim memorization in LLMs)

### Bird, 2020
Steven Bird. Decolonising speech and language technology. In Donia Scott, Nuria Bel, and Chengqing Zong, editors, *Proceedings of the 28th International Conference on Computational Linguistics*, pages 3504-3519, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.313.
- Cited in 05c_realistic-long-context-evaluations.md (footnote 12 on cultural sensitivity of ML for indigenous languages)

### Biten et al., 2019
Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marcal Rusinol, C.V. Jawahar, Ernest Valveny, and Dimosthenis Karatzas. Scene text visual question answering. In *2019 IEEE/CVF International Conference on Computer Vision (ICCV)*, pages 4290-4300, 2019. doi: 10.1109/ICCV.2019.00439.
- Cited in 06a_core-vision-multimodal-evaluations.md (ANLS metric for DocVQA)

### Bohnet et al., 2024
Bernd Bohnet, Kevin Swersky, Rosanne Liu, Pranjal Awasthi, Azade Nova, Javier Snaider, Hanie Sedghi, Aaron T Parisi, Michael Collins, Angeliki Lazaridou, Orhan Firat, and Noah Fiedel. Long-span question-answering: Automatic question generation and qa-system ranking via side-by-side evaluation, 2024.
- Cited in 05d_realistic-long-context-continued.md (side-by-side comparison methodology for long-doc QA)

### Bradley and Terry, 1952
Ralph A. Bradley and Milton E. Terry. The rank analysis of incomplete block designs -- I. The method of paired comparisons. *Biometrika*, 39:324-345, 1952.
- Cited in 05d_realistic-long-context-continued.md (Bradley-Terry model for ranking)

### Brants et al., 2007
Thorsten Brants, Ashok Popat, Peng Xu, Franz Och, and Jeffrey Dean. Large language models in machine translation. pages 858-867, 01 2007.
- Cited in 01_introduction.md (n-gram models constrained to 5 tokens of context)

### Brown et al., 2020
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, *Advances in Neural Information Processing Systems*, volume 33, pages 1877-1901. Curran Associates, Inc., 2020.
- Cited in 03_model-architecture.md (language model research), 05d_realistic-long-context-continued.md (few-shot ICL), 06_core-capability-evaluations.md (decontamination)

### Bucila et al., 2006
Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In *Knowledge Discovery and Data Mining*, 2006.
- Cited in 03_model-architecture.md (Flash distillation), 12_appendix.md (Model Card: Flash distillation)

### Bulatov et al., 2022
Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, *Advances in Neural Information Processing Systems*, volume 35, pages 11079-11091. Curran Associates, Inc., 2022.
- Cited in 05a_long-context-evaluations.md (memory-augmented models)

### Bulatov et al., 2023
Aydar Bulatov, Yuri Kuratov, and Mikhail S Burtsev. Scaling transformer to 1m tokens and beyond with rmt. *arXiv preprint arXiv:2304.11062*, 2023.
- Cited in 05a_long-context-evaluations.md (memory-augmented models)

### Buolamwini and Gebru, 2018
Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In *Conference on fairness, accountability and transparency*, pages 77-91. PMLR, 2018.
- Cited in 09_safety-security-responsibility.md (stereotypical voice assumptions)

## C

### Carlini et al., 2022
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. *arXiv preprint arXiv:2202.07646*, 2022.
- Cited in 06_core-capability-evaluations.md (long tail knowledge), 09_safety-security-responsibility.md (verbatim memorization)

### Carlini et al., 2024
Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramer, and Ludwig Schmidt. Are aligned neural networks adversarially aligned?, 2024.
- Cited in 09_safety-security-responsibility.md (jailbreak attacks on LLMs)

### Chao et al., 2024
Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George J. Pappas, Florian Tramer, Hamed Hassani, and Eric Wong. Jailbreakbench: An open robustness benchmark for jailbreaking large language models, 2024.
- Cited in 09_safety-security-responsibility.md (JailbreakBench)

### Chen and Goodman, 1999
Stanley F Chen and Joshua Goodman. An empirical study of smoothing techniques for language modeling. *Computer Speech & Language*, 13(4):359-394, 1999.
- Cited in 01_introduction.md (n-gram models)

### Chen et al., 2021
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. *arXiv preprint arXiv:2107.03374*, 2021.
- Cited in 06_core-capability-evaluations.md (HumanEval benchmark)

### Chen et al., 2023a
Yizheng Chen, Zhoujie Ding, Lamya Alowain, Xinyun Chen, and David Wagner. DiverseVul: A new vulnerable source code dataset for deep learning based vulnerability detection. In *International Symposium on Research in Attacks, Intrusions and Defenses*, pages 654-668, April 2023a.
- Cited in 09_safety-security-responsibility.md (vulnerability detection datasets)

### Chen et al., 2023b
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. *arXiv preprint arXiv:2309.12307*, 2023b.
- Cited in 05a_long-context-evaluations.md (post-training modifications)

### Chowdhery et al., 2023a
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. *Journal of Machine Learning Research*, 24(240):1-113, 2023a.
- Cited in 09_safety-security-responsibility.md (memorization rates)

### Chowdhery et al., 2023b
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling Language Modeling with Pathways. *Journal of Machine Learning Research*, 24(240):1-113, 2023b.
- Cited in 03_model-architecture.md (parallel attention/feedforward, language model research)

### Clark et al., 2022
Aidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, George van den Driessche, Eliza Rutherford, Tom Hennigan, Matthew Johnson, Katie Millican, Albin Cassirer, Chris Jones, Elena Buchatskaya, David Budden, Laurent Sifre, Simon Osindero, Oriol Vinyals, Jack Rae, Erich Elsen, Koray Kavukcuoglu, and Karen Simonyan. Unified scaling laws for routed language models, 2022.
- Cited in 03_model-architecture.md (MoE research at Google), 12_appendix.md (Model Card cites as "Clark et al., 2020" -- likely a typo in the paper; the bibliography lists this as 2022)

### Cobbe et al., 2021
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. *arXiv preprint arXiv:2110.14168*, 2021.
- Cited in 06_core-capability-evaluations.md (GSM8K)

### Conneau et al., 2023
Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. Fleurs: Few-shot learning evaluation of universal representations of speech. In *2022 IEEE Spoken Language Technology Workshop (SLT)*, pages 798-805. IEEE, 2023.
- Cited in 06b_core-audio-multimodal-evaluations.md (FLEURS benchmark)

## D

### Dean, 2021
Jeff Dean. Introducing Pathways: A next-generation AI architecture, 2021. URL https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/.
- Cited in 12_appendix.md (Model Card: Hardware & Software -- ML Pathways training infrastructure)

### Dasigi et al., 2021
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A dataset of information-seeking questions and answers anchored in research papers. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, *Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pages 4599-4610, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.365.
- Cited in 06_core-capability-evaluations.md (Qasper dataset for STEM QA)

### Davis and Arel, 2014
Andrew Davis and Itamar Arel. Low-rank approximations for conditional feedforward computation in deep neural networks, 2014.
- Cited in 03_model-architecture.md (conditional computation)

### Dhinakaran, 2024
Aparna Dhinakaran. 2024. URL https://twitter.com/aparnadhinak/status/1744771295940669689.
- Cited in 05a_long-context-evaluations.md (needle-in-haystack prior work)

### Ding et al., 2023
Yan Ding, Xiaohan Zhang, Chris Paxton, and Shiqi Zhang. Task and motion planning with large language models for object rearrangement. In *2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*, pages 2086-2092. IEEE, 2023.
- Cited in 05d_realistic-long-context-continued.md (LLM common sense for planning)

### Ding et al., 2024
Yangruibo Ding, Yanjun Fu, Omniyyah Ibrahim, Chawin Sitawarin, Xinyun Chen, Basel Alomair, David Wagner, Baishakhi Ray, and Yizheng Chen. Vulnerability detection with code language models: How far are we? *arXiv preprint:2403.18624*, 2024.
- Cited in 09_safety-security-responsibility.md (vulnerability detection datasets)

### Du et al., 2022
Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. GLaM: Efficient Scaling of Language Models with Mixture-of-Experts. *ICML*, 2022.
- Cited in 03_model-architecture.md (MoE research at Google)

### Dua et al., 2019
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages 2368-2378, 2019.
- Cited in 06_core-capability-evaluations.md (DROP benchmark)

### Duchi et al., 2011
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. *Journal of Machine Learning Research*, 12(61):2121-2159, 2011.
- Cited in 03_model-architecture.md (higher-order preconditioned methods)

## E

### Eloundou et al., 2023
Tyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. Gpts are gpts: An early look at the labor market impact potential of large language models. *arXiv preprint arXiv:2303.10130*, 2023.
- Cited in 06_core-capability-evaluations.md (productivity/economic impact studies)

## F

### Fang et al., 2024
Meng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, and Kai Zou. Mathodyssey: Benchmarking mathematical problem-solving skills in large language models using odyssey math data. *arXiv preprint arXiv:2406.18321*, 2024.
- Cited in 07_advancing-mathematical-reasoning.md (MathOdyssey benchmark)

### Fernando et al., 2023
Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktaschel. Promptbreeder: Self-referential self-improvement via prompt evolution, 2023.
- Cited in 12_appendix.md (automatically optimized prompt alternative for long-context math prompting, Section 12.6.4)

### Fedus et al., 2021
William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. *arXiv preprint arXiv:2101.03961*, 2021.
- Cited in 03_model-architecture.md (MoE research at Google), 12_appendix.md (Model Card: model architecture)

### Felten et al., 2018
Edward W Felten, Manav Raj, and Robert Seamans. A method to link advances in artificial intelligence to occupational abilities. In *AEA Papers and Proceedings*, volume 108, pages 54-57. American Economic Association, 2018.
- Cited in 06_core-capability-evaluations.md (productivity studies)

### Fu et al., 2024
Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. *arXiv preprint arXiv:2404.12390*, 2024.
- Cited in 06a_core-vision-multimodal-evaluations.md (BLINK benchmark)

## G

### Garcia et al., 2023
Xavier Garcia, Yamini Bansal, Colin Cherry, George Foster, Maxim Krikun, Fangxiaoyu Feng, Melvin Johnson, and Orhan Firat. The unreasonable effectiveness of few-shot learning for machine translation, 2023.
- Cited in 05d_realistic-long-context-continued.md (few-shot MT prior work)

### Gehman et al., 2020
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In *Findings of the Association for Computational Linguistics: EMNLP 2020*, pages 3356-3369, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.301.
- Cited in 09_safety-security-responsibility.md (Real Toxicity Prompts)

### Gemini-Team et al., 2023
Gemini-Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. *arXiv preprint arXiv:2312.11805*, 2023.
- Cited in 01_introduction.md, 03_model-architecture.md, 04_training-infrastructure-dataset.md, 05_evaluation-results.md, 06_core-capability-evaluations.md, 09_safety-security-responsibility.md, 10_discussion.md (Gemini 1.0 Technical Report -- the most frequently cited reference), 12_appendix.md (Model Card: Application, Hardware & Software)

### Gemma-Team et al., 2024
Gemma-Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Riviere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. *arXiv preprint arXiv:2403.08295*, 2024.
- Cited in 09_safety-security-responsibility.md (memorization methodology)

### Google, 2023
Google. Google's AI Principles. 2023. URL https://ai.google/responsibility/principles/.
- Cited in 09_safety-security-responsibility.md (Google AI Principles)

### Goyal et al., 2017
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 6904-6913, 2017.
- Cited in 06a_core-vision-multimodal-evaluations.md (VQAv2)

### Gu and Dao, 2023
Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. *arXiv preprint arXiv:2312.00752*, 2023.
- Cited in 05a_long-context-evaluations.md (novel architectures)

### Guan et al., 2024
Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao Kambhampati. Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. *Advances in Neural Information Processing Systems*, 36, 2024.
- Cited in 05d_realistic-long-context-continued.md (LLM plan generation)

### Guo et al., 2021
Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. Longt5: Efficient text-to-text transformer for long sequences. *arXiv preprint arXiv:2112.07916*, 2021.
- Cited in 05a_long-context-evaluations.md (novel architectures)

### Guu et al., 2020
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In Hal Daume III and Aarti Singh, editors, *Proceedings of the 37th International Conference on Machine Learning*, volume 119 of *Proceedings of Machine Learning Research*, pages 3929-3938. PMLR, 13-18 Jul 2020.
- Cited in 05a_long-context-evaluations.md (retrieval-augmented models)

## H

### Hao et al., 2023
Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. *arXiv preprint arXiv:2305.14992*, 2023.
- Cited in 05d_realistic-long-context-continued.md (LLM plan generation)

### Heek et al., 2023
Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2023. URL http://github.com/google/flax.
- Cited in 03_model-architecture.md (Flax codebase)

### Hendrycks et al., 2021a
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. *Proceedings of the International Conference on Learning Representations (ICLR)*, 2021a.
- Cited in 06_core-capability-evaluations.md (MMLU), 08_flash-8b.md

### Hendrycks et al., 2021b
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. *arXiv preprint arXiv:2103.03874*, 2021b.
- Cited in 02_improved-gemini-1.5-pro.md (MATH benchmark), 06_core-capability-evaluations.md, 07_advancing-mathematical-reasoning.md, 08_flash-8b.md

### Heskes, 2000
Tom Heskes. On "Natural" Learning and Pruning in Multilayered Perceptrons. *Neural Computation*, 12(4):881-901, 04 2000. ISSN 0899-7667. doi: 10.1162/089976600300015637.
- Cited in 03_model-architecture.md (higher-order preconditioned methods)

### Hinton et al., 2015
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015.
- Cited in 03_model-architecture.md (Flash distillation), 12_appendix.md (Model Card: Flash distillation)

### Hoffmann et al., 2022
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. *arXiv preprint arXiv:2203.15556*, 2022.
- Cited in 03_model-architecture.md (language model research)

### Huang et al., 2022
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. *arXiv preprint arXiv:2207.05608*, 2022.
- Cited in 05d_realistic-long-context-continued.md (LLM common sense)

## I

### Ippolito et al., 2022
Daphne Ippolito, Florian Tramer, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher A Choquette-Choo, and Nicholas Carlini. Preventing verbatim memorization in language models gives a false sense of privacy. *arXiv preprint arXiv:2210.17546*, 2022.
- Cited in 09_safety-security-responsibility.md (approximate memorization)

### Izacard et al., 2022
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with retrieval augmented language models. *arXiv preprint arXiv:2208.03299*, 2022.
- Cited in 05a_long-context-evaluations.md (retrieval-augmented models)

## J

### Jacobs et al., 1991
Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. *Neural computation*, 3(1):79-87, 1991.
- Cited in 03_model-architecture.md (conditional computation)

### Jelinek, 1998
Frederick Jelinek. *Statistical methods for speech recognition*. MIT press, 1998.
- Cited in 01_introduction.md (n-gram models)

### Jiang et al., 2022
Zhengbao Jiang, Luyu Gao, Zhiruo Wang, Jun Araki, Haibo Ding, Jamie Callan, and Graham Neubig. Retrieval as attention: End-to-end learning of retrieval and reading within a single transformer. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*, pages 2336-2349, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.149.
- Cited in 05a_long-context-evaluations.md (retrieval-augmented models)

### Jiang et al., 2024
Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. *arXiv preprint arXiv:2401.04088*, 2024.
- Cited in 03_model-architecture.md (language model research)

### Jones et al., 2023
Alex Jones, Isaac Caswell, Ishank Saxena, and Orhan Firat. Bilex rx: Lexical data augmentation for massively multilingual machine translation. *arXiv preprint arXiv:2303.15265*, 2023.
- Cited in 05d_realistic-long-context-continued.md (Gatitos dataset)

### Jozefowicz et al., 2016
Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling, 2016.
- Cited in 01_introduction.md (RNN language models)

## K

### Kamradt, 2023
Gregory Kamradt. 2023. URL https://github.com/gkamradt/LLMTest_NeedleInAHaystack/blob/main/README.md.
- Cited in 01_introduction.md (needle-in-a-haystack), 05a_long-context-evaluations.md

### Kaplan et al., 2020
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. *arXiv preprint arXiv:2001.08361*, 2020.
- Cited in 05a_long-context-evaluations.md (power-law scaling)

### Karpukhin et al., 2020
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages 6769-6781, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.550.
- Cited in 05a_long-context-evaluations.md (retrieval-augmented models)

### Kembhavi et al., 2016
Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is worth a dozen images. In *ECCV*, 2016.
- Cited in 06a_core-vision-multimodal-evaluations.md (Ai2D)

### Kim et al., 2021
Young Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andres Felipe Cruz Salinas, Liyang Lu, Amr Hendy, Samyam Rajbhandari, Yuxiong He, and Hany Hassan Awadalla. Scalable and efficient moe training for multitask multilingual models. *arXiv preprint arXiv:2109.10465*, 2021.
- Cited in 03_model-architecture.md (language model research)

### Kinniment et al., 2023
Megan Kinniment, Lucas Jun Koba Sato, Haoxing Du, Brian Goodrich, Max Hasin, Lawrence Chan, Luke Harold Miles, Tao R Lin, Hjalmar Wijk, Joel Burget, et al. Evaluating language-model agents on realistic autonomous tasks. *arXiv preprint arXiv:2312.11671*, 2023.
- Cited in 09_safety-security-responsibility.md (self-proliferation evaluation)

### Kirillov et al., 2023
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 4015-4026, 2023.
- Cited in 06a_core-vision-multimodal-evaluations.md (SA-1B dataset)

### Kneser and Ney, 1995
R. Kneser and H. Ney. Improved backing-off for m-gram language modeling. In *1995 International Conference on Acoustics, Speech, and Signal Processing*, volume 1, pages 181-184 vol.1, 1995. doi: 10.1109/ICASSP.1995.479394.
- Cited in 01_introduction.md (n-gram models)

### Kocmi et al., 2023
Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof Monz, Makoto Morishita, Kenton Murray, Makoto Nagata, Toshiaki Nakazawa, Martin Popel, Maja Popovic, and Mariya Shmatova. Findings of the 2023 conference on machine translation (WMT23): LLMs are here but not quite there yet. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, *Proceedings of the Eighth Conference on Machine Translation*, pages 1-42, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.wmt-1.1.
- Cited in 06_core-capability-evaluations.md (WMT23 benchmark construction)

### Kohs, 2017
Greg Kohs. Alphago. Motion Picture, 2017. Produced by DeepMind Technologies and distributed by Netflix.
- Cited in 05b_diagnostic-long-context-continued.md (AlphaGo documentary)

### Kudugunta et al., 2023
Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A. Choquette-Choo, Katherine Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, and Orhan Firat. Madlad-400: A multilingual and document-level large audited dataset, 2023.
- Cited in 09_safety-security-responsibility.md (memorization rates)

## L

### Landeghem et al., 2023
J. Landeghem, R. Powalski, R. Tito, D. Jurkiewicz, M. Blaschko, L. Borchmann, M. Coustaty, S. Moens, M. Pietruszka, B. Ackaert, T. Stanislawek, P. Joziak, and E. Valveny. Document understanding dataset and evaluation (dude). In *2023 IEEE/CVF International Conference on Computer Vision (ICCV)*, pages 19471-19483, 2023.
- Cited in 06a_core-vision-multimodal-evaluations.md (DUDE benchmark)

### Lepikhin et al., 2020
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling giant models with conditional computation and automatic sharding. In *International Conference on Learning Representations*, 2020.
- Cited in 03_model-architecture.md (MoE research at Google), 12_appendix.md (Model Card: model architecture)

### Liu et al., 2023
Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, and Julian Eisenschlos. MatCha: Enhancing visual language pretraining with math reasoning and chart derendering. In *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 12756-12770, Toronto, Canada, July 2023. doi: 10.18653/v1/2023.acl-long.714.
- Cited in 12i_more-betterchartqa.md (prior work in chart QA evaluation)

### Liu et al., 2024
Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with ringattention, 2024.
- Cited in 05a_long-context-evaluations.md (concurrent work on 1M multimodal context)

### Lu et al., 2023
Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. *arXiv preprint arXiv:2310.02255*, 2023.
- Cited in 02_improved-gemini-1.5-pro.md (MathVista), 06a_core-vision-multimodal-evaluations.md

## M

### MAA, 2024
MAA. American invitational mathematics examination - aime. In *American Invitational Mathematics Examination - AIME 2024*, February 2024. URL https://maa.org/math-competitions/american-invitational-mathematics-examination-aime.
- Cited in 07_advancing-mathematical-reasoning.md (AIME 2024)

### Macknight, Aung, and Gomes
Macknight, Aung, and Gomes. Personal Communication.
- Cited in 09_safety-security-responsibility.md (chemical hazards evaluation approach for CBRN)

### Majumdar et al., 2024
Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, et al. Openeqa: Embodied question answering in the era of foundation models. In *2nd Workshop on Mobile Manipulation and Embodied Intelligence at ICRA 2024*, 2024.
- Cited in 06a_core-vision-multimodal-evaluations.md (OpenEQA)

### Malaviya et al., 2024
Chaitanya Malaviya, Priyanka Agrawal, Kuzman Ganchev, Pranesh Srinivasan, Fantine Huot, Jonathan Berant, Mark Yatskar, Dipanjan Das, Mirella Lapata, and Chris Alberti. Dolomites: Domain-specific long-form methodical tasks, 2024.
- Cited in 06_core-capability-evaluations.md (DOLOMITES benchmark)

### Mangalam et al., 2023
Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. EgoSchema: A diagnostic benchmark for very long-form video language understanding. In *Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track*, 2023.
- Cited in 02_improved-gemini-1.5-pro.md (EgoSchema), 05d_realistic-long-context-continued.md, 06a_core-vision-multimodal-evaluations.md

### Martins et al., 2022
Pedro Henrique Martins, Zita Marinho, and Andre Martins. Infinity-former: Infinite memory transformer. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 5468-5485, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.375.
- Cited in 05a_long-context-evaluations.md (memory-augmented models)

### Masry et al., 2022
Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: A benchmark for question answering about charts with visual and logical reasoning. In *Findings of ACL*, 2022.
- Cited in 06a_core-vision-multimodal-evaluations.md (ChartQA), 12i_more-betterchartqa.md (prior work in chart QA evaluation)

### MATH-AI-2023-Panel, 2023
MATH-AI-2023-Panel. Panel discussion. In *MATH-AI Workshop at NeurIPS 2023*, New Orleans, Louisiana, USA, December 2023. URL https://mathai2023.github.io/.
- Cited in 07_advancing-mathematical-reasoning.md (panel discussion)

### Mathew et al., 2021
Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document images. In *Proceedings of the IEEE/CVF winter conference on applications of computer vision*, pages 2200-2209, 2021.
- Cited in 06a_core-vision-multimodal-evaluations.md (DocVQA), 08_flash-8b.md

### Mathew et al., 2022
Minesh Mathew, Viraj Bagal, Ruben Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision*, pages 1697-1706, 2022.
- Cited in 02_improved-gemini-1.5-pro.md (InfographicVQA), 06a_core-vision-multimodal-evaluations.md

### Mazeika et al., 2023
Mantas Mazeika, Andy Zou, Norman Mu, Long Phan, Zifan Wang, Chunru Yu, Adam Khoja, Fengqing Jiang, Aidan O'Gara, Ellie Sakhaee, Zhen Xiang, Arezoo Rajabi, Dan Hendrycks, Radha Poovendran, Bo Li, and David Forsyth. Tdc 2023 (llm edition): The trojan detection challenge. In *NeurIPS Competition Track*, 2023.
- Cited in 09_safety-security-responsibility.md (JailbreakBench datasets)

### Mazeika et al., 2024
Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, David Forsyth, and Dan Hendrycks. Harmbench: A standardized evaluation framework for automated red teaming and robust refusal. *arXiv preprint arXiv:2402.04249*, 2024.
- Cited in 09_safety-security-responsibility.md (JailbreakBench datasets)

### Mikolov et al., 2010
Tomas Mikolov, Martin Karafiat, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Recurrent neural network based language model. In *INTERSPEECH*, 2010.
- Cited in 01_introduction.md (RNN language models)

### Min et al., 2022
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*, pages 11048-11064, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.759.
- Cited in 05d_realistic-long-context-continued.md (ICL examples limited)

### Mitchell et al., 2019a
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In *Proceedings of the Conference on Fairness, Accountability, and Transparency*, FAT* '19, page 220-229, New York, NY, USA, 2019a. Association for Computing Machinery. ISBN 9781450361255. doi: 10.1145/3287560.3287596.
- Cited in 01_introduction.md (model card)

### Mitchell et al., 2019b
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In *Proceedings of the conference on Fairness, Accountability, and Transparency*, pages 220-229, 2019b.
- Cited in 09_safety-security-responsibility.md (internal model cards)

### Mu et al., 2023
Jesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens. *arXiv preprint arXiv:2304.08467*, 2023.
- Cited in 05a_long-context-evaluations.md (memory-augmented models)

## N

### Nasr et al., 2023
Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tramer, and Katherine Lee. Scalable extraction of training data from (production) language models, 2023.
- Cited in 09_safety-security-responsibility.md (divergence attack, verbatim memorization, suffix array)

## O

### OpenAI, 2023
OpenAI. Whisper, 2023. URL https://github.com/openai/whisper.
- Cited in 05c_realistic-long-context-evaluations.md (Whisper)

### OpenAI, 2023a
OpenAI. GPT-4 Technical Report. 2023a.
- Cited in 03_model-architecture.md (language model research), 05a_long-context-evaluations.md (GPT-4 Turbo context), 06_core-capability-evaluations.md (decontamination)

### OpenAI, 2023b
OpenAI. GPT-4V(ision) System Card, 2023b.
- Cited in 09_safety-security-responsibility.md (GPT4V system card, grounded/ungrounded terminology)

### Orvieto et al., 2023
Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. *arXiv preprint arXiv:2303.06349*, 2023.
- Cited in 05a_long-context-evaluations.md (novel architectures)

## P

### Parrish et al., 2021
Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel R. Bowman. BBQ: A hand-built bias benchmark for question answering. *CoRR*, abs/2110.08193, 2021.
- Cited in 09_safety-security-responsibility.md (BBQ dataset)

### Phuong et al., 2024
Mary Phuong, Matthew Aitchison, Elliot Catt, Sarah Cogan, Alexandre Kaskasoli, Victoria Krakovna, David Lindner, Matthew Rahtz, Yannis Assael, Sarah Hodkinson, Heidi Howard, Tom Lieberum, Ramana Kumar, Maria Abi Raad, Albert Webson, Lewis Ho, Sharon Lin, Sebastian Farquhar, Marcus Hutter, Gregoire Deletang, Anian Ruoss, Seliem El-Sayed, Sasha Brown, Anca Dragan, Rohin Shah, Allan Dafoe, and Toby Shevlane. Evaluating frontier models for dangerous capabilities. *arXiv preprint:2403.13793*, 2024.
- Cited in 09_safety-security-responsibility.md (dangerous capabilities evaluations, Charm Offensive, Hidden Agenda, Money Talks, Web of Lies, self-proliferation)

### Pope et al., 2023
Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. *Proceedings of Machine Learning and Systems*, 5, 2023.
- Cited in 03_model-architecture.md (time per output character during inference)

### Popovic, 2015
Maja Popovic. chrF: character n-gram F-score for automatic MT evaluation. In Ondrej Bojar, Rajan Chatterjee, Christian Federmann, Barry Haddow, Chris Hokamp, Matthias Huck, Varvara Logacheva, and Pavel Pecina, editors, *Proceedings of the Tenth Workshop on Statistical Machine Translation*, pages 392-395, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/W15-3049.
- Cited in 05c_realistic-long-context-evaluations.md (chrF metric), 05d_realistic-long-context-continued.md

### Pratap et al., 2020
Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. Mls: A large-scale multilingual dataset for speech research. *arXiv preprint arXiv:2012.03411*, 2020.
- Cited in 06b_core-audio-multimodal-evaluations.md (Multilingual Librispeech)

### Press et al., 2021
Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. *arXiv preprint arXiv:2108.12409*, 2021.
- Cited in 05a_long-context-evaluations.md (post-training modifications)

## R

### Rae et al., 2021
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training Gopher. *CoRR*, abs/2112.11446, 2021.
- Cited in 03_model-architecture.md (language model research)

### Raffel et al., 2020
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. *Journal of Machine Learning Research*, 21(140):1-67, 2020.
- Cited in 03_model-architecture.md (language model research)

### Rein et al., 2023
David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a benchmark. *arXiv preprint arXiv:2311.12022*, 2023.
- Cited in 02_improved-gemini-1.5-pro.md (GPQA), 06_core-capability-evaluations.md (GPQA benchmark), 08_flash-8b.md

### Riquelme et al., 2021
Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andre Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts, 2021.
- Cited in 03_model-architecture.md (MoE research at Google), 12_appendix.md (Model Card: model architecture)

### Rojas et al., 2022
William A Gaviria Rojas, Sudnya Diamos, Keertan Ranjan Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman. The dollar street dataset: Images representing the geographic and socioeconomic diversity of the world. In *Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track*, 2022.
- Cited in 09_safety-security-responsibility.md (Dollar Street dataset)

### Roller et al., 2021
Stephen Roller, Sainbayar Sukhbaatar, Jason Weston, et al. Hash layers for large sparse models. *Advances in Neural Information Processing Systems*, 34:17555-17566, 2021.
- Cited in 03_model-architecture.md (language model research)

### Russell and Norvig, 2016
Stuart J Russell and Peter Norvig. *Artificial intelligence: a modern approach*. Pearson, 2016.
- Cited in 05d_realistic-long-context-continued.md (planning definition)

## S

### Saab et al., 2024
Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, et al. Capabilities of Gemini models in medicine. *arXiv preprint arXiv:2404.18416*, 2024.
- Cited in 06_core-capability-evaluations.md (long tail knowledge)

### Santhanam et al., 2021
Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. Colbertv2: Effective and efficient retrieval via lightweight late interaction. *arXiv preprint arXiv:2112.01488*, 2021.
- Cited in 05a_long-context-evaluations.md (retrieval-augmented models)

### Sellam et al., 2020
Thibault Sellam, Dipanjan Das, and Ankur Parikh. BLEURT: Learning robust metrics for text generation. In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*, pages 7881-7892, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.704.
- Cited in 05c_realistic-long-context-evaluations.md (BLEURT metric)

### Shannon, 1948
Claude Elwood Shannon. A mathematical theory of communication. *The Bell System Technical Journal*, 27:379-423, 1948.
- Cited in 01_introduction.md (2-gram language model)

### Shazeer et al., 2017
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In *ICLR (Poster)*. OpenReview.net, 2017.
- Cited in 03_model-architecture.md (MoE research at Google), 12_appendix.md (Model Card: model architecture)

### Shevlane et al., 2023
Toby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, Jade Leung, Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, Lewis Ho, Divya Siddarth, Shahar Avin, Will Hawkins, Been Kim, Iason Gabriel, Vijay Bolina, Jack Clark, Yoshua Bengio, Paul Christiano, and Allan Dafoe. Model evaluation for extreme risks. *arXiv preprint arXiv:2305.15324*, 2023.
- Cited in 09_safety-security-responsibility.md (dangerous capabilities)

### Shi et al., 2023a
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language Models are Multilingual Chain-of-Thought Reasoners. In *Proceedings of ICLR 2023*, 2023a.
- Cited in 06_core-capability-evaluations.md (MGSM), 08_flash-8b.md

### Shi et al., 2023b
Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Victoria Lin, Noah A Smith, Luke Zettlemoyer, Scott Yih, and Mike Lewis. In-context pretraining: Language modeling beyond document boundaries. *arXiv preprint arXiv:2310.10638*, 2023b.
- Cited in 05a_long-context-evaluations.md (long-context datasets)

## S (continued)

### Singh et al., 2019
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards VQA models that can read. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 8317-8326, 2019.
- Cited in 06a_core-vision-multimodal-evaluations.md (TextVQA), 08_flash-8b.md

### Singh et al., 2023
Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans using large language models. In *2023 IEEE International Conference on Robotics and Automation (ICRA)*, pages 11523-11530. IEEE, 2023.
- Cited in 05d_realistic-long-context-continued.md (LLM common sense for planning)

### Srivastava et al., 2022
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. *arXiv preprint arXiv:2206.04615*, 2022.
- Cited in 06_core-capability-evaluations.md (BigBench), 08_flash-8b.md, 12h_more-mtob-results.md (BIG-bench canary for contamination mitigation)

### Srivastava et al., 2024
Saurabh Srivastava, Annarose M B, Anto P V au2, Shashank Menon, Ajay Sukumar, Adwaith Samod T, Alan Philipose, Stevin Prince, and Sooraj Thomas. Functional benchmarks for robust evaluation of reasoning performance, and the reasoning gap, 2024.
- Cited in 06_core-capability-evaluations.md (Functional MATH)

### Staniszewski et al., 2023
Konrad Staniszewski, Szymon Tworkowski, Sebastian Jaszczur, Henryk Michalewski, Lukasz Kucinski, and Piotr Milos. Structured packing in llm training improves long context utilization. *arXiv preprint arXiv:2312.17296*, 2023.
- Cited in 05a_long-context-evaluations.md (long-context datasets)

### Suzgun et al., 2022
Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. *arXiv preprint arXiv:2210.09261*, 2022.
- Cited in 06_core-capability-evaluations.md (BigBench-Hard), 08_flash-8b.md

## T

### Toshniwal et al., 2024
Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman. Openmathinstruct-1: A 1.8 million math instruction tuning dataset, 2024.
- Cited in 12_appendix.md (targeted Python fine-tuning alternative for long-context math prompting, Section 12.6.4)

### Tanzer et al., 2023
Garrett Tanzer, Mirac Suzgun, Eline Visser, Dan Jurafsky, and Luke Melas-Kyriazi. A benchmark for learning to translate a new language from one grammar book. In *Arxiv*, 2023.
- Cited in 05c_realistic-long-context-evaluations.md (MTOB benchmark), 12g_blink-realworldqa-video-prompts.md (MTOB test splits and prompt comparison baselines), 12h_more-mtob-results.md (MTOB ablation discussion, Tables 55/56 captions, qualitative examples contamination measures)

### Team et al., 2022
NLLB Team, Marta R. Costa-jussa, James Cross, Onur Celebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzman, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language left behind: Scaling human-centered machine translation. 2022.
- Cited in 05d_realistic-long-context-continued.md (Flores-200)

### Thoppilan et al., 2022
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. LaMDA: Language models for dialog applications. *arXiv preprint arXiv:2201.08239*, 2022.
- Cited in 03_model-architecture.md (language model research)

### Tom et al., 2023
Same paper as Kocmi et al., 2023 (listed under first name "Tom Kocmi" in Table 11 citation). See Kocmi et al., 2023 above for full entry.
- Cited in 06_core-capability-evaluations.md (WMT23 benchmark, Table 11)

### Touvron et al., 2023a
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*, 2023a.
- Cited in 03_model-architecture.md (language model research)

### Touvron et al., 2023b
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*, 2023b.
- Cited in 03_model-architecture.md (language model research)

## V

### Valmeekam et al., 2024
Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. On the planning abilities of large language models-a critical investigation. *Advances in Neural Information Processing Systems*, 36, 2024.
- Cited in 05d_realistic-long-context-continued.md (LLM plan generation)

### Vaswani et al., 2017
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. *CoRR*, abs/1706.03762, 2017.
- Cited in 01_introduction.md (Transformer architecture), 03_model-architecture.md (language model research)

### Vedantam et al., 2015
Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 4566-4575, 2015.
- Cited in 06a_core-vision-multimodal-evaluations.md (CIDER metric), 08_flash-8b.md (CIDER metric for VATEX)

### Visser, 2020a
Eline Visser. Kalamang dictionary. *Dictionaria*, (13):1-2737, 2020a. URL https://dictionaria.clld.org/contributions/kalamang.
- Cited in 05c_realistic-long-context-evaluations.md (Kalamang bilingual wordlist and parallel sentences)

### Visser, 2020b
Eline Visser. A grammar of kalamang: The papuan language of the karas islands. 2020b.
- Cited in 05c_realistic-long-context-evaluations.md (Kalamang reference grammar), 10_discussion.md

### Visser, 2020c
Eline Visser. The Kalamang collection: an archive of linguistic and cultural material from Karas. Lund: Humlab, Lund University Humanities Lab, 2020c. URL http://hdl.handle.net/10050/00-0000-0000-0003-C3E8-1.
- Cited in 05c_realistic-long-context-evaluations.md (The Kalamang Collection for ASROB)

## W

### Wang et al., 2019a
Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. VATEX: A large-scale, high-quality multilingual dataset for video-and-language research. In *ICCV*, 2019a.
- Cited in 06a_core-vision-multimodal-evaluations.md (VATEX), 08_flash-8b.md

### Wang et al., 2019b
Xinda Wang, Kun Sun, Archer Batcheller, and Sushil Jajodia. Detecting "0-day" vulnerability: An empirical study of secret security patch in OSS. In *IEEE/IFIP International Conference on Dependable Systems and Networks*, pages 485-492. IEEE, 2019b.
- Cited in 09_safety-security-responsibility.md (vulnerability detection datasets)

### Wang et al., 2020
Changhan Wang, Anne Wu, and Juan Pino. Covost 2 and massively multilingual speech-to-text translation. *arXiv preprint arXiv:2007.10310*, 2020.
- Cited in 06b_core-audio-multimodal-evaluations.md (CoVoST-2), 08_flash-8b.md

### Wang et al., 2021
Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux. Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. *arXiv preprint arXiv:2101.00390*, 2021.
- Cited in 05b_diagnostic-long-context-continued.md (VoxPopuli dataset)

### Weidinger et al., 2021
Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. Ethical and social risks of harm from language models. *arXiv preprint arXiv:2112.04359*, 2021.
- Cited in 09_safety-security-responsibility.md (representational harms from audio)

### Weidinger et al., 2024
Laura Weidinger, Joslyn Barnhart, Jenny Brennan, Christina Butterfield, Susie Young, Will Hawkins, Lisa Anne Hendricks, Ramona Comanescu, Oscar Chang, Mikel Rodriguez, et al. Holistic safety and responsibility evaluations of advanced ai models. *arXiv preprint arXiv:2404.14068*, 2024.
- Cited in 09_safety-security-responsibility.md (assurance evaluations definition)

### World Economic Forum, 2023
World Economic Forum. Jobs of Tomorrow: Large Language Models and Jobs. 2023. URL https://www3.weforum.org/docs/WEF_Jobs_of_Tomorrow_Generative_AI_2023.pdf.
- Cited in 06_core-capability-evaluations.md (productivity studies)

### Wu and Xie, 2023
Penghao Wu and Saining Xie. V*: Guided visual search as a core mechanism in multimodal llms. *arXiv preprint arXiv:2312.14135*, 2023.
- Cited in 06a_core-vision-multimodal-evaluations.md (V* Benchmark, SEAL technique)

### Wu et al., 2022a
Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, Alborz Geramifard, and Zhou Yu. Memformer: A memory-augmented transformer for sequence modeling. In Yulan He, Heng Ji, Sujian Li, Yang Liu, and Chua-Hui Chang, editors, *Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022*, pages 308-318, Online only, November 2022a. Association for Computational Linguistics.
- Cited in 05a_long-context-evaluations.md (memory-augmented models)

### Wu et al., 2022b
Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. *arXiv preprint arXiv:2203.08913*, 2022b.
- Cited in 05a_long-context-evaluations.md (memory-augmented models)

### wunderwuzzi, 2023
wunderwuzzi. Hacking Google Bard - From Prompt Injection to Data Exfiltration. https://embracethered.com/blog/posts/2023/google-bard-data-exfiltration/, 2023.
- Cited in 09_safety-security-responsibility.md (prompt injection attack template)

## X

### XLA, 2019
XLA. XLA: Optimizing compiler for TensorFlow. https://www.tensorflow.org/xla, 2019. [Online; accessed December-2023].
- Cited in 12_appendix.md (Model Card: Hardware & Software -- XLA compiler powering JAX)

### x.ai
x.ai. Grok-1.5 vision preview. URL https://x.ai/blog/grok-1.5v.
- Cited in 06a_core-vision-multimodal-evaluations.md (RealWorldQA, Ai2D evaluation protocol)

### Xiong et al., 2023
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. *arXiv preprint arXiv:2309.16039*, 2023.
- Cited in 05a_long-context-evaluations.md (post-training modifications)

### Xu et al., 2021
Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, et al. Gspmd: general and scalable parallelization for ml computation graphs. *arXiv preprint arXiv:2105.04663*, 2021.
- Cited in 12_appendix.md (Model Card: Hardware & Software -- GSPMD partitioner for automatic parallelization)

## Y

### Yan et al., 2024
Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. Berkeley function calling leaderboard. 2024.
- Cited in 06_core-capability-evaluations.md (BFCL benchmark)

### Yang et al., 2023a
Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers, 2023a.
- Cited in 12_appendix.md (automatically optimized prompt alternative for long-context math prompting, Section 12.6.4)

### Yang et al., 2023b
John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. InterCode: Standardizing and benchmarking interactive coding with execution feedback. *ArXiv*, June 2023b.
- Cited in 09_safety-security-responsibility.md (InterCodeCTF)

### Yeung, 2024
Kenneth Yeung. New google gemini vulnerability enabling profound misuse. 2024. URL https://hiddenlayer.com/research/new-google-gemini-content-manipulation-vulns-found/#Indirect-Injections-are-back!
- Cited in 09_safety-security-responsibility.md (prompt injection attack template)

### Yu et al., 2019
Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. ActivityNet-QA: A dataset for understanding complex web videos via question answering. In *AAAI*, 2019.
- Cited in 06a_core-vision-multimodal-evaluations.md (ActivityNet-QA)

### Yue et al., 2023
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi, 2023.
- Cited in 06a_core-vision-multimodal-evaluations.md (MMMU), 08_flash-8b.md

## Z

### Zaheer et al., 2020
Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. *Advances in Neural Information Processing Systems*, 33:17283-17297, 2020.
- Cited in 05a_long-context-evaluations.md (novel architectures)

### Zellers et al., 2019
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? *arXiv preprint arXiv:1905.07830*, 2019.
- Cited in 06_core-capability-evaluations.md (Hellaswag benchmark)

### Zhang and Re, 2022
Michael Zhang and Christopher Re. Contrastive adapters for foundation model group robustness. *Advances in Neural Information Processing Systems*, 35:21682-21697, 2022.
- Cited in 09_safety-security-responsibility.md (robustness metrics for Dollar Street)

### Zhang et al., 2023a
Biao Zhang, Barry Haddow, and Alexandra Birch. Prompting large language model for machine translation: a case study. In *Proceedings of the 40th International Conference on Machine Learning*, ICML'23. JMLR.org, 2023a.
- Cited in 05d_realistic-long-context-continued.md (ICL examples limited)

### Zhang et al., 2023b
Yu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen, Bo Li, Vera Axelrod, Gary Wang, Zhong Meng, Ke Hu, Andrew Rosenberg, Rohit Prabhavalkar, Daniel S. Park, Parisa Haghani, Jason Riesa, Ginger Perng, Hagen Soltau, Trevor Strohman, Bhuvana Ramabhadran, Tara Sainath, Pedro Moreno, Chung-Cheng Chiu, Johan Schalkwyk, Francoise Beaufays, and Yonghui Wu. Google usm: Scaling automatic speech recognition beyond 100 languages. *arXiv preprint arXiv:2303.01037*, 2023b.
- Cited in 05c_realistic-long-context-evaluations.md (USM model)

### Zhao et al., 2021
Dora Zhao, Angelina Wang, and Olga Russakovsky. Understanding and evaluating racial biases in image captioning. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 14830-14840, 2021.
- Cited in 09_safety-security-responsibility.md (COCO captioning skin tone annotations)

### Zhong et al., 2022
Zexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmentation. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*, pages 5657-5673, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.382.
- Cited in 05a_long-context-evaluations.md (memory-augmented models)

### Zhou et al., 2018
Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards automatic learning of procedures from web instructional videos. In *AAAI Conference on Artificial Intelligence*, pages 7590-7598, 2018.
- Cited in 06a_core-vision-multimodal-evaluations.md (YouCook2)

### Zhou et al., 2021
Yaqin Zhou, Jing Kai Siow, Chenyu Wang, Shangqing Liu, and Yang Liu. SPI: Automated identification of security patches via commits. *ACM Transactions on Software Engineering and Methodology*, 31(1):1-27, 2021.
- Cited in 09_safety-security-responsibility.md (vulnerability detection datasets)

### Zhu et al., 2022
Fengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang, Haozhou Zhang, and Tat-Seng Chua. Towards complex document understanding by discrete reasoning. In *Proceedings of the 30th ACM International Conference on Multimedia*, page 4857-4866, 2022.
- Cited in 06a_core-vision-multimodal-evaluations.md (TAT-DQA)

### Zoph et al., 2022
Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. Designing effective sparse expert models. *arXiv preprint arXiv:2202.08906*, 2022.
- Cited in 03_model-architecture.md (MoE research at Google), 12_appendix.md (Model Card: model architecture)

### Zou et al., 2023
Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. *arXiv preprint arXiv:2307.15043*, 2023.
- Cited in 09_safety-security-responsibility.md (jailbreak attacks, GCG algorithm)
