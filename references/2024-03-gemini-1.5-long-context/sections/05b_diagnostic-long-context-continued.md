# 5.2.1. Diagnostic Long-Context Evaluations (continued) [p. 12–16]

### 5.2.1.3 Video Haystack

[p. 12] As Gemini 1.5 Pro is natively multimodal, its long-context abilities translate directly to other modalities, enabling it to retrieve specific information across multiple hours of video. To test this capability, the text needle-in-a-haystack evaluation is adapted into a cross-modal evaluation, wherein a needle is hidden in one modality while the retrieval query is given in text. Rather than asking the model to retrieve a randomly inserted phrase from a corpus of text, the model is asked to retrieve information embedded in a random frame (the "needle") in a 10.5-hour-long video (the "haystack") that is sampled at one frame-per-second.

Concretely, the text `"The secret word is "needle""` is overlaid on a single randomly sampled video frame in a 10.5 hour video constructed from concatenating seven copies of the full AlphaGo documentary (Kohs, 2017) back-to-back (for a total of 37994 frames, or 9.9M tokens). See Figure 28 in the Appendix for an example of such an embedded frame. After feeding it the video, the model is asked to answer the question "What is the secret word?". [p. 12]

[p. 13] As Figure 9 shows, Gemini 1.5 Pro successfully answers this question across a breadth of video lengths and a range of randomly inserted needle locations in the 10.5 hour video. In contrast, the GPT-4V API supports video lengths only up to around the first 3 minutes. Gemini 1.5 Flash was also evaluated on the video-haystack problem with up to 2M tokens and obtained >99.8% recall, demonstrating its best-in-class long-context retrieval performance in vision modality. [p. 13]

**Figure 9** (p. 13): "Video Haystack. This figure compares Gemini 1.5 Pro with GPT-4V for the video needle-in-a-haystack task, where the models are given video clips of different lengths up to 10.5 hours of video and are asked to retrieve a secret word embedded as text at different points within the clip. All video clips are sampled at one frame-per-second (1 fps). The first pair of 10 x 50 haystack plots on the left compare Gemini 1.5 Pro with GPT-4V on the first hour of the AlphaGo documentary. The x-axis represents the video duration which ranges from 1.2 minutes to 1 hour, and the y-axis represents the depth, namely the relative offset of the needle (e.g., the top left cell represents providing the model with the first 1.2 minutes and inserting the needle in a randomly sampled frame in the first seven seconds of that trimmed video). A green cell indicates that the model successfully retrieved the needle, whereas a gray cell indicates an API error. Whereas the GPT-4V API supports video lengths only up to around the first 3 minutes, Gemini 1.5 Pro successfully retrieves the secret word inserted at all depth percentages for the full hour, as shown by the all-green plot. Finally, the 10 x 10 grid on the right shows Gemini 1.5 Pro's perfect retrieval capabilities across 10.5 hours of video, constructed by concatenating seven copies of the AlphaGo documentary back-to-back."
- **Top left panel (Gemini 1.5 Pro: 1 minute to 1 hour):** A 10 x 50 heatmap with x-axis "Minutes" (6 to 60) and y-axis "Depth (%)" (10, 30, 50, 70, 90). Entirely green cells -- perfect recall at all depths and video durations up to 1 hour.
- **Top right panel (Up to 10 hours):** A 10 x 10 heatmap with x-axis "Hours" (2, 4, 6, 8, 10) and y-axis "Depth (%)". Entirely green cells -- perfect recall even across 10.5 hours of video (9.9M tokens).
- **Bottom panel (GPT-4V: 1 minute to 1 hour):** A 10 x 50 heatmap with x-axis "Minutes" (6 to 60) and y-axis "Depth (%)" (10, 30, 50, 70, 90). Green cells visible only in the leftmost columns (up to ~3 minutes). All cells beyond ~3 minutes are gray (API errors), indicating that GPT-4V API does not support longer video inputs.

### 5.2.1.4 Audio Haystack

[p. 13–14] A similar strategy is followed for testing Gemini 1.5 Pro's long context capabilities on audio understanding. A very short clip of audio lasting a few seconds where a speaker says `"the secret keyword is needle"` is hidden within an audio signal (the haystack) up to almost five days long (i.e., 107 hours). The task for the model is then to retrieve the secret keyword, given a question in text, hence requiring cross-modal reasoning. To further challenge the model beyond increasing context, the large audio signal is built from an unlabeled speech corpus from the VoxPopuli dataset (Wang et al., 2021) so that the input signal contains multiple speakers.

[p. 14] In Figure 10 the result of the experiment is plotted when the input audio ranges from 12 minutes to 107 hours (or 9.9M tokens), inserting the needle in different positions across the signal. The red boxes indicate a score of 0.0 (meaning the model did not identify the keyword), and green indicates a score of 1.0 (meaning the model identified the keyword correctly). The Gemini 1.5 models succeed at finding the secret keyword in all instances, with the overall accuracy of Gemini 1.5 Pro being 100% and Gemini 1.5 Flash being 98.7% on this task. [p. 14]

Unlike Gemini 1.5 Pro and Gemini 1.5 Flash, existing models cannot natively handle more than a few seconds of audio in the context. As such, in order to fairly compare against them, a strategy is employed where the audio is first transcribed into text using windows of tens of seconds, and then text models are used to extend beyond that limited window. [p. 14]

Specifically, to compare against Whisper, the audio input is chunked into 30 second segments, the audio is transcribed using the model to produce a text transcript, the transcripts for each chunk are concatenated, and finally GPT-4 Turbo is prompted to find the "secret keyword" given the text transcript. Figure 10 shows the performance for each depth percent and number of hours. The overall accuracy of Whisper combined with GPT-4 Turbo to identify the needle is around 94.5%. [p. 14]

**Figure 10** (p. 14): "Audio Haystack. This figure presents the audio version of the needle-in-the-haystack experiment comparing Gemini 1.5 Pro and a combination of Whisper and GPT-4 Turbo. In this setting, the needle is a short segment of audio that is inserted within a very large audio segment (of up to 107 hours) containing concatenated audio clips. The task is to retrieve the 'secret keyword' which is revealed in the needle. Red indicates that the model did not identify the keyword, whereas green indicates that the model identified the keyword correctly."
- **Top left panel (Gemini 1.5 Pro: From 12 minutes to 11 hours):** A heatmap with x-axis "Minutes" (36 to 660) and y-axis "Depth (%)" (10, 30, 50, 70, 90). Entirely green cells -- 100% accuracy at all depths and audio durations up to 11 hours.
- **Top right panel (Up to 107 hours):** A smaller heatmap with x-axis "Minutes" (640 to 5760) and y-axis "Depth (%)". Entirely green cells -- 100% accuracy across audio durations up to 107 hours (9.9M tokens).
- **Bottom panel (Whisper + GPT-4 Turbo: From 12 minutes to 11 hours):** A heatmap with x-axis "Minutes" (36 to 660) and y-axis "Depth (%)" (10, 30, 50, 70, 90). Mix of green and red cells, with red cells (failures) scattered throughout all depths and durations. Notably more failures at depth 10 and 30. Overall accuracy around 94.5%.

### 5.2.1.5 Improved Diagnostics

[p. 14] Despite the excellent performance of Gemini 1.5 Pro and Gemini 1.5 Flash on the needle-in-a-haystack tasks for all three modalities, significantly surpassing previously reported results (>99.7% for text, 100% for video and 100% for audio), the authors also present early findings of observed limitations. By design, the needle-in-a-haystack task is a retrieval task measuring recall and so far the simplest possible setup has been considered. A natural extension to the task is to increase the number of unique "needles" in each haystack, and require the model to retrieve them all. For a context length of up to 1M tokens, 100 different needles are inserted and the total number of correct needles retrieved is measured. [p. 14]

## Multiple Needles-in-a-Haystack

[p. 15] Figure 11 compares the recall of Gemini 1.5 Pro^9 and GPT-4 Turbo on the "multiple needles-in-haystack" task, which requires retrieving 100 unique needles in a single turn.

When comparing Gemini 1.5 Pro to GPT-4 Turbo, higher recall is observed at shorter context lengths, and a very small decrease in recall towards 1M tokens. An improved recall from Gemini 1.5 Pro over GPT-4 Turbo is seen up until 128K tokens. It is important to note that GPT-4 Turbo's context length is limited to 128K tokens and its retrieval quality largely oscillates with longer context lengths with an average recall of around 50% at 128K tokens. In contrast, Gemini 1.5 Pro maintains around 70% recall up to 128K tokens, and >60% recall up to 1M tokens. Further results on this task are reported in the Appendix 12.2, including results with different numbers of needles, where consistent trends are observed. [p. 15]

In line with other tasks in the literature of LLMs, it is also observed that the choice of the prompting method and type of needle affect final performance of models, and future versions of "needle(s)-in-a-haystack" style tests should account for prompt robustness. [p. 15]

**Figure 11** (p. 15): "Retrieval performance of the 'multiple needles-in-haystack' task, which requires retrieving 100 unique needles in a single turn. When comparing Gemini 1.5 Pro to GPT-4 Turbo we observe higher recall at shorter context lengths, and a very small decrease in recall towards 1M tokens."
- X-axis: "Number of tokens in context" (1K to 1M, log scale). Y-axis: "Recall" (0.0 to 1.0).
- **Gemini 1.5 Pro** (blue dots): Recall generally ranges between 0.6 and 1.0, with most data points clustered between 0.6 and 0.9 across context lengths. A few outliers dip lower. Maintains around 70% recall at 128K and >60% at 1M.
- **GPT-4 Turbo** (red/pink dots): Recall is highly variable, ranging from near 0.0 to ~0.8. At 1K tokens, recall is very low (around 0.1-0.3). Performance oscillates significantly across context lengths up to its 128K limit, with an average of about 50% at 128K tokens. One very low outlier near 0.0 at around 1K tokens.

> ^9 "All the results for this task are obtained with the previously reported Gemini 1.5 Pro version from February." [p. 15]

## Multi-round Co-reference Resolution (MRCR)

[p. 15] Retrieval difficulty is also modulated on another axis: the similarity of the needles. In the Multi-round Co-reference Resolution (MRCR) task, the model is presented with a long conversation between a user and a model, in which the user requests writing (e.g. poems, riddles, essays) on different topics proceeded by the model responses. In each conversation, two user requests containing topics and writing formats distinct from the rest of the conversation are randomly placed in the context. Given the conversation, the model must reproduce the model's output (the needle) resulting from one of the two requests (the key). Either the formats, the topics, or both, overlap in order to create a single key that is adversarially similar to the query key. For instance, the request "Reproduce the poem about penguins." requires the model to distinguish the poem about penguins from the poem about flamingos, and "Reproduce the *first* poem about penguins." requires the model to reason about ordering. MRCR is scored via a string-similarity measure between the model output and the correct response.^10 [p. 15]

[p. 15–16] Figure 12 compares the capabilities of Gemini 1.5 Pro, Gemini 1.5 Flash, GPT-4 Turbo 01-25-2024 and 04-09-2024 models, Claude 3 models, and Claude 2.1 on MRCR. Key findings:
- Gemini 1.5 Pro overtakes GPT-4 Turbo 04-09-2024 and Claude 3 Opus at around 32K tokens and GPT-4 Turbo 01-25-2024 at around 8K tokens
- Gemini 1.5 Flash consistently dominates Claude 3 Sonnet at around 5K tokens (though performs comparably at < 5K tokens), overtakes GPT-4 Turbo 01-25-2024 at around 20K tokens, and always strictly dominates Claude 3 Haiku
- After 32K tokens, Gemini 1.5 Flash is on par with both GPT-4 Turbo 04-09-2024 and Claude 3 Opus, and retains this performance all the way to 1M tokens
- Both Gemini models achieve an average score of around 75% at 1M tokens
- GPT-4 Turbo and Claude 3 model performances fall off steadily as a function of context length, all the way up to 128K tokens, though GPT-4 Turbo 04-09-2024 and Claude 3 Opus have less negative slopes compared to the other models from their families
- Claude 2.1 (with context length going up to 200K tokens) under performs the other models by hallucinating that the needle is not in context and punting on requests to solve the task, despite following Claude 2.1 prompting guidelines for retrieval tasks (Anthropic, 2023b)

**Figure 12** (p. 16): "Cumulative average string similarity score as a function of context length over 2000 instances of the MRCR task. When comparing Gemini 1.5 Pro and Gemini 1.5 Flash to GPT-4 Turbo and Claude 3 models, we observe that after 32K tokens, Gemini 1.5 Pro outperforms Claude 3 Opus and GPT-4 Turbo (04-09-2024). Gemini 1.5 Flash outperforms Claude 3 Sonnet starting at around 5K tokens, GPT-4 Turbo (01-25-2024) at around 20K tokens, and always outperforms Claude 3 Haiku and Claude 2.1. Both Gemini 1.5 Pro and Gemini 1.5 Flash have smaller decreases in performance as a function of context length up to 1M tokens compared to all competitors."
- X-axis: "Number of tokens in context" (2K to 1M, log scale). Y-axis: "Cumulative Average Score" (0.0 to 1.0). Title: "MRCR". A dashed vertical line at 128K tokens.
- **Gemini 1.5 Pro** (solid blue line): Starts around 0.85 at 2K, gradually decreases but remains around 0.75 at 1M tokens. Highest-performing model at longer context lengths.
- **Gemini 1.5 Flash** (dashed blue line): Starts around 0.80 at 2K, gradually decreases to around 0.75 at 1M tokens. Very close to Gemini 1.5 Pro at longer contexts.
- **GPT-4 Turbo (040924)** (dashed red line): Starts around 0.85 at 2K, decreases more steeply than Gemini models, ending around 0.65 at 128K tokens.
- **GPT-4 Turbo (012524)** (dashed pink line): Starts around 0.80 at 2K, decreases to around 0.55 at 128K tokens.
- **Claude 3 Opus** (dashed green line): Starts around 0.85 at 2K, decreases to around 0.6 at 128K tokens.
- **Claude 3 Sonnet** (dashed medium-green line): Starts around 0.75 at 2K, decreases to around 0.4-0.5 at 128K tokens.
- **Claude 3 Haiku** (dashed light-green line): Starts around 0.65 at 2K, decreases to around 0.3-0.4 at 128K tokens.
- **Claude 2.1** (dashed orange line): Starts around 0.25 at 2K, remains very low (around 0.2) across all context lengths up to 200K tokens.

> ^10 "SequenceMatcher ratio as implemented in https://docs.python.org/3/library/difflib.html" [p. 15]

[p. 16] The authors highlight that "multiple needles-in-a-haystack" and MRCR capture different aspects of the retrieval task: MRCR is harder and requires stronger reasoning and disambiguation skills, while the multiple needles challenge is a test of the model's recall ability, explaining disparities between the model orderings up to 8K tokens. Gemini 1.5 Pro and Gemini 1.5 Flash impressively avoid serious degradation on both of these axes all the way up to 1M tokens.

[p. 16] While the "multiple needles-in-a-haystack" and MRCR evaluations offer two challenging setups that stress-test long-context retrieval and reasoning capabilities in different ways, the authors advocate for pushing the boundaries even further. Evaluating models on tasks that demand complex reasoning over multiple pieces of information scattered across a long context would provide even deeper insights into their true capabilities. This could involve tasks that require integrating disparate facts, drawing inferences, or resolving inconsistencies within the retrieved information. By incorporating such assessments alongside prompt robustness studies, a more comprehensive and nuanced understanding of how effectively models can utilize long contexts for advanced reasoning and knowledge extraction can be gained.
