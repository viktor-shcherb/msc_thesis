# 10. Discussion [p. 74]

[p. 74] The authors present Gemini 1.5 Pro and Gemini 1.5 Flash, the first members of the Gemini 1.5 family. This new family of multi-modal models pushes the boundary of *efficiency*, *multi-modality*, *long-context reasoning* and *downstream performance*. Gemini 1.5 Pro extends the context window over the Gemini 1.0 series from 32K to multiple millions of tokens, making this the first commercially available models to greatly surpass the current ceiling of 200k tokens offered by Claude 3 across modalities. Improved long-context performance out to 10M tokens has been further demonstrated. [p. 74]

[p. 74] Extensive evaluations with diagnostic and realistic multi-modal long-context benchmarks show that 1.5 Pro is able to maintain near-perfect recall on multi-modal versions of needle-in-a-haystack (see Section 5.2.1.2) and is able to effectively use its context to retrieve and reason over large amounts of data. This enables the model to perform realistic long-context tasks such as long-document QA from 700k-word material and long-video QA from 40 to 105 minutes long videos. Finally, Gemini 1.5 series have the ability to use in-context learn to translate from English to Kalamang, an extremely low-resource language with fewer than 200 speakers (Visser, 2020b). This capability is achieved solely by providing a grammar manual in the models' context at inference time, which demonstrates the Gemini 1.5 Pro's and Gemini 1.5 Flash's remarkable ability to in-context learn from information it has never seen before at training time. [p. 74]

[p. 74] Most importantly, this leap in long-context performance of the 1.5 Gemini series does not come at the expense of the multi-modal core capabilities (i.e., performance on non long-context tasks) that the 1.0 series excelled at. 1.5 Pro is able to outperform 1.0 Pro across the board of the comprehensive evaluation benchmarks presented in this report. More strikingly, 1.5 Pro, despite using significantly less training compute, surpasses 1.0 Ultra, a state-of-the-art model, on text capabilities like math, science and reasoning, code, multilinguality and instruction following. Similarly, 1.5 Flash outperforms 1.0 Pro, despite being more lightweight and efficient. The authors conclude that the Gemini 1.5 series presents a generational leap in performance in comparison to the Gemini 1.0 series. [p. 74]

## Long-context evaluations, call-to-action

[p. 74] Evaluating the capabilities of models that can handle very long contexts presents a new set of challenges, especially in the multi-modal domain where text, images, video, and audio can be combined. Current benchmarks often fail to adequately stress-test models like those in Gemini 1.5 series, as they are typically designed for evaluating shorter context models. As the evaluation requirements for frontier models increasingly require benchmarks with both length and complexity, the task of human labeling and annotation will become significantly more costly and time-consuming. This additionally challenges traditional evaluation methods that rely heavily on manual evaluation. [p. 74]

[p. 74] Given the limitations of existing benchmarks and the challenges of human annotation, there is a pressing need for innovative evaluation methodologies. These methodologies should be able to effectively assess model performance on very long-context tasks while minimizing the burden of human labeling. To begin addressing some of these concerns, the authors recommend researchers and practitioners adopt a "multiple needles-in-haystack" setup for diagnostic evaluations which was observed to be signal-rich and more challenging compared to its "single needle" counterpart. The authors believe there is room for new benchmark tasks based on new or improved automatic metrics that require complex reasoning over long inputs (both human and model generated). This is also an intriguing research direction for creating challenging evaluations that stress more than just the retrieval capabilities of long-context models. The authors will continue the development of such benchmarks for realistic and comprehensive evaluation of model capabilities in the multimodal space. By addressing these open challenges and developing new benchmarks and evaluation methodologies, progress can be driven in the field of very long-context AI models and their full potential unlocked. [p. 74]
