# References

Only references cited in the section notes are included below.

---

**01.AI, 2023a**
01.AI. 2023a. Yi-34b-200k. https://huggingface.co/01-ai/Yi-34B-200K.
- Cited in 01_introduction.md and 02_related-work.md as an open-source 100K+ LLM.

**01.AI, 2023b**
01.AI. 2023b. Yi-6b-200k. https://huggingface.co/01-ai/Yi-6B-200K.
- Cited in 01_introduction.md and 02_related-work.md as an open-source 100K+ LLM.

**AI, 2023**
Moonshot AI. 2023. Kimi chat. https://kimi.moonshot.cn/.
- Cited in 01_introduction.md, 02_related-work.md, and 04_experiments.md as the Kimi-Chat model baseline.

**Ainslie et al., 2023**
Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr'on, and Sumit K. Sanghai. 2023. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. *ArXiv*, abs/2305.13245.
- Cited in 02_related-work.md as a memory optimization approach for attention.

**An et al., 2023**
Chen An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: Instituting standardized evaluation for long context language models. *ArXiv*, abs/2307.11088.
- Cited in 02_related-work.md as the L-Eval benchmark.

**Anil et al., 2022**
Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. 2022. Exploring length generalization in large language models. *Advances in Neural Information Processing Systems*, 35:38546–38556.
- Cited in 01_introduction.md as an effort to improve length generalization.

**Anthropic, 2023**
Anthropic. 2023. Model card and evaluations for claude models.
- Cited in 01_introduction.md, 02_related-work.md, and 04_experiments.md as the Claude 2 model baseline.

**Bai et al., 2023**
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023. Longbench: A bilingual, multitask benchmark for long context understanding.
- Cited in 01_introduction.md and 02_related-work.md as an existing long-context benchmark (~10K tokens).

**Beltagy et al., 2020**
Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. *arXiv preprint arXiv:2004.05150*.
- Cited in 02_related-work.md as a sliding window attention approach.

**Biderman et al., 2023**
Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. In *International Conference on Machine Learning*, pages 2397–2430. PMLR.
- Cited in 01_introduction.md as evidence that LLMs are typically trained on sequences up to 8K tokens.

**Brown et al., 2020**
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. *CoRR*, abs/2005.14165.
- Cited in 01_introduction.md as a foundational LLM reference.

**Bubeck et al., 2023**
Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. *arXiv preprint arXiv:2303.12712*.
- Cited in 03_infinitebench.md as demonstrating GPT-4's state tracking capability.

**Chen et al., 2021**
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. *arXiv preprint arXiv:2107.03374*.
- Cited in 01_introduction.md as an example of LLM-based code repository analysis.

**Chen et al., 2023a**
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023a. Extending context window of large language models via positional interpolation. *arXiv preprint arXiv:2306.15595*.
- Cited in 01_introduction.md and 02_related-work.md as a method for extending context via positional interpolation.

**Chen et al., 2023b**
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023b. Extending context window of large language models via positional interpolation. *ArXiv*, abs/2306.15595.
- Cited in 01_introduction.md as an effort to improve length generalization.

**Dao, 2023**
Tri Dao. 2023. FlashAttention-2: Faster attention with better parallelism and work partitioning.
- Cited in 01_introduction.md and 02_related-work.md as improved training infrastructure for long sequences.

**Dao et al., 2022**
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. *Advances in Neural Information Processing Systems*, 35:16344–16359.
- Cited in 01_introduction.md and 02_related-work.md as improved IO management for attention.

**Dao et al., 2023**
Tri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. 2023. Flash-decoding for long-context inference.
- Cited in 02_related-work.md as enhanced parallelization in decoding.

**Dasigi et al., 2021**
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021. A dataset of information-seeking questions and answers anchored in research papers. *ArXiv*, abs/2105.03011.
- Cited in 02_related-work.md as a pre-existing task aggregated into LongBench.

**Dong et al., 2023**
Zican Dong, Tianyi Tang, Lunyi Li, and Wayne Xin Zhao. 2023. A survey on long text modeling with transformers. *arXiv preprint arXiv:2302.14502*.
- Cited in 01_introduction.md on the importance of long-context processing.

**Han et al., 2021**
Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Yuan Yao, Ao Zhang, Liang Zhang, et al. 2021. Pre-trained models: Past, present and future. *AI Open*, 2:225–250.
- Cited in 01_introduction.md as evidence of LLM performance across NLP tasks.

**Han et al., 2023**
Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. 2023. Lm-infinite: Simple on-the-fly length generalization for large language models. *ArXiv*, abs/2308.16137.
- Cited in 02_related-work.md as an attention variant for handling infinitely long sequences.

**Hong et al., 2023**
Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Yuhan Dong, and Yu Wang. 2023. Flashdecoding++: Faster large language model inference on gpus.
- Cited in 02_related-work.md as enhanced parallelization in decoding.

**Huang et al., 2021**
Luyang Robby Huang, Shuyang Cao, Nikolaus Nova Parulian, Heng Ji, and Lu Wang. 2021. Efficient attentions for long document summarization. *ArXiv*, abs/2104.02112.
- Cited in 02_related-work.md as a pre-existing task aggregated into benchmarks.

**Huang et al., 2023**
Yunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan Yang, Zhou Xin, and Xiaoxing Ma. 2023. Advancing transformer architecture in long-context large language models: A comprehensive survey. *arXiv preprint arXiv:2311.12351*.
- Cited in 01_introduction.md on the importance of long-context processing.

**Jiang et al., 2023**
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. 2023. Mistral 7b.
- Cited in 04_experiments.md as the base model for YaRN-Mistral.

**Joshi et al., 2017**
Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. *ArXiv*, abs/1705.03551.
- Cited in 02_related-work.md as a pre-existing task aggregated into benchmarks.

**Kociskỳ et al., 2017**
Tomas Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. 2017. The narrativeqa reading comprehension challenge. *Transactions of the Association for Computational Linguistics*, 6:317–328.
- Cited in 02_related-work.md as a pre-existing task aggregated into benchmarks.

**Kociskỳ et al., 2018**
Tomas Kociskỳ, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. 2018. The narrativeqa reading comprehension challenge. *Transactions of the Association for Computational Linguistics*, 6:317–328.
- Cited in 01_introduction.md as an example of document information extraction task.

**Kwon et al., 2023**
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In *Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles*.
- Cited in 02_related-work.md as a memory optimization approach.

**Li et al., 2023**
Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023. Loogle: Can long-context language models understand long contexts? *ArXiv*, abs/2311.04939.
- Cited in 02_related-work.md as the LooGLE benchmark.

**Lin, 2004**
Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In *Text Summarization Branches Out*, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.
- Cited in 03_infinitebench.md as the evaluation metric for En.Sum.

**Liu et al., 2023**
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts.
- Cited in 01_introduction.md, 03_infinitebench.md (Retrieve.KV task origin), and 05_analysis.md (lost-in-the-middle phenomenon comparison).

**Mohtashami and Jaggi, 2023**
Amirkeivan Mohtashami and Martin Jaggi. 2023. Landmark attention: Random-access infinite context length for transformers. *ArXiv*, abs/2305.16300.
- Cited in 03_infinitebench.md as the origin of the Retrieve.PassKey task.

**Nakano et al., 2021**
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted question-answering with human feedback. *arXiv preprint arXiv:2112.09332*.
- Cited in 01_introduction.md as an example of LLM-based web navigation.

**Narayanan et al., 2021**
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. 2021. Efficient large-scale language model training on gpu clusters using megatron-lm. In *Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis*, pages 1–15.
- Cited in 01_introduction.md as advancement in training infrastructure.

**OpenAI, 2023a**
OpenAI. 2023a. Gpt-4 technical report. *ArXiv*, abs/2303.08774.
- Cited in 01_introduction.md as a foundational LLM reference.

**OpenAI, 2023b**
OpenAI. 2023b. Gpt-4 turbo.
- Cited in 01_introduction.md, 02_related-work.md, and 04_experiments.md as the GPT-4 baseline supporting 128K contexts.

**OpenAI, 2023c**
OpenAI. 2023c. Tiktoken.
- Cited in 03_infinitebench.md as the tokenizer used for Code.Debug filtering.

**Penedo et al., 2023**
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. *arXiv preprint arXiv:2306.01116*.
- Cited in 01_introduction.md as evidence that LLMs are typically trained on sequences up to 8K tokens.

**Peng et al., 2023b**
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023b. Yarn: Efficient context window extension of large language models.
- Cited in 01_introduction.md, 02_related-work.md, and 04_experiments.md as the YaRN method for extending context and as a baseline model.

**Press et al., 2021**
Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input length extrapolation. *arXiv preprint arXiv:2108.12409*.
- Cited in 02_related-work.md as a positional encoding approach for longer sequences.

**Qiu et al., 2020**
Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. 2020. Pre-trained models for natural language processing: A survey. *Science China Technological Sciences*, 63(10):1872–1897.
- Cited in 01_introduction.md on LLM performance across NLP tasks.

**Shazeer, 2019**
Noam M. Shazeer. 2019. Fast transformer decoding: One write-head is all you need. *ArXiv*, abs/1911.02150.
- Cited in 02_related-work.md as a memory optimization approach.

**Shoeybi et al., 2019**
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. *ArXiv*, abs/1909.08053.
- Cited in 01_introduction.md as advancement in training infrastructure.

**Su et al., 2023**
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2023. Roformer: Enhanced transformer with rotary position embedding. *Neurocomputing*, page 127063.
- Cited in 02_related-work.md as the rotary positional encoding being modified for longer contexts.

**Sun et al., 2022**
Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. 2022. A length-extrapolatable transformer. *arXiv preprint arXiv:2212.10554*.
- Cited in 02_related-work.md as a positional encoding approach for longer sequences.

**Tay et al., 2020**
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020. Long range arena: A benchmark for efficient transformers.
- Cited in 01_introduction.md and 02_related-work.md as the LRA benchmark (~10K tokens).

**Touvron et al., 2023**
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*.
- Cited in 01_introduction.md as a foundational LLM reference and evidence of 8K training sequence limit.

**Xiao et al., 2023**
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023. Efficient streaming language models with attention sinks. *ArXiv*, abs/2309.17453.
- Cited in 02_related-work.md as StreamingLLM, an attention variant for infinite-length sequences.

**Yang et al., 2018**
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In *Conference on Empirical Methods in Natural Language Processing*.
- Cited in 02_related-work.md as a pre-existing task aggregated into benchmarks.

**Zhu et al., 2023**
Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. 2023. Pose: Efficient context window extension of llms via positional skip-wise training. *arXiv preprint arXiv:2309.10400*.
- Cited in 02_related-work.md as a post-training adjustment for longer sequences.
