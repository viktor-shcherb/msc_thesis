# 1.5 Related Work [p. 6-7]

## Parameter Estimation

[p. 6] It is interesting to compare using a Markov model for prediction with methods that attempt to *properly* learn an underlying model. Method of moments algorithms [26, 27] allow one to estimate a certain class of Hidden Markov Models with polynomial sample and computational complexity. These ideas have been extended to learning neural networks [28] and input-output RNNs [29]. Using different methods, Arora et al. [30] showed how to learn certain random deep neural networks. Learning the model directly can result in better sample efficiency and provide insights into the structure of the data. The major drawback of these approaches is that they usually require the true data-generating distribution to be in (or extremely close to) the model family that we are learning -- a very strong assumption that often does not hold in practice.

## Universal Prediction and Information Theory

[p. 7] On the other end of the spectrum is the class of no-regret online learning methods which assume that the data generating distribution can even be adversarial [31]. However, the nature of these results is fundamentally different: whereas the present paper compares to the perfect model that can look at the infinite past, online learning methods typically compare to a fixed set of experts, which is much weaker. Information-theoretic tools have also been employed in the online learning literature to show near-optimality of Thompson sampling with respect to a fixed set of experts in the context of online learning with prior information [32]. Proposition 1 can be thought of as an analogous statement about the strong performance of Markov models with respect to the optimal predictions in the context of sequential prediction.

There is much work on sequential prediction based on KL-error from the information theory and statistics communities. The philosophy of these approaches is often more adversarial, with perspectives ranging from minimum description length [33, 34] and individual sequence settings [35], where no model of the data distribution process is assumed. Regarding worst-case guarantees (where there is no data generation process) and *regret* as the notion of optimality, there is a line of work on both minimax rates and the performance of Bayesian algorithms, the latter of which has favorable guarantees in a sequential setting. Regarding minimax rates, [36] provides an exact characterization of the minimax strategy, though the applicability of this approach is often limited to settings where the number of strategies available to the learner is relatively small (i.e., the normalizing constant in [36] must exist). More generally, there has been considerable work on the regret in information-theoretic and statistical settings, such as the works in [35, 37, 38, 39, 40, 41, 42, 43].

Regarding log-loss more broadly, there is considerable work on information consistency (convergence in distribution) and minimax rates with regards to statistical estimation in parametric and non-parametric families [44, 45, 46, 47, 48, 49]. In some of these settings, e.g. minimax risk in parametric, i.i.d. settings, there are characterizations of the regret in terms of mutual information [45].

There is also work on universal lossless data compression algorithms, such as the celebrated Lempel-Ziv algorithm [50]. The setting is rather different as it is one of coding the entire sequence (in a block setting) rather than prediction loss.

## Sequential Prediction in Practice

[p. 7] The authors' work was initiated by the desire to understand the role of memory in sequential prediction, and the belief that modeling long-range dependencies is important for complex tasks such as understanding natural language. There have been many proposed models with explicit notions of memory, including recurrent neural networks [51], Long Short-Term Memory (LSTM) networks [2, 3], attention-based models [7, 8], neural Turing machines [4], memory networks [5], differentiable neural computers [6], etc. While some of these models often fail to capture long-range dependencies (for example, in the case of LSTMs, it is not difficult to show that they forget the past exponentially quickly if they are "stable" [1]), the empirical performance in some settings is quite promising (see, e.g. [9, 10]).
