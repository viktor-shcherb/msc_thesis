# 1 Memory, Modeling, and Prediction [p. 1-2]

## Problem setting

[p. 1] The paper considers the problem of predicting the next observation $x_t$ given a sequence of past observations $x_1, x_2, \ldots, x_{t-1}$, which could have complex and long-range dependencies. This is called the *sequential prediction* problem, encountered in natural language modeling, speech synthesis, financial forecasting, and other domains with sequential or chronological elements.

The fundamental question is: *How do we consolidate and reference memories about the past in order to effectively predict the future?*

## Context and related models

[p. 1] Various algorithms have been developed for storing and referencing sequence information, including $n$-gram models and Hidden Markov Models (HMMs). Recently there has been significant interest in recurrent neural networks (RNNs) [1], which encode the past as a real vector of fixed length updated after every observation, and specific classes such as Long Short-Term Memory (LSTM) networks [2, 3]. Other recently popular models with explicit memory include neural Turing machines [4], memory networks [5], differentiable neural computers [6], and attention-based models [7, 8]. These models have been quite successful (see e.g. [9, 10]), but consistently learning long-range dependencies in settings such as natural language remains an extremely active research area.

[p. 1] Parallel efforts from the neuroscience community study how humans and animals make accurate predictions about their environment, including understanding memory "consolidation" and retrieval [11, 12, 13].

## Fundamental questions posed

[p. 1] Despite the long history of studying sequential prediction, the authors identify four fundamental questions:

- How much memory is necessary to accurately predict future observations, and what properties of the underlying sequence determine this requirement?
- Must one remember significant information about the distant past or is a short-term memory sufficient?
- What is the computational complexity of accurate prediction?
- How do answers to the above questions depend on the metric used to evaluate prediction accuracy?

The paper provides insights into the first three questions.

## Proposition 1 (main positive result)

[p. 1-2] **Proposition 1.** *Let $\mathcal{M}$ be any distribution over sequences with mutual information $I(\mathcal{M})$ between the past observations $\ldots, x_{t-2}, x_{t-1}$ and future observations $x_t, x_{t+1}, \ldots$ The best $\ell$-th order Markov model, which makes predictions based only on the most recent $\ell$ observations, predicts the distribution of the next observation with average KL error $I(\mathcal{M})/\ell$ or average $\ell_1$ error $\sqrt{I(\mathcal{M})/\ell}$, with respect to the actual conditional distribution of $x_t$ given all past observations.*

The "best" $\ell$-th order Markov model predicts $x_t$ based on $x_{t-\ell}, \ldots, x_{t-1}$, according to the conditional distribution of $x_t$ given $x_{t-\ell}, \ldots, x_{t-1}$ under the data generating distribution. If the output alphabet is of size $d$, this conditional distribution can be estimated with small error given $O(d^{\ell+1})$ sequences drawn from the distribution.

[p. 2] Without additional assumptions beyond the mutual information bound, it is necessary to observe multiple sequences because the distribution could be highly non-stationary with different behaviors at different times while still having small mutual information. For HMMs, the "best" Markov model can be learned from a single sequence (see Theorem 1).

## Intuition behind Proposition 1

[p. 2] At time $t$, the predictor either predicts accurately (and is unsurprised when $x_t$ is revealed) or predicts poorly (and $x_t$ must contain significant information about the history of the sequence, which can be leveraged in subsequent predictions of $x_{t+1}, x_{t+2}$, etc.). Every timestep with a "bad" prediction yields information about the past. Since the mutual information between history and future is bounded by $I(\mathcal{M})$, at most $I(\mathcal{M})$ consecutive bad predictions can occur; as long as the window spans these observations, prediction should be good.

## Corollary 1 (HMM specialization)

[p. 2] **Corollary 1.** *Suppose observations are generated by a Hidden Markov Model with at most $n$ hidden states. The best $\frac{\log n}{\epsilon}$-th order Markov model, which makes predictions based only on the most recent $\frac{\log n}{\epsilon}$ observations, predicts the distribution of the next observation with average KL error $\leq \epsilon$ or $\ell_1$ error $\leq \sqrt{\epsilon}$, with respect to the optimal predictor that knows the underlying HMM and has access to all past observations.*

This follows because for an HMM with $n$ hidden states, the mutual information of the generated sequence is trivially bounded by $\log n$.

## Theorem 1 (learning from a single sequence)

[p. 2] In the HMM setting, the "best" $\ell$-th order Markov model is easy to learn from a single sufficiently long sequence, and corresponds to the naive "empirical" $\ell$-th order Markov model (i.e., $(\ell+1)$-gram model). This model, given $x_{t-\ell}, x_{t-\ell+1}, \ldots, x_{t-1}$, outputs the observed (empirical) distribution of the observation that has followed this length-$\ell$ sequence.

**Theorem 1.** *Suppose observations are generated by a Hidden Markov Model with at most $n$ hidden states, and output alphabet of size $d$. For $\epsilon > 1/\log^{0.25} n$ there exists a window length $\ell = O(\frac{\log n}{\epsilon})$ and absolute constant $c$ such that for any $T \geq d^{c\ell}$, if $t \in \{1, 2, \ldots, T\}$ is chosen uniformly at random, then the expected $\ell_1$ distance between the true distribution of $x_t$ (given the entire history and knowledge of the HMM), and the distribution predicted by the naive "empirical" $\ell$-th order Markov model based on $x_0, \ldots, x_{t-1}$, is bounded by $\sqrt{\epsilon}$.*

Footnote 1: Theorem 1 does not have a guarantee on the average KL loss, as such a guarantee is not possible because KL loss can be unbounded if there are rare characters which have not been observed so far. [p. 2]

[p. 3] The window length necessary to predict well is independent of the mixing time of the HMM, and holds even if the model does not mix. The amount of data required scales exponentially in $\ell$ (i.e., $T = d^{O(\ell)}$); the lower bounds in Section 1.3 argue this exponential dependency is unavoidable.
