# 3 Definitions and Notation [p. 9-11]

[p. 9] Before proving the general Proposition 1, the authors introduce the necessary notation.

## Basic definitions

For any random variable $X$, its distribution is denoted $Pr(X)$. The mutual information between two random variables $X$ and $Y$ is defined as:

$$I(X; Y) = H(Y) - H(Y|X)$$

where $H(Y)$ is the entropy of $Y$ and $H(Y|X)$ is the conditional entropy of $Y$ given $X$. The conditional mutual information $I(X; Y|Z)$ is defined as:

$$I(X; Y|Z) = H(X|Z) - H(X|Y, Z) = \mathbb{E}_{x,y,z} \log \frac{Pr(X|Y, Z)}{Pr(X|Z)}$$

$$= \mathbb{E}_{y,z} D_{KL}(Pr(X|Y, Z) \| Pr(X|Z)),$$

where $D_{KL}(p \| q) = \sum_x p(x) \log \frac{p(x)}{q(x)}$ is the KL divergence between distributions $p$ and $q$.

[p. 10] Note that the notation slightly abuses $D_{KL}(Pr(X|Y, Z) \| Pr(X|Z))$: this should technically be $D_{KL}(Pr(X|Y=y, Z=z) \| Pr(X|Z=z))$, but the assignment is ignored when clear from context.

Mutual information obeys the following chain rule:

$$I(X_1, X_2; Y) = I(X_1; Y) + I(X_2; Y|X_1).$$

## Sequences and stationarity

[p. 10] Given a distribution over infinite sequences $\{x_t\}$ generated by some model $\mathcal{M}$ where $x_t$ is a random variable denoting the output at time $t$, the shorthand $x_i^j$ denotes the collection of random variables for the subsequence of outputs $\{x_i, \ldots, x_j\}$. The distribution of $\{x_t\}$ is *stationary* if the joint distribution of any subset of the sequence of random variables $\{x_t\}$ is invariant with respect to shifts in the time index:

$$Pr(x_{i_1}, x_{i_2}, \ldots, x_{i_n}) = Pr(x_{i_1+l}, x_{i_2+l}, \ldots, x_{i_n+l})$$

for any $l$ if the process is stationary.

## Predictors

[p. 10] The paper studies how well the output $x_t$ can be predicted by an algorithm which only looks at the past $\ell$ outputs. The predictor $\mathcal{A}_\ell$ maps a sequence of $\ell$ observations to a predicted distribution of the next observation. The predictive distribution of $\mathcal{A}_\ell$ at time $t$ is denoted $Q_{\mathcal{A}_\ell}(x_t | x_{t-\ell}^{t-1})$.

The Bayes optimal predictor using only windows of length $\ell$ is denoted $\mathcal{P}_\ell$; hence the prediction of $\mathcal{P}_\ell$ at time $t$ is $Pr(x_t | x_{t-\ell}^{t-1})$. Note that $\mathcal{P}_\ell$ is just the naive $\ell$-th order Markov predictor provided with the true distribution of the data.

The Bayes optimal predictor that has access to the entire history of the model is denoted $\mathcal{P}_\infty$; the prediction of $\mathcal{P}_\infty$ at time $t$ is $Pr(x_t | x_{-\infty}^{t-1})$.

The paper evaluates average performance of predictions of $\mathcal{A}_\ell$ and $\mathcal{P}_\ell$ with respect to $\mathcal{P}_\infty$ over a long time window $[0 : T-1]$.

## Mutual information of a model

[p. 10] For a stochastic process $\{x_t\}$ generated by some model $\mathcal{M}$, the mutual information $I(\mathcal{M})$ of the model is defined as the mutual information between the past and future, averaged over the window $[0 : T-1]$:

$$I(\mathcal{M}) = \lim_{T \to \infty} \frac{1}{T} \sum_{t=0}^{T-1} I(x_{-\infty}^{t-1}; x_t^\infty). \quad (3.1)$$

If the process $\{x_t\}$ is stationary, then $I(x_{-\infty}^{t-1}; x_t^\infty)$ is the same for all time steps hence $I(\mathcal{M}) = I(x_{-\infty}^{0}; x_0^\infty)$. If the average does not converge and hence the limit in (3.1) does not exist, then one can define $I(\mathcal{M}, [0:T-1])$ as the mutual information for the window $[0:T-1]$, and the results hold true with $I(\mathcal{M})$ replaced by $I(\mathcal{M}, [0:T-1])$.

## Loss metrics

[p. 10] The metrics considered for comparing the predictions of $\mathcal{P}_\ell$ and $\mathcal{A}_\ell$ with respect to $\mathcal{P}_\infty$ are: KL-divergence, $\ell_1$ distance, and relative zero-one loss. Let $F(P, Q)$ be some measure of distance between two predictive distributions. The relative zero-one loss is defined as the difference between the zero-one loss of the optimal predictor $\mathcal{P}_\infty$ and the algorithm $\mathcal{A}_\ell$. The expected loss of any predictor $\mathcal{A}_\ell$ with respect to $\mathcal{P}_\infty$ and a loss function $F$ is defined as:

$$\delta_F^{(t)}(\mathcal{A}_\ell) = \mathbb{E}_{x_{-\infty}^{t-1}} \left[ F(Pr(x_t | x_{-\infty}^{t-1}), Q_{\mathcal{A}_\ell}(x_t | x_{t-\ell}^{t-1})) \right],$$

$$\delta_F(\mathcal{A}_\ell) = \lim_{T \to \infty} \frac{1}{T} \sum_{t=0}^{T-1} \delta_F^{(t)}(\mathcal{A}_\ell).$$

The authors also define $\hat{\delta}_F^{(t)}(\mathcal{A}_\ell)$ and $\hat{\delta}_F(\mathcal{A}_\ell)$ for the algorithm $\mathcal{A}_\ell$ in the same fashion as the error in estimating the true conditional distribution of the model $\mathcal{M}$.

---
[p. 10-11 continued]

$$\hat{\delta}_F^{(t)}(\mathcal{A}_\ell) = \mathbb{E}_{x_{t-\ell}^{t-1}} \left[ F(Pr(x_t | x_{t-\ell}^{t-1}), Q_{\mathcal{A}_\ell}(x_t | x_{t-\ell}^{t-1})) \right],$$

$$\hat{\delta}_F(\mathcal{A}_\ell) = \lim_{T \to \infty} \frac{1}{T} \sum_{t=0}^{T-1} \hat{\delta}_F^{(t)}(\mathcal{A}_\ell).$$

Here $P(x_t | x_{t-\ell}^{t-1})$ is the true conditional distribution of the model $\mathcal{M}$.
