# 1.2 Implications of Proposition 1 and Corollary 1 [p. 3-4]

## Key implication: Markov models suffice for average prediction

[p. 3-4] These results show that a Markov model -- a model that cannot capture long-range dependencies or structure of the data -- can predict accurately on *any* data-generating distribution (even those corresponding to complex models such as RNNs), provided the order of the Markov model scales with the complexity of the distribution, as parameterized by the mutual information between the past and future.

This parameterization is indifferent to whether the dependencies in the sequence are relatively short-range (as in a quickly mixing HMM) or very long-range (as in an HMM that mixes slowly or does not mix at all). Independent of the nature of these dependencies, provided the mutual information is small, accurate prediction is possible based only on the most recent few observations.

## Figure 1

**Figure 1** (p. 4): *"A depiction of a HMM on $n$ states, that repeats a given length $n$ binary sequence of outputs, and hence does not mix. Corollary 1 and Theorem 1 imply that accurate prediction is possible based only on short sequences of $O(\log n)$ observations."*

The figure shows a circular HMM with $n$ states arranged in a ring, each emitting a binary output (0 or 1). The HMM cycles deterministically through states, so it never mixes. Despite the long-range dependencies (the entire cycle length is $n$), short windows of $O(\log n)$ observations suffice for accurate average prediction.

## Implications for natural language vs. other domains

[p. 4] The authors note that at a time when increasingly complex models (RNNs, neural Turing machines) are in vogue, these results serve as a baseline theoretical result. They also help explain the practical success of simple Markov models such as Kneser-Ney smoothing [15, 16] for machine translation and speech recognition. Although recent recurrent neural networks have yielded empirical gains (see e.g. [9, 10]), current models still lack the ability to consistently capture long-range dependencies.

Footnote 3: One amusing example is the sci-fi short film *Sunspring* whose script was automatically generated by an LSTM. Locally, each sentence (mostly) makes sense, though there is no cohesion over longer time frames, and no overarching plot trajectory. [p. 4]

[p. 4] In some settings such as natural language, capturing long-range dependencies seems crucial for achieving human-level results. The main message of a narrative is not conveyed in any single short segment. Higher-level intelligence seems to be about the ability to judiciously decide what aspects of the observation sequence are worth remembering and updating a model of the world based on these aspects.

## Average error as a metric -- negative interpretation

[p. 4] For settings requiring long-range dependencies, Proposition 1 can be interpreted as a *negative result*: average error is not a good metric for training and evaluating models, since models such as the Markov model which are indifferent to the time scale of dependencies can still perform well under it as long as the number of dependencies is not too large. Average prediction error *is* the metric ubiquitously used in practice in NLP and elsewhere. The authors suggest a different metric might be essential to driving progress towards systems that capture long-range dependencies and leverage memory in meaningful ways. This is discussed further in Section 1.4.

## Positive interpretation for other settings

[p. 4] For many settings such as financial prediction and lower level language prediction tasks such as OCR, average prediction error *is* a meaningful metric. For these settings, the result of Proposition 1 is extremely positive: no matter the nature of the dependencies in the financial markets, it is sufficient to learn a Markov model. As one obtains more data, one can learn a higher and higher order Markov model, and average prediction accuracy should continue to improve.

## Computational question

[p. 4] For these applications, the question becomes computational: the naive approach to learning an $\ell$-th order Markov model in a domain with an alphabet of size $d$ might require $\Omega(d^\ell)$ space to store, and data to learn. *From a computational standpoint, is there a better algorithm? What properties of the underlying sequence imply that such models can be learned, or approximated more efficiently or with less data?*
