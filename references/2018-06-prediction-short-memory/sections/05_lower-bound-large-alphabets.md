# 5 Lower Bound for Large Alphabets [p. 13-16]

[p. 13] The lower bounds for the sample complexity in the large alphabet case leverage a class of Constraint Satisfaction Problems (CSPs) with high *complexity*.

## Background on CSPs

[p. 13] A class of (Boolean) $k$-CSPs is defined via a predicate -- a function $P : \{0, 1\}^k \to \{0, 1\}$. An instance of such a $k$-CSP on $n$ variables $\{x_1, \ldots, x_n\}$ is a collection of sets (clauses) of size $k$ whose $k$ elements consist of $k$ variables or their negations. Such an instance is *satisfiable* if there exists an assignment to the variables $x_1, \ldots, x_n$ such that the predicate $P$ evaluates to 1 for every clause. More generally, the *value* of an instance is the maximum, over all $2^n$ assignments, of the ratio of number of satisfied clauses to the total number of clauses.

The lower bounds are based on the presumed hardness of distinguishing *random* instances of a certain class of CSP, versus instances of the CSP with *high value*. There has been much work attempting to characterize the difficulty of CSPs -- one notion the authors leverage is the *complexity* of a class of CSPs, first defined in Feldman et al. [18] and studied in Allen et al. [19] and Kothari et al. [20].

### Definition 1 (Complexity of a class of $k$-CSPs)

[p. 13] **Definition 1.** The *complexity* of a class of $k$-CSPs defined by predicate $P : \{0, 1\}^k \to \{0, 1\}$ is the largest $r$ such that there exists a distribution supported on the support of $P$ that is $(r-1)$-wise independent (i.e. "uniform"), and no such $r$-wise independent distribution exists.

### Example 1

[p. 13] Both $k$-XOR and $k$-SAT are well-studied classes of $k$-CSPs, corresponding, respectively, to the predicates $P_{XOR}$ that is the XOR of the $k$ Boolean inputs, and $P_{SAT}$ that is the OR of the inputs. These predicates both support $(k-1)$-wise uniform distributions, but not $k$-wise uniform distributions, hence their complexity is $k$. In the case of $k$-XOR, the uniform distribution over $\{0,1\}^k$ restricted to the support of $P_{XOR}$ is $(k-1)$-wise uniform. The same distribution is also supported by $k$-SAT.

## Random vs. planted instances

[p. 13] A *random* instance of a CSP with predicate $P$ is an instance such that all the clauses are chosen uniformly at random (by selecting the $k$ variables uniformly, and independently negating each variable with probability $1/2$). A random instance will have value close to $\mathbb{E}[P]$, where $\mathbb{E}[P]$ is the expectation of $P$ under the uniform distribution.

A *planted* instance is generated by first fixing a satisfying assignment $\boldsymbol{\sigma}$ and then sampling clauses that are satisfied, by uniformly choosing $k$ variables, and picking their negations according to a $(r-1)$-wise independent distribution associated with the predicate. Hence a planted instance always has value 1.

A *noisy planted* instance with planted assignment $\boldsymbol{\sigma}$ and noise level $\eta$ is generated by sampling consistent clauses (as above) with probability $1 - \eta$ and random clauses with probability $\eta$, hence with high probability it has value close to $1 - \eta + \eta\mathbb{E}[P]$. The hardness results are based on distinguishing whether a CSP instance is random versus has a high value (value close to $1 - \eta + \eta\mathbb{E}[P]$).

[p. 13] As expected, the difficulty of distinguishing random instances from noisy planted instances decreases as the number of sampled clauses grows. The following conjecture of Feldman et al. [18] asserts a sharp boundary on the number of clauses, below which this problem becomes computationally intractable, while remaining information theoretically easy.

### Conjectured CSP Hardness [Conjecture 1]

[p. 14] **Conjectured CSP Hardness [Conjecture 1]** [18]: *Let $Q$ be any distribution over $k$-clauses and $n$ variables of complexity $r$ and $0 < \eta < 1$. Any polynomial-time (randomized) algorithm that, given access to a distribution $D$ that equals either the uniform distribution over $k$-clauses $U_k$ or a (noisy) planted distribution $Q_{\boldsymbol{\sigma}}^\eta = (1 - \eta)Q_{\boldsymbol{\sigma}} + \eta U_k$ for some $\boldsymbol{\sigma} \in \{0,1\}^n$ and planted distribution $Q_{\boldsymbol{\sigma}}$, decides correctly whether $D = Q_{\boldsymbol{\sigma}}^\eta$ or $D = U_k$ with probability at least $2/3$ needs $\tilde{\Omega}(n^{r/2})$ clauses.*

[p. 14] Feldman et al. [18] proved the conjecture for the class of *statistical algorithms*.$^4$ Recently, Kothari et al. [20] showed that the natural Sum-of-Squares (SOS) approach requires $\tilde{\Omega}(n^{r/2})$ clauses to refute random instances of a CSP with complexity $r$, hence proving Conjecture 1 for refutation. Note that $\tilde{\Omega}(n^{r/2})$ is tight, as Allen et al. [19] give a SOS algorithm for refuting random CSPs beyond this regime. Other recent papers such as Daniely and Shalev-Shwartz [53] and Daniely [54] have also used presumed hardness of strongly refuting random $k$-SAT and random $k$-XOR instances with a small number of clauses to derive conditional hardness for various learning problems.

Footnote 4: Statistical algorithms are an extension of the statistical query model. These are algorithms that do not directly access samples from the distribution but instead have access to estimates of the expectation of any bounded function of a sample, through a "statistical oracle". Feldman et al. [52] point out that almost all algorithms that work on random data also work with this limited access to samples; refer to Feldman et al. [52] for more details and examples. [p. 14]

## Encoding a $k$-CSP as a sequential model

[p. 14] A first attempt to encode a $k$-CSP as a sequential model is to construct a model which outputs $k$ randomly chosen literals for the first $k$ time steps $0$ to $k-1$, and then their (noisy) predicate value for the final time step $k$. Clauses from the CSP correspond to samples from the model, and the algorithm would need to solve the CSP to predict the final time step $k$. However, as all the outputs up to the final time step are random, the trivial prediction algorithm that guesses randomly and does not try to predict the output at time $k$ would be near optimal.

To get strong lower bounds, the authors output $m > 1$ functions of the $k$ literals after $k$ time steps, while still ensuring that all the functions remain collectively hard to invert without a large number of samples.

[p. 14] Elementary results from the theory of error correcting codes are used to achieve this, proving hardness due to a reduction from a specific family of CSPs to which Conjecture 1 applies. By choosing $k$ and $m$ carefully, the near-optimal dependence on the mutual information and error $\epsilon$ -- matching the upper bounds implied by Proposition 1 -- is obtained. A short outline of the argument is provided, followed by the detailed proof in the appendix.

## 5.1 Sketch of Lower Bound Construction

[p. 14-15] A sequential model $\mathcal{M}$ is constructed such that making good predictions on the model requires distinguishing random instances of a $k$-CSP $\mathcal{C}$ on $n$ variables from instances of $\mathcal{C}$ with a high value.

### Alphabet and variable encoding

The output alphabet of $\mathcal{M}$ is $\{a_i\}$ of size $2n$. A mapping is chosen from the $2n$ characters $\{a_i\}$ to the $n$ variables $\{x_i\}$ and their $n$ negations $\{\bar{x}_i\}$. For any clause $C$ and planted assignment $\boldsymbol{\sigma}$, let $\boldsymbol{\sigma}(C)$ be the $k$-bit string of values assigned by $\boldsymbol{\sigma}$ to literals in $C$.

### Model output structure

The model $\mathcal{M}$ will output $k$ characters from time $0$ to $k-1$ chosen uniformly at random, which correspond to literals in the CSP $\mathcal{C}$; hence the $k$ outputs correspond to a clause $C$ of the CSP.

For some $m$ (to be specified later) a binary matrix $\mathbf{A} \in \{0, 1\}^{m \times k}$ is constructed, which corresponds to a good error-correcting code. For time steps $k$ to $k + m - 1$, with probability $1 - \eta$ the model outputs $\mathbf{y} \in \{0, 1\}^m$ where $\mathbf{y} = \mathbf{A}\mathbf{v} \mod 2$ and $\mathbf{v} = \boldsymbol{\sigma}(C)$ with $C$ being the clause associated with the outputs of the first $k$ time steps. With the remaining probability $\eta$, the model outputs $m$ uniformly random bits.

Note that the mutual information $I(\mathcal{M})$ is at most $m$ as only the outputs from time $k$ to $k + m - 1$ can be predicted.

### HMM simulation

[p. 15] The authors claim that $\mathcal{M}$ can be simulated by an HMM with $2^m(2k + m) + m$ hidden states. For every time step from $0$ to $k-1$ there will be $2^{m+1}$ hidden states, for a total of $k2^{m+1}$ hidden states. Each of these hidden states has two labels: the current value of the $m$ bits of $\mathbf{y}$, and an "output label" of 0 or 1 corresponding to the output at that time step having an assignment of 0 or 1 under the planted assignment $\boldsymbol{\sigma}$.

The output distribution for each of these hidden states is either of the following: if the state has an "output label" 0 then it is uniform over all the characters which have an assignment of 0 under the planted assignment $\boldsymbol{\sigma}$, similarly if the state has an "output label" 1 then it is uniform over all the characters which have an assignment of 1 under the planted assignment $\boldsymbol{\sigma}$.

The transition matrix for the first $k$ time steps simply connects a state $h_1$ at the $(i-1)$th time step to a state $h_2$ at the $i$th time step if the value of $\mathbf{y}$ corresponding to $h_1$ should be updated to the value of $\mathbf{y}$ corresponding to $h_2$ if the output at the $i$th time step corresponds to the "output label" of $h_2$.

For time steps $k$ through $(k + m - 1)$, there are $2^m$ hidden states for each time step, each corresponding to a particular choice of $\mathbf{y}$. The output of a hidden state corresponding to the $(k+i)$th time step with a particular label $\mathbf{y}$ is simply the $i$th bit of $\mathbf{y}$.

Finally, an additional $m$ hidden states are needed to output $m$ uniform random bits from time $k$ to $(k + m - 1)$ with probability $\eta$. This accounts for a total of $k2^{m+1} + m2^m + m$ hidden states.

After $k + m$ time steps the HMM transitions back to one of the starting states at time 0 and repeats. The larger $m$ is with respect to $k$, the higher the cost (in terms of average prediction error) of failing to correctly predict the outputs from time $k$ to $(k + m - 1)$. Tuning $k$ and $m$ allows controlling the number of hidden states and average error incurred by a computationally constrained predictor.

### CSP definition from the model

[p. 15] The CSP $\mathcal{C}$ is defined in terms of a collection of predicates $P(\mathbf{y})$ for each $\mathbf{y} \in \{0, 1\}^m$. While Conjecture 1 does not directly apply to $\mathcal{C}$ (as it is defined by a collection of predicates instead of a single one), a reduction from a related CSP $\mathcal{C}_0$ defined by a single predicate is later shown, for which Conjecture 1 holds.

For each $\mathbf{y}$, the predicate $P(\mathbf{y})$ of $\mathcal{C}$ is the set of all $\mathbf{v} \in \{0, 1\}^k$ which satisfy $\mathbf{y} = \mathbf{A}\mathbf{v} \mod 2$. Hence each clause has an additional label $\mathbf{y}$ which determines the satisfying assignments, and this label is just the output of the sequential model $\mathcal{M}$ from time $k$ to $k + m - 1$.

For any planted assignment $\boldsymbol{\sigma}$, the set of satisfying clauses $C$ of the CSP $\mathcal{C}$ are all clauses such that $\mathbf{A}\mathbf{v} = \mathbf{y} \mod 2$ where $\mathbf{y}$ is the label of the clause and $\mathbf{v} = \boldsymbol{\sigma}(C)$.

A (noisy) planted distribution over clauses $Q_{\boldsymbol{\sigma}}^\eta$ is defined by first uniformly randomly sampling a label $\mathbf{y}$, and then sampling a consistent clause with probability $(1 - \eta)$, otherwise with probability $\eta$ sampling a uniformly random clause. Let $U_k$ be the uniform distribution over all $k$-clauses with uniformly chosen labels $\mathbf{y}$.

[p. 15-16] The authors show that Conjecture 1 implies that distinguishing between the distributions $Q_{\boldsymbol{\sigma}}^\eta$ and $U_k$ is hard without sufficiently many clauses. This gives the hardness results desired for the sequential model $\mathcal{M}$: if an algorithm obtains low prediction error on the outputs from time $k$ through $(k + m - 1)$, then it can be used to distinguish between instances of the CSP $\mathcal{C}$ with a high value and random instances, as no algorithm obtains low prediction error on random instances. Hence hardness of strongly refuting the CSP $\mathcal{C}$ implies hardness of making good predictions on $\mathcal{M}$.

### Reduction from $\mathcal{C}_0$ to $\mathcal{C}$

[p. 16] The argument for why Conjecture 1 implies the hardness of strongly refuting the CSP $\mathcal{C}$ proceeds by defining another CSP $\mathcal{C}_0$ which reduces to $\mathcal{C}$. The predicate $P$ of the CSP $\mathcal{C}_0$ is the set of all $\mathbf{v} \in \{0, 1\}^k$ such that $\mathbf{A}\mathbf{v} = 0 \mod 2$. Hence for any planted assignment $\boldsymbol{\sigma}$, the set of satisfying clauses of $\mathcal{C}_0$ are all clauses such that $\mathbf{v} = \boldsymbol{\sigma}(C)$ is in the nullspace of $\mathbf{A}$.

As before, the planted distribution over clauses is uniform on all satisfying clauses with probability $(1-\eta)$, with probability $\eta$ a uniformly random $k$-clause is added. For some $\gamma \geq 1/10$, if a matrix $\mathbf{A}$ can be constructed such that the set of satisfying assignments $\mathbf{v}$ (which are the vectors in the nullspace of $\mathbf{A}$) supports a $(\gamma k - 1)$-wise uniform distribution, then by Conjecture 1 any polynomial time algorithm cannot distinguish between the planted distribution and uniformly randomly chosen clauses with less than $\tilde{\Omega}(n^{\gamma k/2})$ clauses.

Choosing a matrix $\mathbf{A}$ whose null space is $(\gamma k - 1)$-wise uniform corresponds to finding a binary linear code with rate at least $1/2$ and relative distance $\gamma$, the existence of which is guaranteed by the Gilbert-Varshamov bound.

### Reduction details

[p. 16] The key idea is that the CSPs $\mathcal{C}_0$ and $\mathcal{C}$ are defined by linear equations. If a clause $C = (x_1, x_2, \ldots, x_k)$ in $\mathcal{C}_0$ is satisfied with some assignment $\mathbf{t} \in \{0, 1\}^k$ to the variables in the clause then $\mathbf{A}\mathbf{t} = 0 \mod 2$. Therefore, for some $\mathbf{w} \in \{0, 1\}^k$ such that $\mathbf{A}\mathbf{w} = \mathbf{y} \mod 2$, $\mathbf{t} + \mathbf{w}$ satisfies $\mathbf{A}(\mathbf{t} + \mathbf{w}) = \mathbf{y} \mod 2$. A clause $C' = (x_1', x_2', \ldots, x_k')$ with assignment $\mathbf{t} + \mathbf{w}$ to the variables can be obtained from clause $C$ by switching the literal $x_i' = \bar{x}_i$ if $\mathbf{w}_i = 1$ and retaining $x_i' = x_i$ if $\mathbf{w}_i = 0$.

Hence for any label $\mathbf{y}$, one can efficiently convert a clause $C$ in $\mathcal{C}_0$ to a clause $C'$ in $\mathcal{C}$ which has the desired label $\mathbf{y}$ and is only satisfied with a particular assignment to the variables if $C$ in $\mathcal{C}_0$ is satisfied with the same assignment to the variables. It is also ensured that consistent clauses in $\mathcal{C}$ are uniformly sampled if the original clause $C$ was a uniformly sampled consistent clause in $\mathcal{C}_0$.

### Small example

[p. 16] A small example is provided to illustrate the sequential model construction. Let $k = 3$, $m = 1$ and $n = 3$. Let $\mathbf{A} \in \{0, 1\}^{1 \times 3}$. The output alphabet of the model $\mathcal{M}$ is $\{a_i, 1 \leq i \leq 6\}$. The letter $a_1$ maps to the variable $x_1$, $a_2$ maps to $\bar{x}_1$, similarly $a_3 \to x_2$, $a_4 \to \bar{x}_2$, $a_5 \to x_3$, $a_6 \to \bar{x}_3$. Let $\boldsymbol{\sigma}$ be some planted assignment to $\{x_1, x_2, x_3\}$, which defines a particular model $\mathcal{M}$.

---
[p. 16 continued]

If the output of the model $\mathcal{M}$ is $a_1, a_3, a_6$ for the first three time steps, then this corresponds to the clause with literals $(x_1, x_2, \bar{x}_3)$. For the final time step, with probability $(1 - \eta)$ the model outputs $y = \mathbf{A}\mathbf{v} \mod 2$ for the clause $C = (x_1, x_2, \bar{x}_3)$ and planted assignment $\boldsymbol{\sigma}$, and with probability $\eta$ it outputs a uniform random bit. For an algorithm to make a good prediction at the final time step, it needs to be able to distinguish if the output at the final time step is always a random bit or if it is dependent on the clause, hence it needs to distinguish random instances of the CSP from planted instances.

## Theorem 2 (restated)

[p. 16] The full Theorem 2 is re-stated in terms of the notation defined in Section 5, deferring its full proof to Appendix B.

**Theorem 2.** *Assuming Conjecture 1, for all sufficiently large $T$ and $1/T^c < \epsilon \leq 0.1$ for some fixed constant $c$, there exists a family of HMMs with $T$ hidden states and an output alphabet of size $n$ such that, any prediction algorithm that achieves average KL-error, $\ell_1$ error or relative zero-one error less than $\epsilon$ with probability greater than $2/3$ for a randomly chosen HMM in the family, and runs in time $f(T, \epsilon) \cdot n^{g(T,\epsilon)}$ for any functions $f$ and $g$, requires $n^{\Omega(\log T / \epsilon)}$ samples from the HMM.*
