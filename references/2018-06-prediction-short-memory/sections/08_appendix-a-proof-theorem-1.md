# A Proof of Theorem 1 [p. 18-21]

[p. 18] **Theorem 1** (restated). *Suppose observations are generated by a Hidden Markov Model with at most $n$ hidden states, and output alphabet of size $d$. For $\epsilon > 1/\log^{0.25} n$ there exists a window length $\ell = O(\frac{\log n}{\epsilon})$ and absolute constant $c$ such that for any $T \geq d^{c\ell}$, if $t \in \{1, 2, \ldots, T\}$ is chosen uniformly at random, then the expected $\ell_1$ distance between the true distribution of $x_t$ given the entire history (and knowledge of the HMM), and the distribution predicted by the naive "empirical" $\ell$-th order Markov model based on $x_0, \ldots, x_{t-1}$, is bounded by $\sqrt{\epsilon}$.*

## Proof setup

[p. 18] Let $\pi_t$ be a distribution over hidden states such that the probability of the $i$th hidden state under $\pi_t$ is the empirical frequency of the $i$th hidden state from time 1 to $t - 1$ normalized by $(t - 1)$. For $0 \leq s \leq \ell - 1$, consider the predictor $\mathcal{P}_t$ which makes a prediction for the distribution of observation $x_{t+s}$ given observations $x_t, \ldots, x_{t+s-1}$ based on the true distribution of $x_t$ under the HMM, conditioned on the observations $x_t, \ldots, x_{t+s-1}$ and the distribution of the hidden state at time $t$ being $\pi_t$.

The proof shows that in expectation over $t$, $\mathcal{P}_t$ gets small error averaged across the time steps $0 \leq s \leq \ell - 1$, with respect to the optimal prediction of $x_{t+s}$ with knowledge of the true hidden state $h_t$ at time $t$. To show this, it is first established that the true hidden state $h_t$ at time $t$ does not have very small probability under $\pi_t$, with high probability over the choice of $t$.

## Lemma 4

[p. 18] **Lemma 4.** *With probability $1 - 2/n$ over the choice of $t \in \{1, \ldots, T\}$, the hidden state $h_t$ at time $t$ has probability at least $1/n^3$ under $\pi_t$.*

### Proof of Lemma 4

[p. 18-19] Consider the ordered set $\mathcal{S}_i$ of time indices $t$ where the hidden state $h_t = i$, sorted in increasing order. First, picking a time step $t$ where the hidden state $h_t$ is a state $j$ which occurs rarely in the sequence is not very likely. For sets corresponding to hidden states $j$ which have probability less than $1/n^2$ under $\pi_T$, the cardinality $|\mathcal{S}_j| \leq T/n^2$. The sum of the cardinality of all such small sets is at most $T/n$, and hence the probability that a uniformly random $t \in \{1, \ldots, T\}$ lies in one of these sets is at most $1/n$.

[p. 19] Now consider the set of time indices $\mathcal{S}_i$ corresponding to some hidden state $i$ which has probability at least $1/n^2$ under $\pi_T$. For all $t$ which are not among the first $T/n^3$ time indices in this set, the hidden state $i$ has probability at least $1/n^3$ under $\pi_t$. The first $T/n^3$ time indices in any set $S_i$ are referred to as the "bad" time steps for hidden state $i$. The fraction of the "bad" time steps corresponding to any hidden state which has probability at least $1/n^2$ under $\pi_T$ is at most $1/n$, and hence the total fraction of these "bad" time steps across all hidden states is at most $1/n$.

Therefore using a union bound, with failure probability $2/n$, the hidden state $h_t$ at time $t$ has probability at least $1/n^3$ under $\pi_t$. $\square$

## Key definitions for main proof

[p. 19] Consider any time index $t$, for simplicity assume $t = 0$, and let $OPT_s$ denote the conditional distribution of $x_s$ given observations $x_0, \ldots, x_{s-1}$, and knowledge of the hidden state at time $s = 0$. Let $M_s$ denote the conditional distribution of $x_s$ given only $x_0, \ldots, x_{s-1}$, given that the hidden state at time 0 has the distribution $\pi_0$.

## Lemma 1 (restated with context)

[p. 19] **Lemma 1.** *For $\epsilon > 1/n$, if the true hidden state at time 0 has probability at least $1/n^c$ under $\pi_0$, then for $\ell = c \log n / \epsilon^2$,*

$$\mathbb{E}\left[\frac{1}{\ell} \sum_{s=0}^{\ell-1} \|OPT_s - M_s\|_1\right] \leq 4\epsilon,$$

*where the expectation is with respect to the randomness in the outputs from time 0 to $\ell - 1$.*

### Application of Lemma 1 and Lemma 4

[p. 19] By Lemma 4, for a randomly chosen $t \in \{1, \ldots, T\}$ the probability that the hidden state $i$ at time 0 has probability less than $1/n^3$ in the prior distribution $\pi_t$ is at most $2/n$. As the $\ell_1$ error at any time step can be at most 2, using Lemma 1, the expected average error of the predictor $\mathcal{P}_t$ across all $t$ is at most $4\epsilon + 4/n \leq 8\epsilon$ for $\ell = 3 \log n / \epsilon^2$.

## Empirical predictor analysis

[p. 19] Now consider the predictor $\hat{\mathcal{P}}_t$ which for $0 \leq s \leq \ell - 1$ predicts $x_{t+s}$ given $x_t, \ldots, x_{t+s-1}$ according to the empirical distribution of $x_{t+s}$ given $x_t, \ldots, x_{t+s-1}$, based on the observations up to time $t$. The predictions of $\hat{\mathcal{P}}_t$ are shown to be close in expectation to the predictions of $\mathcal{P}_t$.

Recall that prediction of $\mathcal{P}_t$ at time $t + s$ is the true distribution of $x_t$ under the HMM, conditioned on the observations $x_t, \ldots, x_{t+s-1}$ and the distribution of the hidden state at time $t$ being drawn from $\pi_t$. For any $s < \ell$, let $P_1$ refer to the prediction of $\hat{\mathcal{P}}_t$ at time $t + s$ and $P_2$ refer to the prediction of $\mathcal{P}_t$ at time $t + s$.

### Martingale concentration argument

[p. 19] A martingale concentration argument is used to show that $\|P_1 - P_2\|_1$ is small in expectation over $t$. Consider any string $r$ of length $s$. Let $Q_1(r)$ be the empirical probability of the string $r$ up to time $t$ and $Q_2(r)$ be the true probability of the string $r$ given that the hidden state at time $t$ is distributed as $\pi_t$. The aim is to show that $|Q_1(r) - Q_2(r)|$ is small. Define the random variable

$$Y_\tau = Pr[[x_\tau : x_{\tau+s-1}] = r | h_\tau] - I([x_\tau : x_{\tau+s-1}] = r),$$

where $I$ denotes the indicator function and $Y_0$ is defined to be 0. It is claimed that $Z_\tau = \sum_{i=0}^{\tau} Y_i$ is a martingale with respect to the filtration $\{\phi\}, \{h_1\}, \{h_2, x_1\}, \{h_3, x_2\}, \ldots, \{h_{t+1}, x_t\}$.

[p. 19] To verify, note that:

$$\mathbb{E}[Y_\tau | \{h_1\}, \{h_2, x_1\}, \ldots, \{h_\tau, x_{\tau-1}\}] = Pr[[x_\tau : x_{\tau+s-1}] = r | h_\tau]$$
$$- E[I([x_\tau : x_{\tau+s-1}] = r) | \{h_1\}, \{h_2, x_1\}, \ldots, \{x_{\tau-1}, h_\tau\}]$$
$$= Pr[[x_\tau : x_{\tau+s-1}] = r | h_\tau] - E[I([x_\tau : x_{\tau+s-1}] = r) | h_\tau] = 0.$$

Therefore $\mathbb{E}[Z_\tau | \{h_1\}, \{h_2, x_1\}, \ldots, \{h_\tau, x_{\tau-1}\}] = Z_{\tau-1}$, and hence $Z_\tau$ is a martingale. Also, note that $|Z_\tau - Z_{\tau-1}| \leq 1$ as $0 \leq Pr[[x_\tau : x_{\tau+s-1}] = r | h_\tau] \leq 1$ and $0 \leq I([x_\tau : x_{\tau+s-1}] = r) \leq 1$.

### Azuma's inequality bound

[p. 20] Hence using Azuma's inequality (Lemma 8),

$$Pr[|Z_{t-s}| \geq K] \leq 2e^{-K^2/(2t)}.$$

Note that $Z_{t-s}/(t - s) = Q_2(r) - Q_1(r)$. By Azuma's inequality and doing a union bound over all $d^s \leq d^\ell$ strings $r$ of length $s$, for $c \geq 4$ and $t \geq T/n^2 = d^{c\ell}/n^2 \geq d^{c\ell/2}$, we have $\|Q_1 - Q_2\|_1 \leq 1/d^{c\ell/20}$ with failure probability at most $2d^\ell e^{-\sqrt{t}/2} \leq 1/n^2$.

Similarly, for all strings of length $s + 1$, the estimated probability of the string has error at most $1/d^{c\ell/20}$ with failure probability $1/n^2$. As the conditional distribution of $x_{t+s}$ given observations $x_t, \ldots, x_{t+s-1}$ is the ratio of the joint distributions of $\{x_t, \ldots, x_{t+s-1}, x_{t+s}\}$ and $\{x_t, \ldots, x_{t+s-1}\}$, therefore as long as the empirical distributions of the length $s$ and length $s + 1$ strings are estimated with error at most $1/d^{c\ell/20}$ and the string $\{x_t, \ldots, x_{t+s-1}\}$ has probability at least $1/d^{c\ell/40}$, the conditional distributions $P_1$ and $P_2$ satisfy $\|P_1 - P_2\|_1 \leq 1/n^2$.

By a union bound over all $d^s \leq d^\ell$ strings and for $c \geq 100$, the total probability mass on strings which occur with probability less than $1/d^{c\ell/40}$ is at most $1/d^{c\ell/50} \leq 1/n^2$ for $c \geq 100$. Therefore $\|P_1 - P_2\|_1 \leq 1/n^2$ with overall failure probability $3/n^2$, hence the $\ell_1$ distance between $P_1$ and $P_2$ is at most $1/n$.

## Main conclusion

[p. 20] By the triangle inequality and the fact that the expected average error of $\mathcal{P}_t$ is at most $8\epsilon$ for $\ell = 3 \log n / \epsilon^2$, it follows that the expected average error of $\hat{\mathcal{P}}_t$ is at most $8\epsilon + 1/n \leq 9\epsilon$. Note that the expected average error of $\hat{\mathcal{P}}_t$ is the average of the expected errors of the empirical $s$-th order Markov models for $0 \leq s \leq \ell - 1$. Hence for $\ell = 3 \log n / \epsilon^2$ there must exist at least some $s < \ell$ such that the $s$-th order Markov model gets expected $\ell_1$ error at most $9\epsilon$.

## A.1 Proof of Lemma 1 [p. 20-21]

[p. 20] Let the prior for the distribution of the hidden states at time 0 be $\pi_0$. Let the true hidden state $h_0$ at time 0 be 1 without loss of generality. The output at time $t$ is referred to as $x_s$. Let $H_0^s(i) = Pr[h_0 = i | x_0^s]$ be the posterior probability of the $i$th hidden state at time 0 after seeing the observations $x_0^s$ up to time $t$ and having the prior $\pi_0$ on the distribution of the hidden states at time 0. Let $u_s = H_0^s(1)$ and $v_s = 1 - u_s$.

Define $P_i^s(j) = Pr[x_s = j | x_0^{s-1}, h_0 = i]$ as the distribution of the output at time $t$ conditioned on the hidden state at time 0 being $i$ and observations $x_0^{s-1}$. Note that $OPT_s = P_1^s$.

As before, define $R_s$ as the conditional distribution of $x_s$ given observations $x_0, \ldots, x_{s-1}$ and initial distribution $\pi$ but *not* being at hidden state $h_0$ at time 0, i.e. $R_s = (1/v_s) \sum_{i=2}^{n} H_0^s(i) P_i^s$.

Note that $M_s$ is a convex combination of $OPT_s$ and $R_s$, i.e. $M_s = u_s OPT_s + v_s R_s$. Hence $\|OPT_s - M_s\|_1 \leq \|OPT_s - R_s\|_1$. Define $\delta_s = \|OPT_s - M_s\|_1$.

### Martingale construction for bounded differences

[p. 20] The proof relies on a martingale concentration argument, and in order to ensure that the martingale has bounded differences, outputs which cause a significant drop in the posterior of the true hidden state at time 0 are ignored. Let $B$ be the set of all outputs $j$ at some time $t$ such that $\frac{OPT_s(j)}{R_s(j)} \leq \frac{\epsilon^4}{c \log n}$. Note that $\sum_{j \in B} OPT_s(j) \leq \frac{\epsilon^4 \sum_{j \in B} R_s(j)}{c \log n} \leq \frac{\epsilon^4}{c \log n}$.

Hence by a union bound, with failure probability at most $\epsilon^2$ any output $j$ such that $\frac{OPT_s(j)}{R_s(j)} \leq \frac{\epsilon^4}{c \log n}$ is not emitted in a window of length $c \log n / \epsilon^2$. Hence only sequences of outputs such that the output $j$ emitted at each step satisfies $\frac{OPT_s(j)}{R_s(j)} \geq \frac{\epsilon^4}{c \log n}$ are considered; let the set of all such outputs be $\mathcal{S}_1$, note that $Pr(x_0^\ell \notin \mathcal{S}_1) \leq \epsilon^2$. Let $\mathbb{E}_{\mathcal{S}_1}[X]$ be the expectation of any random variable $X$ conditioned on the output sequence being in the set $\mathcal{S}_1$.

### Log-likelihood ratio sequence

[p. 20-21] Consider the sequence of random variables $X_s = \log u_s - \log v_s$ for $s \in [-1, \ell - 1]$. Let $X_{-1} = \log(\pi_1) - \log(1 - \pi_1)$. Let $\Delta_{s+1} = X_{s+1} - X_s$ be the change in $X_s$ on seeing the output $x_{s+1}$ at time $s + 1$. Let the output at time $s + 1$ be $j$. The posterior probabilities after seeing the $(s+1)$th output get updated according to Bayes rule:

$$H_0^{s+1}(1) = Pr[h_0 = 1 | x_0^s, x[s+1] = j]$$

---
[p. 21â€“22 continued]

### Bayesian update derivation

[p. 21] The posterior update gives:

$$H_0^{s+1}(1) = \frac{Pr[h_0 = 1 | x_0^s] Pr[x[s+1] = j | h_0 = 1, x_0^s]}{Pr[x[s+1] = j | x_0^s]}$$

$$\implies u_{s+1} = \frac{u_s OPT_{s+1}(j)}{Pr[x[s+1] = j | x_0^s]}.$$

Let $Pr[x[s+1] = j | x_0^s] = d_j$. Note that $H_0^{s+1}(i) = H_0^s(i) P_i^{s+1}(j) / d_j$ if the output at time $s + 1$ is $j$. Then:

$$R_{s+1} = \left(\sum_{i=2}^{n} H_0^s(i) P_i^{s+1}\right) / v_s$$

$$v_{s+1} = \sum_{i=2}^{n} H_0^{s+1}(i) = \left(\sum_{i=2}^{n} H_0^s(i) P_i^{s+1}(j)\right) / d_j$$

$$= v_s R_{s+1}(j) / d_j.$$

Therefore $\Delta_{s+1}$ and its expectation $\mathbb{E}[\Delta_{s+1}]$ can be written as:

$$\Delta_{s+1} = \log \frac{OPT_{s+1}(j)}{R_{s+1}(j)}$$

$$\implies \mathbb{E}[\Delta_{s+1}] = \sum_j OPT_{s+1}(j) \log \frac{OPT_{s+1}(j)}{R_{s+1}(j)} = D(OPT_{s+1} \| R_{s+1}).$$

### Truncated KL-divergence and submartingale

[p. 21] Define $\tilde{\Delta}_{s+1}$ as $\tilde{\Delta}_{s+1} := \min\{\Delta_{s+1}, \log \log n\}$ to keep martingale differences bounded. $\mathbb{E}[\tilde{\Delta}_{s+1}]$ then equals a truncated version of the KL-divergence:

**Definition 3.** For any two distributions $\mu(x)$ and $\nu(x)$, define the truncated KL-divergence as $\tilde{D}_C(\mu \| \nu) = \mathbb{E}\left[\log\left(\min\left\{\mu(x)/\nu(x), C\right\}\right)\right]$ for some fixed $C$.

The submartingale is constructed as follows. Define the sequence of random variables $\tilde{X}_s := \tilde{X}_{s-1} + \tilde{\Delta}_s$ for $t \in [0, \ell - 1]$, with $\tilde{X}_{-1} := X_{-1}$. Define $\tilde{Z}_s := \sum_{s=1}^{n} \left(\tilde{X}_s - \tilde{X}_{s-1} - \delta_s^2 / 2\right)$. Note that $\Delta_s \geq \tilde{\Delta}_s \implies X_s \geq \tilde{X}_s$.

**Lemma 5.** $\mathbb{E}_{\mathcal{S}_1}[\tilde{X}_s - \tilde{X}_{s-1}] \geq \delta_s^2 / 2$, where the expectation is with respect to the output at time $t$. Hence the sequence of random variables $\tilde{Z}_s := \sum_{i=0}^{s} \left(\tilde{X}_s - \tilde{X}_{s-1} - \delta_s^2 / 2\right)$ is a submartingale with respect to the outputs.

*Proof.* By definition $\tilde{X}_s - \tilde{X}_{s-1} = \tilde{\Delta}_s$ and $\mathbb{E}[\tilde{\Delta}_s] = \tilde{D}_C(OPT_s \| R_s)$, $C = \log n$. By taking an expectation with respect to only sequences $\mathcal{S}_1$ instead of all possible sequences, we are removing events which have a negative contribution to $\mathbb{E}[\tilde{\Delta}_s]$, hence

$$\mathbb{E}_{\mathcal{S}_1}[\tilde{\Delta}_s] \geq \mathbb{E}[\tilde{\Delta}_s] = \tilde{D}_C(OPT_s \| R_s).$$

Lemma 6 (Modified Pinsker's inequality) is then applied.

**Lemma 6.** *(Modified Pinsker's inequality)* For any two distributions $\mu(x)$ and $\nu(x)$ defined on $x \in X$, define the $C$-truncated KL divergence as $\tilde{D}_C(\mu \| \nu) = \mathbb{E}_\mu\left[\log\left(\min\left\{\frac{\mu(x)}{\nu(x)}, C\right\}\right)\right]$ for some fixed $C$ such that $\log C \geq 8$. Then $\tilde{D}_C(\mu \| \nu) \geq \frac{1}{2}\|\mu - \nu\|_1^2$.

Hence $\mathbb{E}_{\mathcal{S}_1}[\tilde{\Delta}_s] \geq \frac{1}{2}\|OPT_s - R_s\|_1^2 \geq \delta_s^2 / 2$. $\square$

### Bounded differences and concentration

[p. 22] We now claim the submartingale has bounded differences.

**Lemma 7.** $|\tilde{Z}_s - \tilde{Z}_{s-1}| \leq \sqrt{2} \log(c \log n / \epsilon^4)$.

*Proof.* Note that $(\delta_s^2 - \delta_{s-1}^2)/2$ can be at most 2. $Z_s - Z_{s-1} = \tilde{\Delta}_s$. By definition $\tilde{\Delta}_s \leq \log(\log n)$. Also, $\tilde{\Delta}_s \geq -\log(c \log n / \epsilon^4)$ as we restrict ourselves to sequences in $\mathcal{S}_1$. Hence $|\tilde{Z}_s - \tilde{Z}_{s-1}| \leq \log(c \log n / \epsilon^4) + 2 \leq \sqrt{2} \log(c \log n / \epsilon^4)$. $\square$

**Lemma 8.** *(Azuma-Hoeffding inequality)* Let $Z_i$ be a submartingale with $|Z_i - Z_{i-1}| \leq C$. Then $Pr[Z_s - Z_0 \leq -\lambda] \leq \exp\left(\frac{-\lambda^2}{2sC^2}\right)$.

Applying Lemma 8 we can show:

$$Pr[\tilde{Z}_{\ell-1} - \tilde{Z}_0 \leq -c \log n] \leq \exp\left(\frac{-c \log n}{4(1/\epsilon)^2 \log^2(c \log n / \epsilon^4)}\right) \leq \epsilon^2, \tag{A.1}$$

for $\epsilon \geq 1/\log^{0.25} n$ and $c \geq 1$.

### Bounding average error

[p. 22] Now bound the average error in the window $0$ to $\ell - 1$. With failure probability at most $\epsilon^2$ over the randomness in the outputs, $\tilde{Z}_{\ell-1} - \tilde{Z}_0 \geq -c \log n$ by Eq. A.1. Let $\mathcal{S}_2$ be the set of all sequences in $\mathcal{S}_1$ which satisfy $\tilde{Z}_{\ell-1} - \tilde{Z}_0 \geq -c \log n$. Note that $X_0 = \tilde{X}_0 \geq \log(1/\pi_1)$. Consider the last point after which $v_s$ decreases below $\epsilon^2$ and remains below for every subsequent step in the window. Let this point be $\tau$, if there is no such point define $\tau$ to be $\ell - 1$. The total contribution of the error at every step after the $\tau$th step to the average error is at most a $\epsilon^2$ term as the error after this step is at most $\epsilon^2$. Note that $X_\tau \leq \log(1/\epsilon)^2 \implies \tilde{X}_\tau \leq \log(1/\epsilon)^2$ as $\tilde{X}_s \leq X_s$. Hence for all sequences in $\mathcal{S}_2$:

$$\tilde{X}_\tau \leq \log(1/\epsilon)^2$$

$$\implies \tilde{X}_\tau - \tilde{X}_{-1} \leq \log(1/\epsilon)^2 + \log(1/\pi_1)$$

$$\stackrel{(a)}{\implies} 0.5 \sum_{s=0}^{\tau} \delta_s^2 \leq 2 \log n + \log(1/\pi_1) + c \log n$$

$$\stackrel{(b)}{\implies} 0.5 \sum_{s=0}^{\tau} \delta_s^2 \leq 2(c + 1) \log n \leq 4c \log n$$

$$\stackrel{(c)}{\implies} \frac{\sum_{s=0}^{\ell-1} \delta_s^2}{c \log n / \epsilon^2} \leq 8\epsilon^2$$

$$\stackrel{(d)}{\implies} \frac{\sum_{s=0}^{\ell-1} \delta_s}{c \log n / \epsilon^2} \leq 3\epsilon,$$

where (a) follows by Eq. A.1, and as $\epsilon \geq 1/n$; (b) follows as $\log(1/\pi_1) \leq c \log n$, and $c \geq 1$; (c) follows because $\log(1/\pi_1) \leq c \log n$; and (d) follows from Jensen's inequality. As the total probability of sequences outside $\mathcal{S}_2$ is at most $2\epsilon^2$, $\mathbb{E}[\sum_{s=0}^{\ell-1} \delta_s] \leq 4\epsilon$, whenever the hidden state $i$ at time 0 has probability at least $1/n^c$ in the prior distribution $\pi_0$. $\square$
