# References

This file contains only the references that were cited in the section notes.

## A

**Albalak et al., 2024**
Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. A survey on data selection for language models. *arXiv preprint arXiv:2402.16827*, 2024.
- Cited in 12_conclusions.md as future work for improving model performance through larger, more diverse training corpus

## B

**Beltagy et al., 2020**
Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. *arXiv preprint arXiv:2004.05150*, 2020.
- Cited in 01_introduction.md as example of sparsity techniques for sub-quadratic complexity

**Bhakthavatsalam et al., 2021**
Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, and Peter Clark. Think you have solved direct-answer question answering? try arc-da, the direct-answer ai2 reasoning challenge. *arXiv preprint arXiv:2102.03315*, 2021.
- Cited in 08_language-modeling-experiments.md as the AI2 ARC benchmark

**Bisk et al., 2020**
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In *Proceedings of the AAAI conference on artificial intelligence*, volume 34, pp. 7432–7439, 2020.
- Cited in 08_language-modeling-experiments.md as the PIQA benchmark

**Black et al., 2022**
Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-20B: An open-source autoregressive language model. In *Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models*, pp. 95–136, Dublin, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.bigscience-1.9. URL https://aclanthology.org/2022.bigscience-1.9.
- Cited in 05_rwkv-world-tokenizer.md and 18_appendix-f-tokenizer-details.md as tokenizer comparison baseline

## C

**Cho et al., 2014**
Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. *arXiv preprint arXiv:1406.1078*, 2014.
- Cited in 02_background.md as early RNN work

**Choromanski et al., 2020**
Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. In *International Conference on Learning Representations*, 2020.
- Cited in 02_background.md as linear attention work

**Conneau et al., 2018**
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. Xnli: Evaluating cross-lingual sentence representations. In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing*, pp. 2475-2485, 2018.
- Cited in 08_language-modeling-experiments.md as the XNLI multilingual benchmark

## D

**Dai et al., 2023**
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Albert Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. *ArXiv*, abs/2305.06500, 2023. URL https://api.semanticscholar.org/CorpusID:258615426.
- Cited in 10a_multimodal-continued.md (Table 6) as baseline for multimodal experiments

**Dao et al., 2022**
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. *arXiv*, 2022.
- Cited in 01_introduction.md footnote (Table 1) for Flash Attention complexity

**Dao, 2023**
Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In *The Twelfth International Conference on Learning Representations*, 2023.
- Cited in 09_speed-memory-benchmarks.md for speed comparisons

**De et al., 2024**
Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. *arXiv preprint arXiv:2402.19427*, 2024.
- Cited in 01_introduction.md, 02_background.md, and 15a_appendix-c-continued.md as concurrent work on RNN architectures

**Devlin et al., 2019**
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pp. 4171-4186, 2019.
- Cited in 18_appendix-f-tokenizer-details.md (Table 10) as tokenizer baseline

**Dong et al., 2023**
Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. Bamboo: A comprehensive benchmark for evaluating long text modeling capacities of large language models. *arXiv preprint arXiv:2309.13345*, 2023.
- Cited in 08a_language-modeling-continued.md as the Bamboo long-context benchmark

## E

**Elhage et al., 2021**
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. *Transformer Circuits Thread*, 2021. https://transformer-circuits.pub/2021/framework/index.html.
- Cited in 04_method.md for induction heads concept

## F

**Ferdinan et al., 2024**
Teddy Ferdinan, Jan Kocoń, and Przemysław Kazienko. Into the unknown: Self-learning large language models, 2024.
- Cited in 13_appendix-a-author-contributions.md for self-learning capability experiments

**Fu et al., 2022**
Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In *The Eleventh International Conference on Learning Representations*, 2022.
- Cited in 01_introduction.md and 15a_appendix-c-continued.md as H3 architecture

**Fu et al., 2023**
Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Toward language modeling with state space models, 2023.
- Cited in 01_introduction.md for gated recurrence mechanisms

## G

**Gao et al., 2023**
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836.
- Cited in 08_language-modeling-experiments.md as evaluation_harness framework

**Gemmeke et al., 2017**
Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In *2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)*, pp. 776–780. IEEE, 2017.
- Cited in 10a_multimodal-continued.md as AudioSet dataset for audio experiments

**Gu et al., 2020**
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. Hippo: Recurrent memory with optimal polynomial projections. *Advances in neural information processing systems*, 33: 1474–1487, 2020.
- Cited in 02_background.md as HiPPO work

**Gu et al., 2021**
Albert Gu, Karan Goel, , and Christopher R¨e. Efficiently modeling long sequences with structured state spaces. *arXiv2111.00396*, 2021.
- Cited in 01_introduction.md for gated recurrence mechanisms

**Gu et al., 2022**
Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces, 2022.
- Cited in 02_background.md as S4 architecture

**Gu & Dao, 2023**
Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023.
- Cited in 01_introduction.md for gated recurrence mechanisms

**Guo et al., 2022**
Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. LongT5: Efficient text-to-text transformer for long sequences. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (eds.), *Findings of the Association for Computational Linguistics: NAACL 2022*, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-naacl.55. URL https://aclanthology.org/2022.findings-naacl.55.
- Cited in 02_background.md for long sequence modeling

**Gupta et al., 2022**
Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. *Advances in Neural Information Processing Systems*, 35:22982–22994, 2022.
- Cited in 15a_appendix-c-continued.md as DSS architecture

## H

**Hampel, 1974**
Frank R Hampel. The influence curve and its role in robust estimation. *Journal of the american statistical association*, 69(346):383–393, 1974.
- Cited in 08_language-modeling-experiments.md (Table 4 caption) - NOTE: This appears to be an error; should be HellaSwag benchmark citation

**Hu et al., 2022**
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In *International Conference on Learning Representations*, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9.
- Cited in 01_introduction.md and 14_appendix-b-architecture-details.md for LoRA technique used in Finch

**Hudson & Manning, 2019**
Drew A. Hudson and Christopher D. Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In *The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pp. 6693–6702, 2019. URL https://api.semanticscholar.org/CorpusID:152282269.
- Cited in 10a_multimodal-continued.md as GQA benchmark for multimodal experiments

## J

**Jiang et al., 2023**
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023.
- Cited in 18_appendix-f-tokenizer-details.md (Table 10) as tokenizer baseline

## K

**Kaddour, 2023**
Jean Kaddour. The minipile challenge for data-efficient language models, 2023.
- Cited in 23_appendix-k-ddlerp-ablations.md as the minipile dataset for ablations

**Katharopoulos et al., 2020a**
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In *International conference on machine learning*, pp. 5156–5165. PMLR, 2020a.
- Cited in 02_background.md as linear attention work

**Katsch, 2023**
Tobias Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling, 2023.
- Cited in 01_introduction.md for gated recurrence mechanisms

**Ke et al., 2023**
Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, and Minlie Huang. Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation, 2023.
- Cited in 19_appendix-g-additional-evaluations.md as CritiqueLLM judge for AlignBench

**Kitaev et al., 2019**
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In *International Conference on Learning Representations*, 2019.
- Cited in 02_background.md for efficient transformer architectures

## L

**Li et al., 2023a**
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In *International conference on machine learning*, pp. 19730–19742. PMLR, 2023a.
- Cited in 10a_multimodal-continued.md (Table 6) as baseline for multimodal experiments

**Li et al., 2023c**
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji rong Wen. Evaluating object hallucination in large vision-language models. In *Conference on Empirical Methods in Natural Language Processing*, 2023c. URL https://api.semanticscholar.org/CorpusID:258740697.
- Cited in 10a_multimodal-continued.md as POPE benchmark for multimodal experiments

**Lin et al., 2022**
Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li. Narayan Goyal, Shruti Bhosale, Jingfei Du, et al. Few-shot learning with multilingual generative language models. In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*, pp. 9019-9052, 2022.
- Cited in 08_language-modeling-experiments.md as xStoryCloze benchmark

**Liu et al., 2023a**
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. *arXiv preprint arXiv:2310.03744*, 2023a.
- Cited in 10_multimodal-experiments.md for visual instruction tuning

**Liu et al., 2023b**
Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun, Hongning Wang, Jing Zhang, Minlie Huang, Yuxiao Dong, and Jie Tang. Alignbench: Benchmarking chinese alignment of large language models, 2023b.
- Cited in 19_appendix-g-additional-evaluations.md as AlignBench Chinese benchmark

**Lu et al., 2022**
Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and A. Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. *ArXiv*, abs/2209.09513, 2022. URL https://api.semanticscholar.org/CorpusID:252383606.
- Cited in 10a_multimodal-continued.md as ScienceQA-IMG benchmark

**Lutati et al., 2023**
Shahar Lutati, Itamar Zimerman, and Lior Wolf. Focus your attention (with adaptive iir filters), 2023.
- Cited in 15_appendix-c-additional-related-work.md for attention mechanisms

## M

**Ma et al., 2022**
Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. MEGA: Moving average equipped gated attention. *arXiv preprint arXiv:2209.10655*, 2022.
- Cited in 15a_appendix-c-continued.md as MEGA architecture

**Muennighoff et al., 2023**
Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. MTEB: Massive Text Embedding Benchmark. *arXiv preprint arXiv:2210.07316*, 2023.
- Cited in 12_conclusions.md as MTEB benchmark for embedding evaluation

## N

**Nguyen et al., 2023**
Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, et al. HyenaDNA: Long-range genomic sequence modeling at single nucleotide resolution. *arXiv preprint arXiv:2306.15794*, 2023.
- Cited in 15_appendix-c-additional-related-work.md for Hyena architecture

## O

**Orvieto et al., 2023**
Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In *Proceedings of the 40th International Conference on Machine Learning*. ICML'23. JMLR.org, 2023.
- Cited in 15a_appendix-c-continued.md for resurrecting RNNs

## P

**Paperno et al., 2016**
Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The LAMBADA dataset: Word prediction requiring a broad discourse context. *arXiv preprint arXiv:1606.06031*, 2016.
- Cited in 08_language-modeling-experiments.md as LAMBADA benchmark

**Peng et al., 2023**
Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. RWKV: Reinventing RNNs for the Transformer Era. *arXiv preprint arXiv:2305.13048*, 2023.
- Cited throughout as RWKV-4 baseline and previous work

**Ponti et al., 2020**
Edoardo M Ponti, Goran Glavaš, Olga Majewska, Qianchu Liu, Ivan Vulić, and Anna Korhonen. XCOPA: A multilingual dataset for causal commonsense reasoning. *arXiv preprint arXiv:2005.00333*, 2020.
- Cited in 08_language-modeling-experiments.md as XCOPA multilingual benchmark

## Q

**Qin et al., 2022**
Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, Yiran Zhong. cosFormer: Rethinking Softmax In Attention. *International Conference on Learning Representations*, 2022.
- Cited in 01_introduction.md and 02_background.md as TransNormer architecture

**Qin et al., 2023**
Yiran Qin, Yuanfeng Ji, Jiaoyan Chen, Kun He, Yuxiang Li, Yong Wang. HGRN2: Gated Linear RNNs with State Expansion. *arXiv preprint arXiv:2312.11897*, 2023.
- Cited in 01_introduction.md and 02_background.md as HGRN architecture

**Qin et al., 2024**
Yiran Qin, Yuanfeng Ji, Jiaoyan Chen, Kun He, Yuxiang Li, Yong Wang. HGRN: Hierarchical Gated Recurrent Neural Networks. *arXiv preprint*, 2024.
- Cited in 01_introduction.md for gated recurrence mechanisms

## R

**Radford et al., 2019**
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. *OpenAI blog*, 1(8):9, 2019.
- Cited in 05_rwkv-world-tokenizer.md and 18_appendix-f-tokenizer-details.md as GPT2 tokenizer baseline

**Radford et al., 2021**
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In *International conference on machine learning*, pp. 8748–8763. PMLR, 2021.
- Cited in 10_multimodal-experiments.md for CLIP vision encoder

**Rae et al., 2019**
Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive Transformers for Long-Range Sequence Modelling. *arXiv preprint arXiv:1911.05507*, 2019.
- Cited in 08a_language-modeling-continued.md as PG19 dataset

**Roemmele et al., 2011**
Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In *2011 AAAI Spring Symposium Series*, 2011.
- Cited in 08_language-modeling-experiments.md as COPA benchmark

## S

**Sakaguchi et al., 2021**
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. *Communications of the ACM*, 64(9):99–106, 2021.
- Cited in 08_language-modeling-experiments.md as Winogrande benchmark

**Shazeer et al., 2017**
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. *arXiv preprint arXiv:1701.06538*, 2017.
- Cited in 12_conclusions.md for Mixture of Experts future work

**Singh et al., 2019**
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards VQA models that can read. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pp. 8317–8326, 2019.
- Cited in 10a_multimodal-continued.md as Text-VQA benchmark

**Smith et al., 2023**
Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. Simplified state space layers for sequence modeling. *arXiv preprint arXiv:2208.04933*, 2023.
- Cited in 01_introduction.md for gated recurrence mechanisms

**Sun et al., 2022**
Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer. *arXiv preprint arXiv:2212.10554*, 2022.
- Cited in 15_appendix-c-additional-related-work.md for xPos positional encoding

**Sun et al., 2023**
Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models. *arXiv preprint arXiv:2307.08621*, 2023.
- Cited in 01_introduction.md and 15_appendix-c-additional-related-work.md as RetNet architecture

## T

**Tay et al., 2020**
Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. *arXiv preprint arXiv:2009.06732*, 2020.
- Cited in 01_introduction.md for sparse attention mechanisms

**Teknium, 2023**
Teknium. OpenHermes 2.5: An open dataset of synthetic data for generalist LLM assistants. https://huggingface.co/datasets/teknium/OpenHermes-2.5, 2023.
- Cited in 25_appendix-m-chat-examples-comparison.md as training data for RWKV-Eagle-Hermes-7B

**Tiezzi et al., 2024**
Matteo Tiezzi, Stefano Melacci, Alessandro Betti, Marco Maggini, Marco Gori. The return of RNNs: Residual recurrent networks for sequence modeling. *arXiv preprint*, 2024.
- Cited in 02_background.md and 15_appendix-c-additional-related-work.md for RNN revival

**Tolstikhin et al., 2021**
Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. MLP-Mixer: An all-MLP Architecture for Vision. *arXiv preprint arXiv:2105.01601*, 2021.
- Cited in 02_background.md for MLP-based architectures

**Touvron et al., 2023**
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*, 2023.
- Cited in 05_rwkv-world-tokenizer.md, 12_conclusions.md, and 18_appendix-f-tokenizer-details.md for LLaMA2 comparisons

## V

**Vaswani et al., 2023**
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. *Advances in neural information processing systems*, 30, 2017. [Note: Cited as 2023 in notes but original paper is 2017]
- Cited in 01_introduction.md as foundational transformer architecture

**Voelker et al., 2019**
Aaron Voelker, Ivana Kajić, and Chris Eliasmith. Legendre Memory Units: Continuous-time representation in recurrent neural networks. In *Advances in Neural Information Processing Systems*, pp. 15544–15553, 2019.
- Cited in 02_background.md for LMU architecture

## W

**Wang et al., 2018**
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. *arXiv preprint arXiv:1804.07461*, 2018.
- Cited in 08_language-modeling-experiments.md as GLUE benchmark

**Wang et al., 2020**
Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. *arXiv preprint arXiv:2006.04768*, 2020.
- Cited in 02_background.md for linear complexity attention

**Welbl et al., 2017**
Johannes Welbl, Nelson F Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. *arXiv preprint arXiv:1707.06209*, 2017.
- Cited in 08_language-modeling-experiments.md as SciQ benchmark

**Workshop et al., 2023**
BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. BLOOM: A 176B-parameter open-access multilingual language model. *arXiv preprint arXiv:2211.05100*, 2022.
- Cited in 05_rwkv-world-tokenizer.md and 18_appendix-f-tokenizer-details.md as BLOOM tokenizer baseline

**Wu et al., 2023**
Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, Sainbayar Sukhbaatar. TinyStories: How small can language models be and still speak coherent English? *arXiv preprint arXiv:2305.07759*, 2023.
- Cited in 19_appendix-g-additional-evaluations.md for TinyStories evaluation

## X

**Xiao et al., 2023**
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. *arXiv preprint arXiv:2309.17453*, 2023.
- Cited in 15_appendix-c-additional-related-work.md for streaming language models

**Xiong et al., 2021**
Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, Tieyan Liu. On layer normalization in the transformer architecture. In *International Conference on Machine Learning*, pp. 10524–10533. PMLR, 2021.
- Cited in 04_method.md for pre-normalization techniques

## Y

**Yang et al., 2019**
Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. PAWS-X: A cross-lingual adversarial dataset for paraphrase identification. *arXiv preprint arXiv:1908.11828*, 2019.
- Cited in 08_language-modeling-experiments.md as PAWS-X multilingual benchmark

**Yang et al., 2023**
Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim. Gated Linear Attention Transformers with Hardware-Efficient Training. *arXiv preprint arXiv:2312.06635*, 2023.
- Cited in 01_introduction.md and 02_background.md as GLA architecture

**Yuan et al., 2024**
Ke Yuan, Litian Liang, Zhengyu Wang, Qisong Zeng, and Dingwen Zhang. VRWKV: Advancing RWKV with Vision Processing. *arXiv preprint arXiv:2403.09010*, 2024.
- Cited in 10_multimodal-experiments.md and 10a_multimodal-continued.md for VRWKV architecture and Q-Shift

## Z

**Zaheer et al., 2020**
Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. In *Advances in neural information processing systems*, pp. 17283–17297, 2020.
- Cited in 01_introduction.md for sparse attention and sub-quadratic complexity

**Zhai et al., 2021**
Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, Josh Susskind. An Attention Free Transformer. *arXiv preprint arXiv:2105.14103*, 2021.
- Cited in 02_background.md as AFT (Attention Free Transformer) that influenced RWKV

**Zhang et al., 2022**
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open Pre-trained Transformer Language Models. *arXiv preprint arXiv:2205.01068*, 2022.
- Cited in 08_language-modeling-experiments.md for model comparisons

**Zhang et al., 2024**
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. TinyLlama: An open-source small language model. *arXiv preprint*, 2024.
- Cited in 19a_appendix-g-continued.md as TinyLlama baseline in self-learning evaluation

**Zheng et al., 2024**
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. *arXiv preprint arXiv:2306.05685*, 2023.
- Cited in 19_appendix-g-additional-evaluations.md for MT-Bench evaluation

## Additional References

**Ahia et al., 2023**
Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David Mortensen, Noah Smith, and Yulia Tsvetkov. Do all languages cost the same? tokenization in the era of commercial language models. *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, 2023.
- Cited in 05_rwkv-world-tokenizer.md for tokenization cost inequality

**Albalak et al., 2022**
Alon Albalak, Akshat Shrivastava, Chinnadhurai Sankar, Adithya Sagar, and Mike Ross. Data-efficiency with a single gpu: An exploration of transfer methods for small language models. *arXiv preprint arXiv:2210.03871*, 2022.
- Cited in 19a_appendix-g-continued.md for zero-shot evaluation difficulty

**Albalak et al., 2023**
Alon Albalak, Liangming Pan, Colin Raffel, and William Yang Wang. Efficient online data mixing for language model pre-training. *arXiv preprint arXiv:2312.02406*, 2023.
- Cited in 16_appendix-d-training-dataset-details.md for data mixing impact on pretraining

**Arora et al., 2023**
Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher Re. Zoology: Measuring and improving recall in efficient language models, 2023.
- Cited in 08_language-modeling-experiments.md and 08a_language-modeling-continued.md for MQAR benchmarking

**Blelloch, 1990**
Guy E. Blelloch. Prefix sums and their applications. Technical Report CMU-CS-90-190, School of Computer Science, Carnegie Mellon University, November 1990.
- Cited in 15a_appendix-c-continued.md for parallel associative scan

**Cirone et al., 2024**
Nicola Muca Cirone, Antonio Orvieto, Benjamin Walker, Cristopher Salvi, and Terry Lyons. Theoretical foundations of deep selective state-space models. *arXiv preprint arXiv:2402.19047*, 2024.
- Cited in 15a_appendix-c-continued.md for comprehensive review of recurrent models

**Hochreiter & Schmidhuber, 1997**
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. *Neural computation*, 9(8):1735-1780, 1997.
- Cited in 15_appendix-c-additional-related-work.md as LSTM architecture

**Kocon et al., 2023**
Jan Kocon et al. Chatgpt: Jack of all trades, master of none. *Information Fusion*, 99:101861, November 2023.
- Cited in 19a_appendix-g-continued.md for ChatGPT performance comparison datasets

**Lv et al., 2023**
Kaokao Lv, Liang Lv, Chang Wang, Wenxin Zhang, Xuhui Ren, and Haihao Shen. Intel-neural-chat-7b-v1-1, 2023.
- Cited in 19_appendix-g-additional-evaluations.md as neural-chat baseline in self-learning evaluation

**Martin & Cundy, 2018**
Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In *International Conference on Learning Representations*, 2018.
- Cited in 15a_appendix-c-continued.md for parallel associative scan

**Olsson et al., 2022**
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, et al. In-context learning and induction heads, 2022.
- Cited in 08_language-modeling-experiments.md for associative recall and in-context learning

**Poli et al., 2023**
Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. Hyena hierarchy: Towards larger convolutional language models. In *International Conference on Machine Learning*, pp. 28043-28078. PMLR, 2023.
- Cited in 01_introduction.md for gated convolutions and 15_appendix-c-additional-related-work.md for short convolutions

**Sanh et al., 2021**
Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, et al. Multitask prompted training enables zero-shot task generalization. *arXiv preprint arXiv:2110.08207*, 2021.
- Cited in 19a_appendix-g-continued.md for zero-shot evaluation difficulty

**Schmidhuber, 1992**
Jurgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. *Neural Computation*, 4(1):131-139, 1992.
- Cited in 02_background.md for linear attention origins

**Wang et al., 2024**
Xiao Wang, Shiao Wang, Yuhe Ding, Yuehang Li, Wentao Wu, et al. State space model for new-generation network alternative to transformers: A survey. *arXiv preprint arXiv:2404.09516*, 2024.
- Cited in 10a_multimodal-continued.md as VRWKV reference in audio section (note: PDF cites this for VRWKV but the actual VRWKV paper is Yuan et al., 2024)

**Wu et al., 2023 (music)**
Shangda Wu, Xiaobing Li, Feng Yu, and Maosong Sun. Tunesformer: Forming irish tunes with control codes by bar patching. In *CEUR Workshop Proceedings*, volume 3528, 2023.
- Cited in 10_multimodal-experiments.md as Irishman ABC music sheet dataset

**Xie et al., 2024**
Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, et al. Doremi: Optimizing data mixtures speeds up language model pretraining. *Advances in Neural Information Processing Systems*, 36, 2024.
- Cited in 16_appendix-d-training-dataset-details.md for data mixing impact
