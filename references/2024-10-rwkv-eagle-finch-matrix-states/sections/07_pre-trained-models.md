# Pre-Trained Models [p. 9]

[p. 9] We have pre-trained and publicly released the six Apache 2.0 licensed Eagle and Finch models: **Eagle 0.4B, Eagle 1.5B, Eagle 3B, Eagle 7B, Finch 1.6B, and Finch 3B**. All of the models were trained on the 1.12 trillion token RWKV World v2 multilingual corpus. See Appendix E for detailed parameter counts and FLOPs calculations.
