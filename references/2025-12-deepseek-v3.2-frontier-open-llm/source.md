# DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models

**Authors:** DeepSeek-AI (Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, et al., 263+ contributors)
**Affiliation:** DeepSeek

## Publication Status

- **arXiv preprint:** December 2025, arXiv:2512.02556
- **Peer-reviewed:** No
- **Conference/Journal:** None
- **Status:** Preprint

DeepSeek-V3.2 was released alongside open model weights. The paper introduces DeepSeek Sparse Attention (DSA) for efficient long-context processing, a scalable reinforcement learning framework, and large-scale agentic task synthesis. A high-compute variant, DeepSeek-V3.2-Speciale, achieves gold-medal performance at IMO 2025, IOI 2025, ICPC World Finals 2025, and CMO 2025.

## Preferred Citation

> DeepSeek-AI. (2025). DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models. arXiv:2512.02556.

## Links

- arXiv: https://arxiv.org/abs/2512.02556
- Model weights: https://huggingface.co/deepseek-ai/DeepSeek-V3.2
- Inference code: https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/tree/main/inference
