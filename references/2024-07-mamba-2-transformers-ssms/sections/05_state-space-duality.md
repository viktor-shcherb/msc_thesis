# 5 State Space Duality [p. 16]

[p. 16] In Sections 3 and 4, structured state space models and structured attention were defined, their properties discussed, and it was shown that they both have a quadratic algorithm and a linear algorithm. This section connects them together. The main result is showing that a particular case of structured state space models coincides with a particular case of structured attention, and that the linear-time SSM algorithm and quadratic-time kernel attention algorithm are dual forms of each other.

- Section 5.1 specializes state space models to scalar structure, where the naive quadratic computation can be seen as an instance of kernel attention.
- Section 5.2 specializes structured masked attention to semiseparable SMA, which characterizes masked attention with efficient autoregression.
- Section 5.3 summarizes the connection between structured masked attention and structured state space models, termed structured state space duality.

## 5.1 Scalar-Identity Structured State Space Models

[p. 16] Recall that SSMs are defined by $y = \text{SSM}(A, B, C)(x)$, and the matrix form of SSMs uses the SSS (sequentially semiseparable) representation $M = \text{SSS}(A, B, C)$ where $M_{j,i} = C_j^\top A_{j:i} B_i$ (equation (3)).

Now consider the case where $A_j$ is simply a scalar; in other words, an instantiation of a structured SSM where the $A$ matrices are *extremely* structured: $A = aI$ for scalar $a$ and identity matrix $I$. Then we can rearrange

$$M_{j,i} = A_{j:i} \cdot (C_j^\top B_i).$$

And this can be vectorized into

$$L \coloneqq \text{1SS}(a)$$
$$M = L \circ (CB^\top)$$

where $B, C \in \mathbb{R}^{(\mathsf{T},\mathsf{N})}$.

Using this formulation, the full output $Y = MX$ is computed precisely as

$$G = \text{contract}(\mathsf{TN}, \mathsf{SN} \to \mathsf{TS})(C, B) \quad (\mathsf{T}, \mathsf{S})$$
$$M = \text{contract}(\mathsf{TS}, \mathsf{TS} \to \mathsf{TS})(G, L) \quad (\mathsf{T}, \mathsf{S}) \tag{16}$$
$$Y = \text{contract}(\mathsf{TS}, \mathsf{SP} \to \mathsf{TP})(M, X) \quad (\mathsf{T}, \mathsf{P})$$

where $\mathsf{S} = \mathsf{T}$. But this is exactly the same as the original definition of masked kernel attention definition (13)!

Therefore, as alluded to in Section 3.4, naively computing the scalar structured SSM -- by materializing the semiseparable matrix $M$ and performing quadratic matrix-vector multiplication -- is exactly the same as quadratic masked kernel attention.

## 5.2 1-Semiseparable Structured Masked Attention

[p. 16] Structured masked attention allows for the use of any structured mask $L$. When $L$ is the causal mask, it is standard linear attention. Note that the causal mask is $L = \text{SS}(1_T)$, i.e. the 1-SS mask is generated by $a_t = 1$ in definition (6). This motivates generalizing $L$ to the class of 1-semiseparable masks, or **1-semiseparable structured masked attention (1-SS SMA)**, where the cumsum in linear attention's recurrence is replaced by a more general recurrence -- the scalar SSM scan, i.e. 1-semiseparable matrix multiplication (Section 3.2.2).

Finally, the most important reason 1-semiseparable SMA is considered is because the linear form for computing it is a special case of diagonal state space model. The linear form of SMA is algorithm (15), where the bottleneck step (15b) can be viewed as matrix multiplication by the 1-SS mask. In Section 3, the computation for a diagonal SSM (8) was also written out, where the bottleneck step (8b) is a scalar SSM recurrence which is equivalent to 1-SS multiplication. The only difference is that (8b) has an extra N dimension in $L$, because the matrix $A$ is a diagonal matrix of size N. This N dimension would disappear if all diagonal entries of $A$ are the same, which results in Corollary 5.1.

---
[p. 17 continued]

**Corollary 5.1.** *1-SS SMA (masked attention with 1-semiseparable structured matrices $L$) (15) is a special case of a diagonal SSM (8) where the diagonal matrix is a scalar multiple of the identity.* [p. 17]

While Corollary 5.1 says that 1-SS SMA has an efficient recurrent form, the converse result characterizes which instances of SMA have efficient autoregression.

**Theorem 5.2.** *For any instantiation of structured masked attention (Definition 4.2) that is an autoregressive process with bounded order, the structured mask $L$ must be a semiseparable matrix.* [p. 17]

In other words, efficient autoregressive attention is general *semiseparable SMA*. Theorem 5.2 is proved in Appendix C.2.

**Remark 7.** *While 1-semiseparable SMA is a special case of a state space model, general semiseparable SMA is strictly more expressive than 1-SS SMA, and cannot be described by a standard SSM. However, the semiseparable multiplication by $L$ and the linear form of SMA (equation (15a)) each involve an expansion and contraction step, and can be absorbed into a similar instance of 1-SS SMA with a single (larger) expansion.* [p. 17]

In summary, 1-semiseparable structured attention is the most important case of SMA, because it is:

- a natural generalization of linear attention with an input-dependent recurrence.
- the simplest case of general semiseparable attention, which is equivalent to efficient autoregressive attention.
- a special case of a diagonal state space model.

## 5.3 Structured State-Space Duality (SSD)

[p. 17] To summarize the results:

- Structured state-space models (Section 3) are a model usually defined through a linear-time recurrence. However, by expanding the matrix formulation characterizing its linear sequence-to-sequence transformation, one can derive a quadratic form.
- Attention variants (Section 4) are a model defined through quadratic-time pairwise interactions. However, by viewing it as a four-way tensor contraction and reducing in a different order, one can derive a linear form.
- A natural special case of each one -- more precisely, state space models with scalar-identity structure on the $A$ matrices, and structured masked attention with 1-semiseparable structure on its $L$ mask -- are duals of each other with the exact same linear and quadratic forms.

**Figure 4** (p. 18): "(**Structured State Space Duality.**) State space duality describes the close relationship between state space models and masked attention. (*Left*) General SSMs and SMA both possess linear and quadratic forms, with direct analogs in notation. (*Right*) SSMs and SMA intersect at a large class of *state space dual models* (SSD) which capture many sequence models as special cases."

(*Left*) Table of notation duality between SSM and SMA:

| Structured State Space Model | Structured Masked Attention |
|---|---|
| $C$ (contraction matrix) | $Q$ (queries) |
| $B$ (expansion matrix) | $K$ (keys) |
| $X$ (input sequence) | $V$ (values) |
| $A_{j:i}$ (state matrix) | $L_{ji}$ (mask) |
| N (state expansion dim.) | N (kernel feature dim.) |
| $H$ (hidden states (8b)) | SMA linear dual (15) |
| $= L \cdot XB$ (linear mode) | |
| SSM quadratic dual (16) | $G$ (Gram matrix (13a)) |
| | $= Q \cdot K^\top$ (quadratic mode) |

(*Right*) Hierarchical diagram showing:
- Structured State Space Model (SSM) at the top level
  - S4: Diagonal State Space Model
    - DSS: Scalar-Identity SSM
    - S4D: RetNet, GateLoop, TransNormer
    - S5
    - S6: Linear Attention, 1-Semiseparable SMA
  - Efficient Autoregressive Attention: Semiseparable SMA
  - Structured Masked Attention (SMA) at the bottom level
- The intersection region is labeled **Structured State Space Duality (SSD)**.

Figure 4 summarizes the duality between these two representations.

An extended related work and discussion (Section 10) describes the relationship between SSD and general SSMs / attention in more detail.
