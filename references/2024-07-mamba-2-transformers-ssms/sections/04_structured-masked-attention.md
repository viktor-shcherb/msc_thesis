# 4 Structured Masked Attention: Generalizing Linear Attention with Structured Matrices [p. 11-15]

[p. 11] This section revisits the linear attention framework from first principles. The main results in this section are a simple tensor-contraction-based proof of linear attention (Proposition 4.1), and the generalized abstraction of structured masked attention in Definition 4.2. This section derives the main duality results from a different direction than state space models and can be read completely independently of Section 3.

- Section 4.1 sets up the framework for variants of attention, with a particular focus on kernel attention and masked kernel attention.
- Section 4.2 provides the first main attention result, a simple proof of linear attention through the lens of tensor contractions.
- Section 4.3 defines structured masked attention, the generalization of prior attention variants through structured matrices.

## 4.1 The Attention Framework

### 4.1.1 Attention

[p. 12] The basic form of (single-head) attention is a map on three sequences of vectors $(Q, K, V) \mapsto Y$.

$$Q = \text{input} \quad (\mathsf{T}, \mathsf{N})$$
$$K = \text{input} \quad (\mathsf{S}, \mathsf{N})$$
$$V = \text{input} \quad (\mathsf{S}, \mathsf{P})$$
$$G = QK^\top \quad (\mathsf{T}, \mathsf{S})$$
$$M = f(G) \quad (\mathsf{T}, \mathsf{S})$$
$$Y = MV \quad (\mathsf{T}, \mathsf{P}) \tag{9}$$

"Shape annotations" are used to indicate the dimensions of tensors, e.g. $Q \in \mathbb{R}^{(\mathsf{T},\mathsf{N})}$. In this general form, S and T represent *source* and *target* sequence lengths, N represents the *feature dimension*, and P represents the *head dimension*.

The most common variant of **softmax attention** uses a softmax activation $f = \text{softmax}$ to normalize the rows of the $G$ matrix.

### 4.1.2 Self-Attention

[p. 12] The treatment is motivated by the most important case of self-attention, where

(i) the source and target sequences are the same (i.e. $\mathsf{S} = \mathsf{T}$),

(ii) usually the feature and head dimensions are the same (i.e. $\mathsf{N} = \mathsf{P}$),

(iii) and $Q, K, V$ are generated by linear projections on the same input vector ($Q = W_Q \cdot X, K = W_K \cdot X, V = W_V \cdot X$).

However, the presentation abstracts away these choices and begins from the $Q, K, V$ matrices.

**Remark 5.** *Our focus is on the self-attention case with equal head and feature dimensions (i.e. $\mathsf{S} = \mathsf{T}$ and $\mathsf{N} = \mathsf{P}$), which should be used as the running example. We define the general formulation of attention not only so that our framework captures variants such as cross-attention, but also because separating the notation for dimensions (e.g. S and T) makes the contraction notation proofs of our main results in this section more clear.* [p. 12]

**Remark 6.** *While attention is usually framed as an operation on these three inputs $Q, K, V$ which are viewed symmetrically, the input and output dimensions in (9) indicate otherwise. In particular, the feature dimension N is not present in the output; therefore in the case when $\mathsf{S} = \mathsf{T}$ (e.g. self-attention), we view $V$ as the main input, so that (9) defines a proper sequence transformation $V \mapsto Y$ (Definition 2.1).* [p. 12]

### 4.1.3 Kernel Attention

[p. 12-13] The step where the softmax function is applied to the Gram matrix $G$ can be decomposed into two parts:

1. Exponentiating the $G$ matrix.
2. Normalizing the $G$ matrix on the S axis.

The normalization term can be ignored for now, as it amounts to simply passing in $V = 1$ and dividing (revisited in Section 7.3). The exponentiation term can be viewed as a kernel transformation: there is an (infinite-dimensional) feature map $\varphi$ such that $\exp(QK^\top) = \varphi(Q)\varphi(K)^\top$. By abstracting away the feature map into the definition of $Q$ and $K$ itself (i.e. define $Q, K$ as the post-transformed versions), the softmax transformation can be ignored, and it is assumed that $Q, K$ are arbitrarily generated by kernel feature maps and potentially $\mathsf{N} \neq \mathsf{P}$.

Many instantiations of kernel attention have been proposed, including:

- The original Linear Attention (Katharopoulos et al. 2020) defines the kernel feature map as an arbitrary pointwise activation function, such as $x \mapsto 1 + \text{elu}(x)$.

- Random Feature Attention (RFA) (H. Peng et al. 2021) chooses the kernel feature map to approximate softmax attention (i.e. the exp feature map) using the random Fourier feature approximation of Gaussian kernels (Rahimi and Recht 2007). This involves random projections (i.e. multiplying $Q$ and $K$ by a random projection $W$ and applying the activation $x \mapsto (\cos(x), \sin(x))$).

- Performer (Choromanski et al. 2021) proposes the fast attention via positive orthogonal random features (FAVOR+). The positive random features (PRF) part chooses the kernel feature map to be a random projection followed by the feature map $x \mapsto 2^{-1/2}(\exp(x), \exp(-x))$. This choice is motivated so that the kernel elements are positive-valued and provably approximates the softmax attention. [It also proposes choosing the random projections in orthogonal directions, which is not considered.]

- cosFormer (Qin, Weixuan Sun, et al. 2022) augments RFA with a cosine reweighting mechanism that incorporates positional information to emphasize locality. This effectively passes $Q_t, K_t$ through the feature map $x \mapsto (x \cos(\pi t / 2T), \sin(\pi t / 2T))$.

- Linear Randomized Attention (Zheng, C. Wang, and Kong 2022) generalizes RFA from the perspective of importance sampling, and generalizes it to provide better estimates of the full softmax kernel (rather than just the exp-transformed numerator).

[p. 13] Other related attention variants include Linformer (Sinong Wang et al. 2020) and Nystromformer (Xiong et al. 2021), which both use low-rank approximations of the attention matrix $M$ (and are thus compatible with equation (9)), through random projections (Johnson-Lindenstrauss) and kernel approximation (the Nystrom method) respectively.

### 4.1.4 Masked (Kernel) Attention

[p. 13] Let $L$ be a mask of shape $(\mathsf{T}, \mathsf{S})$. Most commonly, in the *autoregressive* self-attention case when $\mathsf{S} = \mathsf{T}$, $L$ may be a lower-triangular matrix of 1's representing a *causal mask*. Besides enforcing causality, many other types of masks can be applied -- in particular various sparsity patterns such as banded, dilated, or block diagonal -- which are motivated by reducing the complexity of dense attention.

Masked attention is usually written in matrix notation as

$$y = (L \circ (QK^\top)) \cdot V. \tag{10}$$

More precisely, with shape annotations and breaking this down into the precise sequence of computations:

$$G = QK^\top \quad (\mathsf{T}, \mathsf{S})$$
$$M = G \circ L \quad (\mathsf{T}, \mathsf{S}) \tag{11}$$
$$Y = MV \quad (\mathsf{T}, \mathsf{P})$$

The improved derivation of attention variants in this section starts by noticing that this formula can be written as a *single contraction*:

$$Y = \text{contract}(\mathsf{TN}, \mathsf{SN}, \mathsf{SP}, \mathsf{TS} \to \mathsf{TP})(Q, K, V, L) \tag{12}$$

and the algorithm in (11) can be reframed as computing (12) by a particular ordering of pairwise contractions

$$G = \text{contract}(\mathsf{TN}, \mathsf{SN} \to \mathsf{TS})(Q, K) \quad (\mathsf{T}, \mathsf{S}) \tag{13a}$$

$$M = \text{contract}(\mathsf{TS}, \mathsf{TS} \to \mathsf{TS})(G, L) \quad (\mathsf{T}, \mathsf{S}) \tag{13b}$$

$$Y = \text{contract}(\mathsf{TS}, \mathsf{SP} \to \mathsf{TP})(M, V) \quad (\mathsf{T}, \mathsf{P}) \tag{13c}$$

## 4.2 Linear Attention

[p. 13-14] Linear attention, and many other variants of efficient attention, is often motivated by changing the order of matrix associativity in the core attention computation $(QK^\top)V = Q(K^\top V)$. However when the mask is added, the derivation is somewhat less straightforward (for example, the original paper (Katharopoulos et al. 2020) and variants (Y. Sun et al. 2023) state the formula without proof).

Roughly, the linear attention method claims that the following formula is equivalent to (10), which must be verified by expanding the sum and tracking indices carefully.

$$Y = Q \cdot \text{cumsum}(K^\top V) \tag{14}$$

**Proposition 4.1** ((Katharopoulos et al. 2020)). *Autoregressive kernel attention, i.e. masked kernel attention with the causal mask, can be computed in $O(\mathsf{T})$ time by a recurrence taking constant time per step.* [p. 14]

### 4.2.1 A Tensor Contraction Proof of Linear Attention

[p. 14] A simple and rigorous derivation of linear attention is presented that also immediately reveals how to generalize it. The main idea is to perform the contraction (12) in an alternate order. Ambiguous matrix notation is avoided and the work proceeds directly with contraction notation:

$$Z = \text{contract}(\mathsf{SP}, \mathsf{SN} \to \mathsf{SPN})(V, K) \quad (\mathsf{S}, \mathsf{P}, \mathsf{N}) \tag{15a}$$

$$H = \text{contract}(\mathsf{TS}, \mathsf{SPN} \to \mathsf{TPN})(L, Z) \quad (\mathsf{T}, \mathsf{P}, \mathsf{N}) \tag{15b}$$

$$Y = \text{contract}(\mathsf{TN}, \mathsf{TPN} \to \mathsf{TP})(Q, H) \quad (\mathsf{T}, \mathsf{P}) \tag{15c}$$

Intuitively, this contraction order is interpreted as follows.

The first step (15a) performs an "expansion" into more features, by a factor of the feature dimension N. The third step (15c) contracts the expanded feature dimension away. If $K$ is viewed as the input (Remark 6), then $V$ and $Q$ perform the expansion and contraction, respectively.

The second step is the most critical, and explains the *linear* part of linear attention. (15b) is just a direct matrix multiplication by $L$ (since the (P, N) axes can be flattened). Also note that this is the only term that involves both T and S axes, hence should have $\Omega(\mathsf{TS})$ complexity (i.e. quadratic in sequence length). However, when the mask $L$ is the standard causal attention mask (lower triangular 1's), matrix-vector multiplication by $L$ is identical to a feature-wise cumulative sum

$$y = \begin{bmatrix} 1 \\ \vdots & \ddots \\ 1 & \cdots & 1 \end{bmatrix} x \iff \begin{aligned} y_0 &= x_0 \\ y_t &= y_{t-1} + x_t. \end{aligned}$$

## 4.3 Structured Masked Attention

[p. 14] With the tensor contraction perspective of masked attention (15), it can immediately be seen that the crux of the original linear attention is the fact that *matrix-vector multiplication by the causal mask is equivalent to the cumulative sum operator*.

However, there is no reason the attention mask has to be all 1's. All that is necessary for linear attention to be fast is for $L$ to be a *structured matrix*, which by definition are those that have fast matrix multiplication (Section 2.3). In particular, *any mask matrix* $L$ that has sub-quadratic (ideally linear) matrix-vector multiplication can be used, which would have the same complexity as standard linear attention by speeding up the bottleneck equation (15b).

**Definition 4.2.** *Structured masked attention (SMA) (or structured attention for short) is defined as a function on queries/keys/values $Q, K, V$ as well as any structured matrix $L$ (i.e. has sub-quadratic matrix multiplication), through the 4-way tensor contraction*

$$Y = \text{contract}(\mathsf{TN}, \mathsf{SN}, \mathsf{SP}, \mathsf{TS} \to \mathsf{TP})(Q, K, V, L).$$

*The SMA **quadratic mode algorithm** is the sequence of pairwise contractions defined by (13), which corresponds to the standard (masked) attention computation.*

*The SMA **linear mode algorithm** is the sequence of pairwise contractions defined by (15), where step (15b) is optimized through the subquadratic structured matrix multiplication.* [p. 14]

Structured masked attention can be instantiated to any given class of matrix structure. Some examples include (Figure 3):

- Linear attention uses a causal mask.
- RetNet (Y. Sun et al. 2023) uses a decay mask $L_{ij} = \gamma^{i-j} \cdot \mathbb{I}[j \le i]$ for some decay factor $\gamma \in [0, 1]$.
- The decay mask could be generalized to a Toeplitz matrix $L_{ij} = \alpha_{i-j}$ for some learnable (or input-dependent) set of parameters $\alpha \in \mathbb{R}^\mathsf{T}$. This can be interpreted as a form of relative positional encoding, reminiscent of other methods such as ALiBi (Press, N. Smith, and Lewis 2022) but multiplicative instead of additive.
- Another variant could use a Fourier matrix $L_{ij} = \omega^{ij/\mathsf{T}}$ to encode positional structure a different way.

In Section 5, semiseparable SMA is considered, which defines the main SSD model.

**Figure 3** (p. 15): "(**Structured Masked Attention.**) SMA constructs a masked attention matrix $M = QK^\top \circ L$ for any structured matrix $L$, which defines a matrix sequence transformation $Y = MV$. All instances of SMA have a dual subquadratic form induced by a different contraction ordering, combined with the efficient structured matrix multiplication by $L$. Previous examples include Linear Attention (Katharopoulos et al. 2020) and RetNet (Y. Sun et al. 2023). Beyond SSD (1-semiseparable SMA), the focus of this paper, many other potential instantiations of structured attention are possible."

The figure shows a visual matrix decomposition. On the left, Queries $Q$ and Keys $K$ form the Gram matrix $QK^\top$, which is element-wise multiplied ($\circ$) by the Structured Mask $L$, yielding the Sequence Transformation Matrix $M$. Five rows of examples are shown:

| Structured Mask $L$ | Sequence Transformation Matrix $M$ |
|---|---|
| Causal Mask | Linear Attention |
| Decay Mask | Retentive Network |
| 1-semiseparable | 1-SS Structured Attention (labeled **SSD**) |
| Toeplitz | Toeplitz Structured Attention |
| Discrete Fourier Transform | Fourier Structured Attention |

Each mask is visualized as a heatmap showing its matrix structure (lower-triangular patterns with varying decay/structure), and the resulting $M$ matrices show how the attention pattern changes.

### 4.3.1 Summary: The Dual Forms of Masked Attention

[p. 15] Standard (masked kernel) attention is often conflated between a function and an algorithm. Separating this distinction presents a clear way to understand different variants of attention.

- Masked attention is viewed as a particular *function* (12).
- The standard **quadratic attention** computation (13) can be viewed as an *algorithm* to compute the function.
- **Linear attention** (15) is an alternate algorithm to compute the same function.

Moreover, in this case:

- The masked attention function is simply a particular *contraction on four terms*.
- The quadratic and linear attention algorithms are simply *two different orders to perform the contractions*.

It is known that contraction orderings can make large differences in computation complexity, leading to the quadratic vs. linear split. Just as state space models are a transformation that can be computed in multiple ways, with dual quadratic vs. linear forms (Section 3.4), linear attention has a similar duality that results from two contraction orders. In fact, these turn out to be different perspectives on the same underlying duality, which is made explicit in Section 5.
