# Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality

**Authors:** Tri Dao, Albert Gu
**Affiliations:** Princeton University, Carnegie Mellon University

## Publication Status

- **arXiv preprint:** May 2024, arXiv:2405.21060
- **Peer-reviewed:** Yes
- **Conference/Journal:** International Conference on Machine Learning (ICML) 2024, Vienna, Austria, July 21-27, 2024
- **Status:** Published conference paper

## Preferred Citation

Cite the ICML 2024 version:

> Dao, T. & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. In Proceedings of the 41st International Conference on Machine Learning (ICML 2024).

## Links

- arXiv: https://arxiv.org/abs/2405.21060
- Code: https://github.com/state-spaces/mamba
- Model weights: Available via the state-spaces/mamba repository
