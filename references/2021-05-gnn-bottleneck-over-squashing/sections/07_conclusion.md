# 7 Conclusion [p. 9]

[p. 9] The authors propose a novel explanation to a well known limitation in training graph neural networks: a bottleneck that causes over-squashing. Problems that depend on long-range interaction require as many GNN layers as the desired radius of each node's receptive field. This causes an exponentially-growing amount of information to be squashed into a fixed-length vector. As a result, the GNN fails to propagate long-range information, learns only short-range signals from the training data, and performs poorly when the prediction task depends on long-range interaction.

The authors demonstrate the existence of the bottleneck in a controlled problem, provide theoretical lower bounds for the hidden size given the problem radius, and show that GCN and GIN are more susceptible to over-squashing than GAT and GGNN. They further show that prior models of chemical, biological and programmatical benchmarks suffer from over-squashing by showing that they can be dramatically improved using a simple FA layer. They conclude that over-squashing in GNNs is so prevalent and untreated -- that even the simplest solution helps. Their observations open the path for a variety of follow-up improvements and even better solutions for over-squashing.
