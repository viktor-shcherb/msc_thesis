# Appendix A: Related Work [p. 15]

## A.1 Attention sink phenomenon

Although the term *attention sink* was introduced by Xiao et al. (2023b), similar observations have been reported in prior studies. Among them, Zhai et al. (2023) demonstrated that attention weights in Transformer models (Vaswani et al., 2017) collapse into one-hot vectors, a phenomenon termed *attention entropy collapse*. This issue has been identified as *attention logit growth* by Wortsman et al. (2023); Dehghani et al. (2023). Furthermore, Bondarenko et al. (2023) observed the emergence of strong outliers in Transformer models, linked to attention heads that learn not to update residuals. Although not explicitly using the term *attention sink*, Han et al. (2024) discussed the significance of the first few tokens in language models (LMs), noting their distinct representational space. In vision transformers (ViTs), Darcet et al. (2023) identified similar behaviors as artifacts in attention maps. Overall, the attention sink phenomenon is prevalent across Transformer models, including ViTs, encoder-only LMs, such as BERT (Devlin et al., 2019), and auto-regressive LMs. Notably, in auto-regressive LMs, attention sink consistently occurs on the first token in addition to tokens with limited semantic information in the middle context (Sun et al., 2024; Yu et al., 2024).

## A.2 Attention sink and activation outliers

Cancedda (2024) found that early FFNs in LLaMA2 blast off the large norm of hidden states for the first token, leading to attention sink at that position. This is referred to as *massive activations* (very few activations exhibit disproportionately large values) in Sun et al. (2024) and *token-wise activation outlier* in outlier literature (Lin et al., 2024; Bondarenko et al., 2023). In contrast, *channel-wise activation outlier* (Dettmers et al., 2022; Xiao et al., 2023a) refer to that outliers appear in specific channels across all token positions, and a concurrent work (Kaul et al., 2024) found that these two types of outliers are distinct and require different mitigation strategies. Despite the high magnitude in hidden states, Bondarenko et al. (2023); Guo et al. (2024b) observed the values associated with sink tokens are typically smaller than those of other tokens.

## A.3 Understanding and mitigate attention sink

To understand the attention sink phenomenon in general Transformer models, Bondarenko et al. (2023) hypothesized that the interplay among softmax, residual connections, and LayerNorm encourages models to learn not to update residuals. In auto-regressive LMs, Sun et al. (2024) attributed attention sink at the first position to implicit biases in keys and values. A concurrent work by Guo et al. (2024a) provided empirical and theoretical analysis of attention sink in very simple Transformer LMs on the Bigram-Backcopy (BB) task, identifying a mutual reinforcement mechanism as the underlying cause. Their findings on the BB task indicated that: (i) replacing LayerNorm with other normalizations does not remove attention sink; (ii) switching the optimizer Adam to SGD eliminates massive activations but attention sink remains.

Besides the above literature, Xiao et al. (2023b); Kaul et al. (2024) found that replacing softmax with softmax-off-by-one (Miller, 2023) alleviates attention sink at the first position, though Xiao et al. (2023b) noted its potential emergence at other initial token positions. Yin et al. (2024) proposed refining the causal mask with pseudo-attention computation to alleviate massive activations and reduce attention sink. Though not explicitly targeting attention sink, methods such as $\sigma$Reparam (Zhai et al., 2023) and qk-norm (Dehghani et al., 2023) were introduced to address attention entropy collapse. To mitigate activation outliers, He et al. (2024) developed a protected block and also highlighted the role of optimization in mitigating the outliers. Besides, Bondarenko et al. (2023) proposed clipped softmax and gated attention, while Kaul et al. (2024) introduced OrthoAdam to reduce activation outliers.

## A.4 Applications of attention sink

Sink tokens in LMs are functionally important and absorb significant attention. This enables computational savings by prioritizing initial and recent tokens over middle-context tokens in attention calculations. Token-wise activation outliers pose challenges for quantization, necessitating specialized handling to facilitate quantization processes. These insights have motivated various applications, including streaming/long context generation (Xiao et al., 2023b; Han et al., 2024; Yang et al., 2024), KV cache optimization (Ge et al., 2023; Wan et al., 2024; Wu & Tu, 2024), efficient inference (Zhang et al., 2024b; Chen et al., 2024), model quantization (Liu et al., 2024b; Huang et al., 2024), and others.
