# Future work [p. 10]

[p. 10]

This work focuses on the sink token in the first position. Sun et al. (2024); Yu et al. (2024) showed that attention sink can also appear on certain word tokens, e.g., period and newline tokens. However, these sink words may vary in different LMs and typically have no fixed positions. In the future, we will extend the research scope to explore how these sink tokens are related to the pre-training. Additionally, it remains unclear whether attention sink benefits LM downstream performance [p. 10].
