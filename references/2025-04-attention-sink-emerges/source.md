# When Attention Sink Emerges in Language Models: An Empirical View

**Authors:** Xiangming Gu*, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, Min Lin
**Affiliations:** Xiangming Gu (Sea AI Lab, Singapore / National University of Singapore*), Tianyu Pang (Sea AI Lab, Singapore), Chao Du (Sea AI Lab, Singapore), Qian Liu (Sea AI Lab, Singapore), Fengzhuo Zhang (Sea AI Lab, Singapore / National University of Singapore), Cunxiao Du (Sea AI Lab, Singapore), Ye Wang (National University of Singapore), Min Lin (Sea AI Lab, Singapore)

*Work done during Xiangming Gu's internship at Sea AI Lab. Correspondence to Tianyu Pang and Ye Wang.

## Publication Status

- **arXiv preprint:** October 2024, arXiv:2410.10781
- **Peer-reviewed:** Yes
- **Conference/Journal:** The Thirteenth International Conference on Learning Representations (ICLR 2025), Singapore, April 24--28, 2025 (Spotlight)
- **Status:** Published conference paper

## Preferred Citation

Cite the ICLR 2025 version:

> Gu, X., Pang, T., Du, C., Liu, Q., Zhang, F., Du, C., Wang, Y., & Lin, M. (2025). When Attention Sink Emerges in Language Models: An Empirical View. In The Thirteenth International Conference on Learning Representations.

## Links

- arXiv: https://arxiv.org/abs/2410.10781
- OpenReview: https://openreview.net/forum?id=78Nn4QJTEN
- Proceedings: https://proceedings.iclr.cc/paper_files/paper/2025/file/f1b04face60081b689ba740d39ea8f37-Paper-Conference.pdf
- Code: https://github.com/sail-sg/Attention-Sink
