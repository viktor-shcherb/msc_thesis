# Appendix E: Generated Text [p. 16–20]

Generated text from the best model trained on the WikiText-103 dataset. The Transformer-XL is seeded with a context of at most 512 consecutive tokens randomly sampled from the test set of WikiText-103. Then, Transformer-XL is run to generate a *pre-defined* number of tokens (500 or 1,000). For each generation step, the top-40 probabilities of the next-step distribution are found and a sample is drawn from top-40 tokens based on the re-normalized distribution. The text is detokenized for readability. [p. 16]

Three generated examples are shown in Tables 11, 12, and 13. The authors note they do not perform any cherry picking and present the first three examples generated. In the text, "= text =", "= = text = =" and "= = = text = = =" denote the Wikipedia page title, section title and subsection title, respectively, due to the original data preprocessing procedure of WikiText-103 (Merity et al., 2016). [p. 16]

## Generation quality observations

Though only trained on 100M tokens, Transformer-XL is a strong model at generating long text articles, particularly in the following aspects: [p. 16]

- Transformer-XL is able to structurally maintain the sectional arrangement of Wikipedia.
- Transformer-XL manages to semantically stay on the same topic throughout the course of generation.
- Long-range references are common in the generated text.
- Transformer-XL often generates novel content that is not present in the training data.

## Limitations caveat

> "Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set." [p. 17]

## Table 11: Example 1 (500 tokens)

**Table 11** (p. 17): "Example 1 -- 500 tokens generated by XL using a snippet from the Wikitext-103 test set as initial context. The sample is randomly generated without any cherry picking."

- Original Wikipedia page: https://en.wikipedia.org/wiki/Clayton_Kershaw
- The context is about Clayton Kershaw's 2010 baseball season with the Dodgers. The XL generation continues into the 2011, 2012, and 2013 seasons. The reference text (right column) provides real Kershaw statistics and career information.

Note: The generated and reference texts are presented side-by-side in a two-column format (XL Generation | Reference), each several paragraphs long. Due to the length of the raw generated text, the key observations are captured below rather than transcribing the full prose.

Observations from this example: [p. 17]
- Kershaw never went to the Royals in real life. Despite that, Transformer-XL stays on the fully imagined topic and keeps hallucinating the experience of Kershaw in the Royals across the generated text.
- XL correctly tracks the chronological order from 2011 to 2012 and to the finally 2013 season in the section titles.
- Transformer-XL accurately uses the phrase "another back injury" in the 2013 season paragraph, since it has talked about one earlier injury in the 2012 season. This shows Transformer-XL's ability of capturing long-term dependency.

## Table 12: Example 2 (500 tokens)

**Table 12** (p. 18): "Example 2 -- 500 tokens generated by XL using a snippet from the Wikitext-103 test set as initial context. The sample is randomly generated without any cherry picking."

- Original Wikipedia page: https://en.wikipedia.org/wiki/The_Tale_of_Mrs._Tittlemouse
- The context describes a species of palm tree (genus A. with <unk> tokens) including habitat, ecology, and uses. After the seed context ends, both the generated and reference texts start a new Wikipedia topic.

Note: As with Table 11, the full generated and reference texts are presented side-by-side and span several paragraphs each. Key observations are captured below.

Observations from this example: [p. 18]
- After finishing the last paragraph of the seed context, both the reference and generated text start a new topic (i.e., Wikipedia page), as marked by the single "= title =" line, and randomly starting with a new topic. This suggests the model has the ability of identifying the end of a topic / page.
- Even more interestingly, a newly-started page is on a book called "The Tale of Mrs. Tittlemouse". Transformer-XL manages to copy the same book title and some related information from the training set, but hallucinates *novel* content of the book. This demonstrates a degree of generalization instead of memorization.

## Table 13: Example 3 (1,000 tokens)

**Table 13** (p. 19–20): "Example 3 -- 1,000 tokens generated by XL using a snippet from the Wikitext-103 test set as initial context. The sample is randomly generated without any cherry picking."

- Original Wikipedia page: https://en.wikipedia.org/wiki/Battle_of_D%C3%BCrenstein
- The context describes the Battle of Durenstein (1805), a Napoleonic Wars engagement on the River Danube near Vienna. The XL generation continues with detailed (hallucinated) military campaign narratives involving Napoleon, various Austrian and French commanders, and multiple battles.

Note: This is the longest example at 1,000 tokens. The generated and reference texts are presented side-by-side. Key observations are captured below.

Observations from this example: [p. 20]
- Although this example is significantly longer, Transformer-XL is still able to stay on the same topic and makes up non-existing stories about the Napoleon wars.
- Notably, from the second section on, the generated text correctly follows a fine-grained chronological order *on the level of month and day* to narrate events in 1805, except a mistake (1804 instead of 1805) near the end of the paragraph. The authors highlight all date-related phrases by magenta in the generation.
