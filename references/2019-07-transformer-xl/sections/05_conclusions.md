# 5 Conclusions [p. 9]

Transformer-XL obtains strong perplexity results, models longer-term dependency than RNNs and Transformer, achieves substantial speedup during evaluation, and is able to generate coherent text articles. The authors envision interesting applications of Transformer-XL in the fields of text generation, unsupervised feature learning, image and speech modeling. [p. 9]
