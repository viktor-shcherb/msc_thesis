# Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context

**Authors:** Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov
**Affiliations:** Carnegie Mellon University (Dai, Yang Z., Yang Y., Carbonell, Salakhutdinov), Google Brain (Dai, Yang Z., Le)

## Publication Status

- **arXiv preprint:** January 2019, arXiv:1901.02860
- **Peer-reviewed:** Yes
- **Conference:** Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019), pages 2978--2988, Florence, Italy, July 2019
- **Status:** Published conference paper

## Preferred Citation

Cite the ACL 2019 version:

> Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., & Salakhutdinov, R. (2019). Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2978--2988.

## Links

- arXiv: https://arxiv.org/abs/1901.02860
- ACL Anthology: https://aclanthology.org/P19-1285/
- DOI: https://doi.org/10.18653/v1/P19-1285
- Code: https://github.com/kimiyoung/transformer-xl
