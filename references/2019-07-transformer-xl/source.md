# Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context

**Authors:** Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov
**Affiliations:** Carnegie Mellon University, Google Brain

## Publication Status

- **arXiv preprint:** January 2019, arXiv:1901.02860
- **Peer-reviewed:** Yes
- **Conference:** ACL 2019 (57th Annual Meeting of the Association for Computational Linguistics)
- **Date:** July 2019
- **Location:** Florence, Italy
- **Pages:** 2978-2988

## Preferred Citation

Dai et al. (2019), "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context", ACL 2019

## Links

- **arXiv:** https://arxiv.org/abs/1901.02860
- **ACL Anthology:** https://aclanthology.org/P19-1285/
- **DOI:** https://doi.org/10.18653/v1/P19-1285
- **Code:** https://github.com/kimiyoung/transformer-xl
