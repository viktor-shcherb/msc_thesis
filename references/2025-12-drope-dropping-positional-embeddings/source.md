# DroPE: Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings

**Authors:** Yoav Gelberg, Koshi Eguchi, Takuya Akiba, Edoardo Cetin
**Affiliations:** Sakana AI, University of Oxford

## Publication Status

- **arXiv preprint:** December 2025, arXiv:2512.12167
- **Peer-reviewed:** No
- **Conference/Journal:** None confirmed as of February 2026
- **Status:** Preprint / Technical Report (Sakana AI)

## Preferred Citation

Cite as arXiv preprint:

> Gelberg, Y., Eguchi, K., Akiba, T., & Cetin, E. (2025). Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings. arXiv:2512.12167.

## Links

- arXiv: https://arxiv.org/abs/2512.12167
- Blog: https://pub.sakana.ai/DroPE/
- Code: https://github.com/SakanaAI/DroPE
