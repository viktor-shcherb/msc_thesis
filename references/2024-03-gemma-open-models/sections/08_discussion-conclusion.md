# Discussion and Conclusion [p. 10]

Gemma is presented as an openly available family of generative language models for text and code. Gemma advances the state of the art of openly available language model performance, safety, and responsible development. [p. 10]

The authors are confident that Gemma models will provide a net benefit to the community given their extensive safety evaluations and mitigations; however, they acknowledge that this release is irreversible and the harms resulting from open models are not yet well defined, so they continue to adopt assessments and safety mitigations proportionate to the potential risks of these models. In addition, their models outperform competitors on 6 standard safety benchmarks, and in human side-by-side evaluations. [p. 10]

Gemma models improve performance on a broad range of domains including dialogue, reasoning, mathematics, and code generation. Results on MMLU (64.3%) and MBPP (44.4%) demonstrate both the high performance of Gemma, as well as the continued headroom in openly available LLM performance. [p. 10]

Beyond state-of-the-art performance measures on benchmark tasks, the authors are excited to see what new use-cases arise from the community, and what new capabilities emerge as the field advances together. They hope that researchers use Gemma to accelerate a broad array of research, and that developers create beneficial new applications, user experiences, and other functionality. [p. 10]

Gemma benefits from many learnings of the Gemini model program including code, data, architecture, instruction tuning, reinforcement learning from human feedback, and evaluations. As discussed in the Gemini technical report, a non-exhaustive set of limitations to the use of LLMs is reiterated. Even with great performance on benchmark tasks, further research is needed to create robust, safe models that reliably perform as intended. Example further research areas include factuality, alignment, complex reasoning, and robustness to adversarial input. As discussed by Gemini, the need for more challenging and robust benchmarks is noted. [p. 10]
