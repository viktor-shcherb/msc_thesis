# 4 Conclusion [p. 10]

In this work, the authors introduce Differential Transformer (a.k.a. DIFF Transformer), which amplifies attention to the relevant context while canceling noise [p. 10]. Experimental results on language modeling show that DIFF Transformer outperforms Transformer in terms of scaling properties, long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers [p. 10]. The results emphasize the importance of reducing attention noise [p. 10]. Moreover, the differential attention mechanism can be easily implemented with FlashAttention (Dao et al., 2022) [p. 10]. The findings position DIFF Transformer as a distinctive and promising foundation architecture for large language models [p. 10]. In the future, they can develop efficient low-bit attention kernels due to the reduced magnitude of activation outliers [p. 10]. As the attention pattern becomes much sparser, they would also like to utilize the property to compress key-value caches [p. 10].
