# Experiments [p. 4]

The authors evaluate Differential Transformer for large language models from the following perspectives [p. 4]:

First, they compare the proposed architecture with Transformer in various downstream tasks (Section 3.1) and study the properties of scaling up model size and training tokens (Section 3.2) [p. 4].

Second, they conduct a length extension to 64K and evaluate long-sequence tasks (Section 3.3) [p. 4].

Third, they present the results of key information retrieval, contextual hallucination evaluation, and in-context learning (Sections 3.4â€“3.6) [p. 4].

Fourth, they show that Differential Transformer can reduce outliers in the model activations compared to Transformer (Section 3.7) [p. 4].

Fifth, they conduct extensive ablation studies for various design choices (Section 3.8) [p. 4].
