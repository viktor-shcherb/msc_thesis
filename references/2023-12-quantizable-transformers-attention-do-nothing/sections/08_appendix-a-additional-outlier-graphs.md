# A Additional Graphs from Outlier Analysis [p. 16]

[p. 16] This section presents additional graphs from the outlier investigation in Section 3 for BERT and vision transformer.

**Figure 9** (p. 16): "A summary of several outlier statistics recorded from ImageNet validation set on ViT. (a) Average infinity norm of the output of each attention layer. (b) A histogram of outlier counts in attention layer #10 vs. hidden dimensions. We use zero-based indexing for dimensions. (c) A heatmap of outlier counts in attention layer #10 vs. patch positions."
- (a) Line chart: x-axis is Attention layer (1 through 12), y-axis is Layer output inf. norm. The infinity norm is relatively low (~50) for layers 1-7, then rises sharply for layers 8-12, peaking at layer 11 (~350-400). This shows that the strongest magnitude outliers appear in the later layers, similar to BERT.
- (b) Bar chart (note: y-axis label "1e6" indicating counts in millions): x-axis shows hidden dimensions (#48, #40, #27, #24, #13, #23, #19, #46, #1, #3, #106, #11), y-axis is outlier count. Dimension #48 has by far the highest count (~1.0 million), followed by #40 (~0.4 million), with remaining dimensions having progressively smaller counts. This confirms that outliers are concentrated in only ~10 hidden dimensions.
- (c) Heatmap: shows outlier counts in attention layer #10 across patch positions (spatial grid). The outliers are concentrated at the boundaries of the image (edges), with counts up to ~80000 at boundary patches and much lower counts (~20000 or less) in the center. This suggests a strong correlation with the background and a negative correlation with the object, which is usually in the center of ImageNet images.

## A.1 BERT

[p. 16] Recall from Figure 1 that all the outliers are only present in hidden dimensions #123, #180, #225, #308, #381, #526, #720 (with the majority of them in #180, #720). These hidden dimensions correspond to attention heads #2, #3, #4, #5, #6, #9, and #12. In Figures 10 and 11 the authors show more examples of the discovered self-attention patterns for attention heads #3 and #12 (corresponding to hidden dim #180 and #720, respectively). Self-attention patterns in attention heads and layers which are not associated with the outliers are also shown in Figures 12 and 13, respectively. Finally, in Figures 14 and 15 the authors show more examples of the attention patterns learned in the network trained with clipped softmax and gated attention.

## A.2 ViT

[p. 16] Figure 9 further shows that there are many similarities in the outlier behavior in the vision transformer, compared to BERT. The strongest magnitude outliers generally happen in the later layers, peaking at layers #10 and #11. The majority of outliers (> 99%) are only ever happening in only 10 hidden dimensions, primarily in dimensions #48 and #43, which corresponds to the attention head #1. Finally, averaged across the entire ImageNet validation set, the outliers seem to be concentrated at the boundaries of the image, which suggests a strong correlation with the background (and a negative correlation with the object, which is usually in the center of the image in the ImageNet dataset).

In Figures 16 and 17, the authors show more examples of outlier and self-attention patterns in the attention head #1 (corresponding to hidden dimensions #48, #43) for a random subset of images from the ImageNet validation set (in layers #10 and #11, respectively).

---
[p. 23–26 continued]

**Figure 10** (p. 23): "Visualization of the self-attention patterns (attention probabilities, values, and their product in left, middle and right columns, respectively) in attention head #3 ($\leftrightarrow$ channel dim #180) for BERT-base trained with vanilla softmax, computed on several random data sequences from MNLI-m validation set."
- Shows 8 subplots (a–h), each depicting attention layers #10 and #11 for data sequences #16, #21, #61, and #88. Three columns per subplot: attention probabilities, values, and their product (attention-weighted values). The attention probability matrices show strong vertical stripes (high attention to specific token positions, consistent with the "no-op" / delimiter-attending pattern). The product columns show how outlier-producing heads create large-magnitude output in specific hidden dimensions.

**Figure 11** (p. 24): "Visualization of the self-attention patterns (attention probabilities, values, and their product in left, middle and right columns, respectively) in attention head #12 ($\leftrightarrow$ channel dim #720) for BERT-base trained with vanilla softmax, computed on several random data sequences from MNLI-m validation set."
- Same layout as Figure 10 (8 subplots, a–h, layers #10 and #11, data sequences #16, #21, #61, #88) but for attention head #12 instead of #3. Similar vertical-stripe patterns in attention probabilities are visible, confirming that head #12 (associated with hidden dim #720) also exhibits the outlier-producing "no-op" attention behavior.

**Figure 12** (p. 25): "Visualization of the self-attention patterns (attention probabilities, values, and their product in left, middle and right columns, respectively) in attention heads that are not associated with the strong outliers for BERT-base trained with vanilla softmax, computed on data sequences #16 from MNLI-m validation set."
- Shows 8 subplots (a–h) for attention heads #1, #7, #8, and #10 in layers #10 and #11. These heads are NOT associated with outlier dimensions. The attention probability matrices do not show the same strong vertical-stripe patterns seen in Figures 10 and 11. Instead, the attention patterns are more distributed and varied, without the concentrated delimiter-attending behavior. This contrast supports the paper's claim that the outlier-producing behavior is specific to certain attention heads.

**Figure 13** (p. 26): "Visualization of the self-attention patterns (attention probabilities, values, and their product in left, middle and right columns, respectively) in attention head #3 ($\leftrightarrow$ channel dim #180) and the first eight layers of BERT-base trained with vanilla softmax, computed on data sequences #16 from MNLI-m validation set."
- Shows 8 subplots (a–h) for attention layers #1 through #8, all for head #3 and data sequence #16. The earlier layers show progressively less pronounced vertical-stripe patterns compared to the later layers (shown in Figure 10). Layer 1 shows a clear diagonal pattern (local/self-attention), which gradually transitions to more complex patterns in middle layers. This demonstrates that the outlier-producing "no-op" attention behavior develops primarily in the later layers of the network.

---
[p. 27–30 continued]

**Figure 14** (p. 27): "Visualization of the self-attention patterns (attention probabilities, values, and their product in left, middle and right columns, respectively) for BERT-base trained with Clipped softmax, computed on several random data sequences from MNLI-m validation set."
- Shows 8 subplots (a–h) for various combinations of attention layers (#10, #11), attention head #3, and data sequences (#1, #5, #7). Each subplot has three columns: attention probabilities (left), values (middle), and their product (right). Additionally, each subplot includes a separate panel on the far left showing the attention probability distribution (y-axis from 0.0 to 1.0, x-axis showing token positions). The clipped softmax attention probability matrices show sparser patterns compared to vanilla softmax (Figures 10–11). The probability distributions show that many attention weights are pushed to exactly zero by the clipping, enabling exact "no-op" behavior without the need for extreme logit magnitudes.
- Subplots (g–h) show attention layer #10 and #11 with attention head #12 on data sequence #1, demonstrating the effect across different heads.

**Figure 15** (p. 28): "Visualization of the self-attention patterns (from left to right: gating probabilities $\pi$ = sigmoid ($G(\mathbf{x})$), output of softmax, values, and their combined product) for BERT-base trained with gated attention, computed on several random data sequences from MNLI-m validation set."
- Shows 8 subplots (a–h) with same layer/head/sequence combinations as Figure 14: layers #10 and #11, attention head #3, data sequences #1, #5, #7, plus attention head #12 on data sequence #1. Each subplot has four columns: gating probabilities (far left, shown with a colorbar), attention probability matrix (second column), values (third column), and combined product (fourth column). The gating probabilities column shows per-token gate values where the sigmoid gate can suppress entire attention outputs. The gate values vary by token and layer, with some tokens having gates near zero (fully suppressed) and others near one (fully active). This provides an alternative mechanism for "no-op" behavior: instead of pushing softmax inputs to extremes, the gate simply scales the attention output toward zero.

**Figure 16** (p. 29): "A summary of our outlier analysis for ViT demonstrated on a random subset from ImageNet validation set. (a) An input image. (b) Outliers in the output of layer #10. (c) Cumulative attention weight spent on every patch (matrix of attention probabilities summed over rows) in the attention head #1, in the next layer #11. (d) A corresponding matrix of attention probabilities. (e) An average magnitude of values ($V$) for outlier and non-outlier patches."
- Shows 8 rows of examples (8 different ImageNet images: chameleon on leaves, bird on flowers, puffin, hummingbird, bananas, deer/elk, classic car, water tower). Each row has 5 panels:
  - (a) Original input image.
  - (b) Overlay of outlier locations on the image. Outliers (shown in green) are concentrated at image boundaries and background patches, not on the main object. This is consistent with the observation in Figure 9(c).
  - (c) Cumulative attention weight per patch, visualized as a sparse grid. Only a few patches (typically at boundaries) receive large cumulative attention, shown as colored squares. Most patches receive near-zero attention.
  - (d) Matrix of attention probabilities for head #1 in layer #11. These show strong vertical stripes (columns) corresponding to the few patches that accumulate most attention, analogous to the delimiter-attending pattern seen in BERT.
  - (e) Bar chart comparing average magnitude of values ($V$) for non-outlier vs. outlier patches. Outlier patches consistently have larger value magnitudes than non-outlier patches across all 8 examples, supporting the claim that outlier tokens carry disproportionately large values.

**Figure 17** (p. 30): "A summary of our outlier analysis for ViT demonstrated on a random subset from ImageNet validation set. (a) An input image. (b) Outliers in the output of layer #11. (c) Cumulative attention weight spent on every patch (matrix of attention probabilities summed over rows) in the attention head #1, in the next layer #12. (d) A corresponding matrix of attention probabilities. (e) An average magnitude of values ($V$) for outlier and non-outlier patches."
- Same 8 ImageNet images and layout as Figure 16, but shifted one layer deeper: outliers shown from the output of layer #11, and attention patterns from head #1 in the next layer #12.
  - (a) Same input images as Figure 16.
  - (b) Outlier overlay for layer #11 output. Outliers again appear at image boundaries and background patches, with a similar spatial distribution to layer #10 (Figure 16).
  - (c) Cumulative attention per patch for head #1 in layer #12. Slightly denser cumulative attention compared to Figure 16(c), with more patches receiving non-trivial attention weight.
  - (d) Attention probability matrix for head #1 in layer #12. Vertical-stripe patterns are still visible but somewhat more diffuse compared to layer #11 in Figure 16(d).
  - (e) Bar chart comparing average value magnitude for outlier vs. non-outlier patches. Outlier patches again show larger magnitudes, though the difference varies by image. Error bars (whiskers) are visible, indicating variability.
