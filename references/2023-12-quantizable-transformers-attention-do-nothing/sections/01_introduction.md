# 1 Introduction [p. 1-2]

[p. 1] Quantization has been one of the most impactful ways to reduce the computational complexity of transformer networks. Previous work has shown that quantizing networks to 4-bit weights is possible without losing too much accuracy [66, 69]. Some research even shows 4-bit weights might be optimal when trading off model size and bit-width [12].

However, quantizing transformers is not always trivial. When quantizing the activations of a transformer, significant problems arise with outliers in specific layers. This has been noted by several researchers that suggest fixes to transformers after training to ameliorate their effect [13, 67]. These methods are frequently tedious and either require retraining the network, require implementing specific hardware for input-channel quantization [13] or require parts of the activations to still be in higher bit-widths, reducing the effectiveness of the activation quantization [67].

[p. 1-2] In this paper, the authors set out to solve the transformer outlier problem entirely by changing the architecture of the network itself, making transformers easy to quantize from the get-go without needing any post-processing. They thoroughly analyze why these outliers appear. Previous work has found the existence of these outliers [4, 13], but this work comes to a fuller understanding of these outlying values. The authors find that the outliers occur because attention heads are trying not to update the hidden state, and in the process, strong outliers appear due to the softmax function. This happens for language and vision transformers and different specific transformer architectures. This understanding is the foundation for two new tweaks they suggest to transformer architectures that can remove the problem of the outliers entirely.
