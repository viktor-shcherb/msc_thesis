# 7 Conclusions [p. 10]

[p. 10] The activation outlier problem that makes transformers difficult to quantize has been thoroughly analyzed. The paper showed that transformer networks try to learn not to update residuals and that by doing so, through the combination of the softmax, residual connections and LayerNorm, significant outliers appear in transformers. Based on this insight, two methods are proposed to address this at the core -- *clipped softmax* and *gated attention*. These structural changes to transformers give similar, if not better, floating-point performance after training but significantly improve the post-training quantization results. The authors hope that with these two architectural changes to transformers, anyone can train high-performance transformers that are easy to quantize and can benefit from efficient integer inference.
