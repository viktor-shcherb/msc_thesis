# References [p. 16-19]

This file contains only the references that are cited in the extracted section notes.

## References cited in the notes

**Abbasi-Yadkori, Yasin et al. (2019)**
"Politex: Regret bounds for policy iteration using expert prediction"
International Conference on Machine Learning, PMLR, pp. 3692–3702
- Cited in 03_reinforcement-learning.md regarding policy optimization foundations

**Ahmadian, Arash et al. (2024)**
"Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms"
arXiv preprint arXiv:2402.14740
- Cited in 03_reinforcement-learning.md regarding REINFORCE-style optimization

**Ankner, Zachary et al. (2024)**
"Critique-out-Loud Reward Models"
arXiv:2408.11791 [cs.LG]
URL: https://arxiv.org/abs/2408.11791
- Cited in 04b_training-recipe-long2short-sft.md regarding CoT reward models

**Berner, Christopher et al. (2019)**
"Dota 2 with large scale deep reinforcement learning"
arXiv preprint arXiv:1912.06680
- Cited in 04c_infrastructure.md as example of RL success in games

**Cassano, Federico et al. (2022, 2023)**
"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"
ArXiv (2022), IEEE Transactions on Software Engineering 49.7 (2023), pp. 3675–3691
DOI: 10.1109/TSE.2023.3267446
URL: https://arxiv.org/abs/2208.08227
- Cited in 05_infrastructure-continued.md and 10_appendix-c-evaluation-details.md regarding code generation benchmarks

**Chen, Jianlv et al. (2024)**
"Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation"
arXiv preprint arXiv:2402.03216
- Cited in 09_appendix-b-pretraining.md regarding document embeddings for similarity analysis

**Chen, Xingyu et al. (2024)**
"Do NOT Think That Much for 2+ 3=? On the Overthinking of o1-Like LLMs"
arXiv preprint arXiv:2412.21187
- Cited in 07_long2short-and-ablations.md regarding token efficiency in long2short problem

**Everitt, Tom et al. (2021)**
"Reward Tampering Problems and Solutions in Reinforcement Learning: A Causal Influence Diagram Perspective"
arXiv:1908.04734 [cs.AI]
URL: https://arxiv.org/abs/1908.04734
- Cited in 02_approach-reinforcement-learning-with-llms.md regarding reward hacking

**Gadre, Samir Yitzhak et al. (2024)**
"Datacomp: In search of the next generation of multimodal datasets"
Advances in Neural Information Processing Systems 36
- Cited in 09_appendix-b-pretraining.md regarding multimodal dataset curation

**Grattafiori, Aaron et al. (2024)**
"The Llama 3 Herd of Models"
arXiv:2407.21783 [cs.AI]
URL: https://arxiv.org/abs/2407.21783
- Cited in 09_appendix-b-pretraining.md regarding pretraining data quality

**Gulcehre, Caglar et al. (2023)**
"Reinforced self-training (rest) for language modeling"
arXiv preprint arXiv:2308.08998
- Cited in 07_long2short-and-ablations.md regarding ReST comparison in ablation studies

**Hendrycks, Dan et al. (2020)**
"Measuring Massive Multitask Language Understanding"
ArXiv abs/2009.03300
URL: https://arxiv.org/abs/2009.03300
- Cited in 06_experiments.md and 10_appendix-c-evaluation-details.md regarding MMLU benchmark

**Hoffmann, Jordan et al. (2022)**
"Training Compute-Optimal Large Language Models"
arXiv:2203.15556 [cs.CL]
URL: https://arxiv.org/abs/2203.15556
- Cited in 01_introduction.md regarding scaling laws

**Huang, Yuzhen et al. (2023)**
"C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models"
ArXiv abs/2305.08322
URL: https://arxiv.org/abs/2305.08322
- Cited in 06_experiments.md and 10_appendix-c-evaluation-details.md regarding Chinese evaluation benchmarks

**Jaech, Aaron et al. (2024)**
"Openai o1 system card"
arXiv preprint arXiv:2412.16720
- Cited in 04c_infrastructure.md regarding RL training methodology for LLMs

**Jain, Naman et al. (2024)**
"LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code"
ArXiv abs/2403.07974
URL: https://arxiv.org/abs/2403.07974
- Cited in 06_experiments.md and 10_appendix-c-evaluation-details.md regarding code benchmark evaluations

**Joulin, Armand et al. (2016)**
"Bag of tricks for efficient text classification"
arXiv preprint arXiv:1607.01759
- Cited in 09_appendix-b-pretraining.md regarding FastText-based classification

**Kaplan, Jared et al. (2020)**
"Scaling Laws for Neural Language Models"
arXiv:2001.08361 [cs.LG]
URL: https://arxiv.org/abs/2001.08361
- Cited in 01_introduction.md regarding scaling laws

**Kool, Wouter, Herke van Hoof, and Max Welling (2019)**
"Buy 4 reinforce samples, get a baseline for free!"
- Cited in 03_reinforcement-learning.md regarding baseline estimation in REINFORCE

**Kwon, Woosuk et al. (2023)**
"Efficient Memory Management for Large Language Model Serving with PagedAttention"
Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles
- Cited in 04c_infrastructure.md regarding memory management for inference serving

**Laurençon, Hugo et al. (2024)**
"Obelics: An open web-scale filtered dataset of interleaved image-text documents"
Advances in Neural Information Processing Systems 36
- Cited in 09_appendix-b-pretraining.md regarding multimodal data sources

**Li, Jeffrey et al. (2024)**
"Datacomp-lm: In search of the next generation of training sets for language models"
arXiv preprint arXiv:2406.11794
- Cited in 09_appendix-b-pretraining.md regarding language model training data

**Li, Ming et al. (2023)**
"From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning"
arXiv preprint arXiv:2308.12032
- Cited in 02_approach-reinforcement-learning-with-llms.md regarding data selection for fine-tuning

**Li, Raymond et al. (2023)**
"StarCoder: may the source be with you!"
arXiv:2305.06161 [cs.CL]
URL: https://arxiv.org/abs/2305.06161
- Cited in 09_appendix-b-pretraining.md regarding code data preprocessing

**Lightman, Hunter et al. (2023)**
"Let's Verify Step by Step"
arXiv preprint arXiv:2305.20050
- Cited in 06_experiments.md and 10_appendix-c-evaluation-details.md regarding MATH-500 benchmark

**Liu, Wei et al. (2023)**
"What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning"
arXiv preprint arXiv:2312.15685
- Cited in 02_approach-reinforcement-learning-with-llms.md regarding data selection strategies

**Lozhkov, Anton et al. (2024)**
"StarCoder 2 and The Stack v2: The Next Generation"
arXiv:2402.19173 [cs.SE]
URL: https://arxiv.org/abs/2402.19173
- Cited in 09_appendix-b-pretraining.md regarding code data preprocessing

**Lu, Pan et al. (2023)**
"Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts"
arXiv preprint arXiv:2310.02255
- Cited in 06_experiments.md and 10_appendix-c-evaluation-details.md regarding vision-based math benchmarks

**McAleese, Nat et al. (2024)**
"LLM Critics Help Catch LLM Bugs"
arXiv:2407.00215 [cs.SE]
URL: https://arxiv.org/abs/2407.00215
- Cited in 04b_training-recipe-long2short-sft.md regarding CoT reward model verification

**Mei, Jincheng et al. (2019)**
"On principled entropy exploration in policy optimization"
Proceedings of the 28th International Joint Conference on Artificial Intelligence, pp. 3130–3136
- Cited in 03_reinforcement-learning.md regarding entropy regularization

**Muennighoff, Niklas et al. (2023)**
"Scaling Data-Constrained Language Models"
arXiv:2305.16264 [cs.CL]
URL: https://arxiv.org/abs/2305.16264
- Cited in 01_introduction.md regarding data constraints in scaling

**Nachum, Ofir et al. (2017)**
"Bridging the gap between value and policy based reinforcement learning"
Advances in neural information processing systems 30
- Cited in 03_reinforcement-learning.md regarding policy optimization theory

**OpenAI (2024)**
"Learning to reason with LLMs"
URL: https://openai.com/index/learning-to-reason-with-llms/
- Cited in 03_reinforcement-learning.md regarding RL training for CoT

**Ouyang, Long et al. (2022)**
"Training language models to follow instructions with human feedback"
Advances in neural information processing systems 35, pp. 27730–27744
- Cited in 04_reinforcement-learning-continued.md and 04c_infrastructure.md regarding RLHF and InstructGPT methodology

**Pan, Alexander, Kush Bhatia, and Jacob Steinhardt (2022)**
"The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models"
International Conference on Learning Representations
URL: https://openreview.net/forum?id=JYtwGwIL7ye
- Cited in 02_approach-reinforcement-learning-with-llms.md regarding reward misspecification

**Paster, Keiran et al. (2023)**
"Openwebmath: An open dataset of high-quality mathematical web text"
arXiv preprint arXiv:2310.06786
- Cited in 09_appendix-b-pretraining.md regarding mathematical pretraining data

**Penedo, Guilherme et al. (2024)**
"The fineweb datasets: Decanting the web for the finest text data at scale"
arXiv preprint arXiv:2406.17557
- Cited in 09_appendix-b-pretraining.md regarding LLM-based quality assessment

**Qin, Ruoyu et al. (2024)**
"Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving"
arXiv:2407.00079 [cs.DC]
URL: https://arxiv.org/abs/2407.00079
- Cited in 04c_infrastructure.md regarding checkpoint transfer over RDMA

**Rafailov, Rafael et al. (2024)**
"Direct preference optimization: Your language model is secretly a reward model"
Advances in Neural Information Processing Systems 36
- Cited in 04b_training-recipe-long2short-sft.md regarding DPO method for long2short

**Schuhmann, Christoph et al. (2022)**
"Laion-5b: An open large-scale dataset for training next generation image-text models"
Advances in Neural Information Processing Systems 35, pp. 25278–25294
- Cited in 09_appendix-b-pretraining.md regarding vision pretraining data

**Shoeybi, Mohammad et al. (2020)**
"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"
arXiv:1909.08053 [cs.CL]
URL: https://arxiv.org/abs/1909.08053
- Cited in 04c_infrastructure.md regarding model parallelism

**Silver, David et al. (2017)**
"Mastering the game of go without human knowledge"
nature 550.7676, pp. 354–359
- Cited in 04c_infrastructure.md as example of RL success in games

**Snell, Charlie et al. (2024)**
"Scaling llm test-time compute optimally can be more effective than scaling model parameters"
arXiv preprint arXiv:2408.03314
- Cited in 03_reinforcement-learning.md regarding test-time compute scaling

**Su, Dan et al. (2024)**
"Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset"
arXiv preprint arXiv:2412.02595
- Cited in 09_appendix-b-pretraining.md regarding data preprocessing

**Su, Jianlin et al. (2024)**
"Roformer: Enhanced transformer with rotary position embedding"
Neurocomputing 568, p. 127063
- Cited in 09_appendix-b-pretraining.md regarding RoPE position embeddings

**Team, Gemini et al. (2024)**
"Gemini: A Family of Highly Capable Multimodal Models"
arXiv:2312.11805 [cs.CL]
URL: https://arxiv.org/abs/2312.11805
- Cited in 09_appendix-b-pretraining.md regarding pretraining data quality

**Tomar, Manan et al. (2020)**
"Mirror descent policy optimization"
arXiv preprint arXiv:2005.09814
- Cited in 03_reinforcement-learning.md regarding mirror descent optimization derivation

**Vaswani, Ashish et al. (2017)**
"Attention is All you Need"
Advances in Neural Information Processing Systems, Vol. 30, Curran Associates, Inc.
URL: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
- Cited in 09_appendix-b-pretraining.md regarding Transformer architecture

**Villalobos, Pablo et al. (2024)**
"Will we run out of data? Limits of LLM scaling based on human-generated data"
arXiv:2211.04325 [cs.LG]
URL: https://arxiv.org/abs/2211.04325
- Cited in 01_introduction.md regarding data scaling limits

**Vinyals, Oriol et al. (2019)**
"Grandmaster level in StarCraft II using multi-agent reinforcement learning"
nature 575.7782, pp. 350–354
- Cited in 04c_infrastructure.md as example of RL success in games

**Wang, Ke et al. (2024)**
"Measuring multimodal mathematical reasoning with math-vision dataset"
arXiv preprint arXiv:2402.14804
- Cited in 06_experiments.md and 10_appendix-c-evaluation-details.md regarding vision-based math benchmarks

**Wei, Haoran et al. (2024)**
"General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model"
arXiv preprint arXiv:2409.01704
- Cited in 09_appendix-b-pretraining.md regarding OCR models for PDF extraction

**Wei, Jason et al. (2022)**
"Chain-of-thought prompting elicits reasoning in large language models"
Advances in neural information processing systems 35, pp. 24824–24837
- Cited in 03_reinforcement-learning.md regarding chain-of-thought reasoning

**Wu, Yangzhen et al. (2024)**
"Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models"
arXiv preprint arXiv:2408.00724
- Cited in 03_reinforcement-learning.md regarding inference scaling laws

**Xu, Liang et al. (2020)**
"CLUE: A Chinese Language Understanding Evaluation Benchmark"
International Conference on Computational Linguistics
URL: https://arxiv.org/abs/2004.05986
- Cited in 06_experiments.md and 10_appendix-c-evaluation-details.md regarding Chinese language benchmarks

**Yang, Enneng et al. (2024)**
"Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities"
arXiv preprint arXiv:2408.07666
- Cited in 04b_training-recipe-long2short-sft.md regarding model merge methods

**Yao, Shunyu et al. (2024)**
"Tree of thoughts: Deliberate problem solving with large language models"
Advances in Neural Information Processing Systems 36
- Cited in 03_reinforcement-learning.md regarding tree search methods

**Yue, Xiang, Yuansheng Ni, et al. (2024)**
"Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi"
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9556–9567
- Cited in 06_experiments.md and 10_appendix-c-evaluation-details.md regarding multimodal understanding benchmarks

**Yue, Xiang, Xingwei Qu, et al. (2023)**
"Mammoth: Building math generalist models through hybrid instruction tuning"
arXiv preprint arXiv:2309.05653
- Cited in 09_appendix-b-pretraining.md regarding math instruction tuning and rejection sampling

**Zhang, Lunjun et al. (2024)**
"Generative verifiers: Reward modeling as next-token prediction, 2024"
URL: https://arxiv.org/abs/2408.15240
- Cited in 03_reinforcement-learning.md regarding generative verifiers providing feedback in language format

**Zheng, Lianmin et al. (2024)**
"SGLang: Efficient Execution of Structured Language Model Programs"
arXiv:2312.07104 [cs.AI]
URL: https://arxiv.org/abs/2312.07104
- Cited in 04c_infrastructure.md regarding structured generation and GPU resource management

**Zhou, Jeffrey et al. (2023)**
"Instruction-Following Evaluation for Large Language Models"
ArXiv abs/2311.07911
URL: https://arxiv.org/abs/2311.07911
- Cited in 06_experiments.md and 10_appendix-c-evaluation-details.md regarding instruction-following evaluation

**Zhu, Wanrong et al. (2024)**
"Multimodal c4: An open, billion-scale corpus of images interleaved with text"
Advances in Neural Information Processing Systems 36
- Cited in 09_appendix-b-pretraining.md regarding multimodal corpus
