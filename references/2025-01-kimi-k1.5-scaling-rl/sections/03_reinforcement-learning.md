# 2.3 Reinforcement Learning [p. 4-6]

## 2.3.1 Problem Setting [p. 4]

Given a training dataset $\mathcal{D} = \{(x_i, y_i^*)\}_{i=1}^n$ of problems $x_i$ and corresponding ground truth answers $y_i^*$, our goal is to train a policy model $\pi_\theta$ to accurately solve test problems. In the context of complex reasoning, the mapping of problem $x$ to solution $y$ is non-trivial. To tackle this challenge, the *chain of thought* (CoT) method proposes to use a sequence of intermediate steps $z = (z_1, z_2, \ldots, z_m)$ to bridge $x$ and $y$, where each $z_i$ is a coherent sequence of tokens that acts as a significant intermediate step toward solving the problem (J. Wei et al. 2022). When solving problem $x$, thoughts $z_t \sim \pi_\theta(\cdot|x, z_1, \ldots, z_{t-1})$ are auto-regressively sampled, followed by the final answer $y \sim \pi_\theta(\cdot|x, z_1, \ldots, z_m)$. We use $y, z \sim \pi_\theta$ to denote this sampling procedure. Note that both the thoughts and final answer are sampled as a language sequence.

To further enhance the model's reasoning capabilities, *planning* algorithms are employed to explore various thought processes, generating improved CoT at inference time (Yao et al. 2024; Y. Wu et al. 2024; Snell et al. 2024). The core insight of these approaches is the explicit construction of a search tree of thoughts guided by value estimations. This allows the model to explore diverse continuations of a thought process or backtrack to investigate new directions when encountering dead ends. In more detail, let $\mathcal{T}$ be a search tree where each node represents a partial solution $s = (x, z_{1:|s|})$. Here, $s$ consists of the problem $x$ and a sequence of thoughts $z_{1:|s|} = (z_1, \ldots, z_{|s|})$ leading up to that node, with $|s|$ denoting number of thoughts in the sequence. The planning algorithm uses a critic model $v$ to provide feedback $v(x, z_{1:|s|})$, which helps evaluate the current progress towards solving the problem and identify any errors in the existing partial solution. We note that the feedback can be provided by either a discriminative score or a language sequence (L. Zhang et al. 2024). Guided by the feedbacks for all $s \in \mathcal{T}$, the planning algorithm selects the most promising node for expansion, thereby growing the search tree. The above process repeats iteratively until a full solution is derived.

We can also approach planning algorithms from an *algorithmic perspective*. Given past search history available at the $t$-th iteration $(s_1, v(s_1), \ldots, s_{t-1}, v(s_{t-1}))$, a planning algorithm $\mathcal{A}$ iteratively determines the next search direction $\mathcal{A}(s_t|s_1, v(s_1), \ldots, s_{t-1}, v(s_{t-1}))$ and provides feedbacks for the current search progress $\mathcal{A}(v(s_t)|s_1, v(s_1), \ldots, s_t)$. Since both thoughts and feedbacks can be viewed as intermediate reasoning steps, and these components can both be represented as sequence of language tokens, we use $z$ to replace $s$ and $v$ to simplify the notations. Accordingly, we view a planning algorithm as a mapping that directly acts on a sequence of reasoning steps $\mathcal{A}(\cdot|z_1, z_2, \ldots)$. In this framework, all information stored in the search tree used by the planning algorithm is flattened into the full context provided to the algorithm. This provides an intriguing perspective on generating high-quality CoT: Rather than explicitly constructing a search tree and implementing a planning algorithm, we could potentially train a model to approximate this process. Here, the number of thoughts (i.e., language tokens) serves as an analogy to the computational budget traditionally allocated to planning algorithms. Recent advancements in long context windows facilitate seamless scalability during both the training and testing phases. If feasible, this method enables the model to run an implicit search over the reasoning space directly via auto-regressive predictions. Consequently, the model not only learns to solve a set of training problems but also develops the ability to tackle individual problems effectively, leading to improved generalization to unseen test problems.

We thus consider training the model to generate CoT with reinforcement learning (RL) (OpenAI 2024). Let $r$ be a reward model that justifies the correctness of the proposed answer $y$ for the given problem $x$ based on the ground truth $y^*$, by assigning a value $r(x, y, y^*) \in \{0, 1\}$. For verifiable problems, the reward is directly determined by predefined criteria or rules. For example, in coding problems, we assess whether the answer passes the test cases. For problems with free-form ground truth, we train a reward model $r(x, y, y^*)$ that predicts if the answer matches the ground truth. Given a problem $x$, the model $\pi_\theta$ generates a CoT and the final answer through the sampling procedure $z \sim \pi_\theta(\cdot|x)$, $y \sim \pi_\theta(\cdot|x, z)$. The quality of the generated CoT is evaluated by whether it can lead to a correct final answer. In summary, we consider the following objective to optimize the policy

$$\max_\theta \mathbb{E}_{(x,y^*)\sim\mathcal{D},(y,z)\sim\pi_\theta} [r(x, y, y^*)] \,. \tag{1}$$

By scaling up RL training, we aim to train a model that harnesses the strengths of both simple prompt-based CoT and planning-augmented CoT. The model still auto-regressively sample language sequence during inference, thereby circumventing the need for the complex parallelization required by advanced planning algorithms during deployment. However, a key distinction from simple prompt-based methods is that the model should not merely follow a series of reasoning steps. Instead, it should also learn critical planning skills including error identification, backtracking and solution refinement by leveraging the entire set of explored thoughts as contextual information.

## 2.3.2 Policy Optimization [p. 5]

We apply a variant of online policy mirror decent as our training algorithm (Abbasi-Yadkori et al. 2019; Mei et al. 2019; Tomar et al. 2020). The algorithm performs iteratively. At the $i$-th iteration, we use the current model $\pi_{\theta_i}$ as a reference model and optimize the following relative entropy regularized policy optimization problem,

$$\max_\theta \mathbb{E}_{(x,y^*)\sim\mathcal{D}} \left[ \mathbb{E}_{(y,z)\sim\pi_\theta} [r(x, y, y^*)] - \tau \text{KL}(\pi_\theta(x)||\pi_{\theta_i}(x)) \right] \,, \tag{2}$$

where $\tau > 0$ is a parameter controlling the degree of regularization. This objective has a closed form solution

$$\pi^*(y, z|x) = \pi_{\theta_i}(y, z|x) \exp(r(x, y, y^*)/\tau)/Z \,.$$

Here $Z = \sum_{y',z'} \pi_{\theta_i}(y', z'|x) \exp(r(x, y', y^*)/\tau)$ is the normalization factor. Taking logarithm of both sides we have for *any* $(y, z)$ the following constraint is satisfied, which allows us to leverage off-policy data during optimization

$$r(x, y, y^*) - \tau \log Z = \tau \log \frac{\pi^*(y, z|x)}{\pi_{\theta_i}(y, z|x)} \,.$$

This motivates the following surrogate loss

$$L(\theta) = \mathbb{E}_{(x,y^*)\sim\mathcal{D}} \left[ \mathbb{E}_{(y,z)\sim\pi_{\theta_i}} \left[ \left( r(x, y, y^*) - \tau \log Z - \tau \log \frac{\pi_\theta(y, z|x)}{\pi_{\theta_i}(y, z|x)} \right)^2 \right] \right] \,.$$

To approximate $\tau \log Z$, we use samples $(y_1, z_1), \ldots, (y_k, z_k) \sim \pi_{\theta_i}$: $\tau \log Z \approx \tau \log \frac{1}{k} \sum_{j=1}^k \exp(r(x, y_j, y^*)/\tau)$. We also find that using empirical mean of sampled rewards $\bar{r} = \text{mean}(r(x, y_1, y^*), \ldots, r(x, y_k, y^*))$ yields effective practical results. This is reasonable since $\tau \log Z$ approaches the expected reward under $\pi_{\theta_i}$ as $\tau \rightarrow \infty$. Finally, we conclude our learning algorithm by taking the gradient of surrogate loss. For each problem $x$, $k$ responses are sampled using the reference policy $\pi_{\theta_i}$, and the gradient is given by

$$\frac{1}{k} \sum_{j=1}^k \left( \nabla_\theta \log \pi_\theta(y_j, z_j|x)(r(x, y_j, y^*) - \bar{r}) - \frac{\tau}{2} \nabla_\theta \left( \log \frac{\pi_\theta(y_j, z_j|x)}{\pi_{\theta_i}(y_j, z_j|x)} \right)^2 \right) \,. \tag{3}$$

To those familiar with policy gradient methods, this gradient resembles the policy gradient of (2) using the mean of sampled rewards as the baseline (Kool et al. 2019; Ahmadian et al. 2024). The main differences are that the responses are sampled from $\pi_{\theta_i}$ rather than on-policy, and an $\ell_2$-regularization is applied. Thus we could see this as the natural extension of a usual on-policy regularized policy gradient algorithm to the off-policy case (Nachum et al. 2017). We sample a batch of problems from $\mathcal{D}$ and update the parameters to $\theta_{i+1}$, which subsequently serves as the reference policy for the next iteration. Since each iteration considers a different optimization problem due to the changing reference policy, we also reset the optimizer at the start of each iteration.

We exclude the value network in our training system which has also been exploited in previous studies (Ahmadian et al. 2024). While this design choice significantly improves training efficiency, we also hypothesize that the conventional use of value functions for credit assignment in classical RL may not be suitable for our context. Consider a scenario where the model has generated a partial CoT $(z_1, z_2, \ldots, z_t)$ and there are two potential next reasoning steps: $z_{t+1}$ and $z'_{t+1}$. Assume that $z_{t+1}$ directly leads to the correct answer, while $z'_{t+1}$ contains some errors. If an oracle value function were accessible, it would indicate that $z_{t+1}$ preserves a higher value compared to $z'_{t+1}$. According to the standard credit assignment principle, selecting $z'_{t+1}$ would be penalized as it has a negative advantages relative to the current policy. However, exploring $z'_{t+1}$ is extremely valuable for training the model to generate long CoT. By using the justification of the final answer derived from a long CoT as the reward signal, the model can learn the pattern of trial and error from taking $z'_{t+1}$ as long as it successfully recovers and reaches the correct answer. The key takeaway from this example is that we should encourage the model to explore diverse reasoning paths to enhance its capability in solving complex problems. This exploratory approach generates a wealth of experience that supports the development of critical planning skills. Our primary goal is not confined to attaining high accuracy on training problems but focuses on equipping the model with effective problem-solving strategies, ultimately improving its performance on test problems.

## 2.3.3 Length Penalty [p. 5-6]

We observe an overthinking phenomenon that the model's response length significantly increases during RL training. Although this leads to better performance, an excessively lengthy reasoning process is costly during training and inference, and overthinking is often not preferred by humans. To address this issue, we introduce a length reward to restrain the rapid growth of token length, thereby improving the model's token efficiency. Given $k$ sampled responses
