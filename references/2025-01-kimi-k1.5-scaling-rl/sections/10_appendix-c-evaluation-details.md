# Appendix C: Evaluation Details [p. 24â€“25]

## C.1 Text Benchmark [p. 24]

**MMLU** (Hendrycks et al. 2020) [p. 24]:
- Covers 57 subjects in STEM, the humanities, social sciences, and more [p. 24]
- Ranges in difficulty from an elementary level to an advanced professional level [p. 24]
- Tests both world knowledge and problem-solving ability [p. 24]

**IF-Eval** (J. Zhou et al. 2023) [p. 24]:
- A benchmark for evaluating large language models' ability to follow verifiable instructions [p. 24]
- Contains 500+ prompts with instructions such as "write an article with more than 800 words", etc. [p. 24]
- **Note:** Due to a version shift, the number of IFEval reported in Table 3 derived from an intermediate model [p. 24]. The authors will update the scores based on the final model [p. 24]

**CLUEWSC** (L. Xu et al. 2020) [p. 24]:
- A coreference resolution task in CLUE benchmark [p. 24]
- Requires models to determine if a pronoun and a noun phrase in a sentence co-refer [p. 24]
- Data from Chinese fiction books [p. 24]

**C-EVAL** (Y. Huang et al. 2023) [p. 24]:
- A comprehensive Chinese evaluation suite for assessing advanced knowledge and reasoning abilities of foundation models [p. 24]
- Includes 13,948 multiple-choice questions across 52 disciplines and four difficulty levels [p. 24]

## C.2 Reasoning Benchmark [p. 25]

**HumanEval-Mul** [p. 25]:
- A subset of Multipl-E (Cassano, Gouwar, D. Nguyen, S. D. Nguyen, et al. 2022) [p. 25]
- MultiPL-E extends the HumanEval benchmark and MBPP benchmark to 18 languages that encompass a range of programming paradigms and popularity [p. 25]
- The authors choose HumanEval translations in 8 mainstream programming languages (Python, Java, Cpp, C#, JavaScript, TypeScript, PHP, and Bash) [p. 25]

**LiveCodeBench** (Jain et al. 2024) [p. 25]:
- Serves as a comprehensive and contamination-free benchmark for assessing large language models (LLMs) in coding tasks [p. 25]
- Features:
  - Live updates to prevent data contamination [p. 25]
  - Holistic evaluation across multiple coding scenarios [p. 25]
  - High-quality problems and tests [p. 25]
  - Balanced problem difficulty [p. 25]
- **Testing protocol:**
  - Short-CoT model tested with questions from 2408-2411 (release v4) [p. 25]
  - Long-CoT model tested with questions from 2412-2502 (release v5) [p. 25]

**AIME 2024** [p. 25]:
- Comprises the competition questions for the AIME in 2024 [p. 25]
- The AIME is a prestigious, invitation-only math contest for top high school students [p. 25]
- Assesses advanced math skills and requires solid foundation and high logical thinking [p. 25]

**MATH-500** (Lightman et al. 2023) [p. 25]:
- A comprehensive mathematics benchmark that contains 500 problems on various mathematics topics including algebra, calculus, probability, and more [p. 25]
- Tests both computational ability and mathematical reasoning [p. 25]
- Higher scores indicate stronger mathematical problem-solving capabilities [p. 25]

**Codeforces** [p. 25]:
- A well-known online judge platform and serves as a popular testbed for evaluating long-CoT coding models [p. 25]
- **Evaluation methodology:** To achieve higher rankings in the Div2 and Div3 competitions, the authors utilize majority voting on the code snippets generated by the k1.5 long-CoT model, employing test cases that are also generated by the same model [p. 25]
- The percentile of the codeforce ELO rating was extracted from OpenAI Day12 talk (see footnote 3) [p. 25]

## C.3 Image Benchmark [p. 25]

**MMMU** (Yue, Ni, et al. 2024) [p. 25]:
- Encompasses a carefully curated collection of 11.5K multimodal questions sourced from college exams, quizzes, and textbooks [p. 25]
- These questions span six major academic fields [p. 25]:
  1. Art & Design
  2. Business
  3. Science
  4. Health & Medicine
  5. Humanities & Social Science
  6. Tech & Engineering

**MATH-Vision (MATH-V)** (K. Wang et al. 2024) [p. 25]:
- A carefully curated collection of 3,040 high-quality mathematical problems with visual contexts that are sourced from real math competitions [p. 25]
- Covers 16 distinct mathematical disciplines [p. 25]
- Graded across 5 levels of difficulty [p. 25]
- This dataset offers a comprehensive and diverse set of challenges, making it ideal for evaluating the mathematical reasoning abilities of LMMs [p. 25]

**MathVista** (Lu et al. 2023) [p. 25]:
- A benchmark that integrates challenges from a variety of mathematical and visual tasks [p. 25]
- Demands participants to exhibit fine-grained, deep visual understanding along with compositional reasoning to successfully complete the tasks [p. 25]
