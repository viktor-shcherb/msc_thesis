# Appendix H. Revision Model Finetuning Details [p. 25â€“26]

[p. 25] For fine-tuning the revision model, the authors follow the procedure outlined in Section 6.1. They first sample 64 outputs per question. They then filter out all answers which end in an invalid solution. For each correct answer, they then sample a number uniformly between 0 and 4 indicating how many incorrect answers to include in context for training. The correct answer is used as the last answer in the trajectory (which they train the model to produce) and the incorrect answers are included in context. If the sampled number is greater than 0, they then find the closest incorrect answer according to a character-level edit distance metric to include as the last incorrect answer in the trajectory. The goal here is to select an incorrect answer which is somewhat correlated with the correct answer, to improve learning. The remaining incorrect answers, they sample randomly from the set of available answers. In the case where there are fewer than 4 incorrect answers sampled, they truncate the uniform distribution's max to match the number of incorrect samples. They use this procedure to generate trajectories for all questions in the training data.

[p. 26] They then finetune the base language model on the correct answer solutions in these generated trajectories. They use the AdamW optimizer with lr 1e-5, batch size 128, dropout 0.0, and Adam betas (0.9, 0.95).

[p. 26] They find that generally evaluating loss on an evaluation set consisting of trajectories generated as described above, does not provide a good signal for early stopping. Rather, they find that checkpoints much after the evaluation loss begins increasing perform more capable of revisions. This is likely because after finetuning the revision model, the evaluation set represents off-policy data, which will naturally be out-of-distribution compared to the trajectories that the model itself would generate on-policy. They therefore select their revision model checkpoint slightly after the point where they observe overfitting on the validation set.
