# 7. Putting it Together: Exchanging Pretraining and Test-Time Compute [p. 14-15]

[p. 14] So far, the paper showed that utilizing additional test-time computation can enable the model to represent more complex distributions than the one predicted by the base LLM itself, thereby improving performance. The authors now posit that this increased flexibility of representing distributions means that additional test-time compute can be expected to make up for the lack of a higher-capacity model or training for more FLOPs during pre-training. In this section, they study to what extent this is possible. They pose the following question:

**Question: Exchanging pretraining and test-time compute** (boxed text, p. 14):
> "Suppose a model was pre-trained with *X* FLOPs. Assume that we plan to run *Y* FLOPs of inference with this model. If we want to improve performance by increasing the total FLOPs budget by a factor of *M* (i.e., *M*(*X* + *Y*) total FLOPs across both pretraining and inference), should we spend our FLOPs on increased pretraining compute or on additional test-time compute?" [p. 14]

[p. 14] Increasing pretraining FLOPs introduces the additional design decision of whether to allocate compute to training with more data or more parameters [14]. The authors focus on the setting in which model parameters are scaled up and training data amount is fixed, matching the approach taken with the open-source LLaMA series of models [41]. They choose this setting as it is representative of a canonical approach to scaling pretraining compute and leave the analysis of compute-optimal scaling of pretraining compute [29] where the data and parameters are both scaled equally to future work.

## Defining an exchange rate between FLOPs

[p. 14] To determine pretraining FLOPs, the authors use the common approximation *X* = 6*ND*_pretrain [14], and for inference FLOPs, they use *Y* = 2*ND*_inference [29]. Here *N* represents model parameters, *D*_pretrain is the number of tokens used for pretraining, and *D*_inference the total number of tokens generated at inference time. With these approximations, if the model parameters are multiplied by a factor of *M*, then both the pretraining and inference FLOPs (due to the cost of greedy decoding with the larger model) increase by a factor of *M* (giving *M*(*X* + *Y*) total FLOPs).

To match the FLOPs from scaling up model parameters using test-time compute with the smaller model, the smaller model's inference compute can be multiplied by a factor of *M* + 3(*D*_pretrain / *D*_inference)(*M* - 1). Notably, the amount of inference compute that can be utilized to match the FLOPs for the larger model depends on the ratio *D*_pretrain / *D*_inference. The authors refer to the inverse of this ratio as *R* (e.g., *R* = *D*_inference / *D*_pretrain). [p. 14]

Depending on the specific production setting or use-case, very different values of *R* are expected. In particular, in many large scale production settings, there may be significantly more inference tokens than pretraining tokens, in which case *R* >> 1. On the other hand, in many contemporary self-improvement setups that would use test-time compute to improve the model, significantly fewer inference tokens would likely be generated, giving *R* << 1. Therefore, since the scale of test-time compute that can be applied is dependent on this ratio, the authors expect differing conclusions depending on the specific setting. [p. 14-15]

[p. 15] In Figure 9, the authors use this approach to exchange test-time and pretraining compute, comparing their compute-optimal scaling against scaling up model parameters by a factor of ~14. They conduct comparisons for 3 different values of *R*: **0.16** (*R* << 1), **0.79** (*R* ~ 1), and **22** (*R* >> 1), with each ratio corresponding to an inference budget. Observations:
- If only very difficult questions (e.g., difficulty bins 4/5) are expected or there is a larger *D*_inference (corresponding to a larger *R* value), then it is often more effective to allocate the budget towards pretraining (e.g., the star is above the line).
- If instead mostly easy or intermediate difficulty questions (e.g., bins 1/2/3 and sometimes 4) are expected or there are lower inference requirements (as is the case in self-improvement pipelines), then utilizing test-time compute is better. [p. 15]

**Figure 9** (p. 15): *"Tradeoff between pretraining and test-time compute in a FLOPs-matched evaluation. Each line represents the performance of scaling test-time compute with our compute-optimal policy in each oracle difficulty bin. We plot the results for revisions on the left and search on the right. The stars represent the greedy pass@1 performance of a base model pretrained with ~ 14 times more parameters. We plot test-time compute budget on the x-axis, and place the stars at three different locations along the x-axis, each corresponding to the FLOPs equivalent point of comparison between scaling parameters and scaling test-time compute for three different inference compute loads (e.g. R = D_inference / D_pretrain). If the star is below the line, this implies that it is more effective to use test-time compute than to scale model parameters, and if the star is above the line this implies that scaling parameters is more effective. We see that on the easy questions or in settings with a lower inference compute load (e.g. R << 1), test-time compute can generally outperform scaling model parameters. However, on the harder questions or in settings with a higher inference load (e.g. R >> 1), pretraining is a more effective way to improve performance."*

The figure has two panels, both titled "Comparing Test-time and Pretraining Compute." Left panel: "Revisions." X-axis: "Proportional to Inference FLOPs" (2^0 to 2^9). Y-axis: "MATH Difficulty Level Accuracy (%)" (0 to 100). Five colored lines represent difficulty levels 1-5, with difficulty level 1 (easiest) reaching ~95% and difficulty level 5 (hardest) remaining near ~20%. Stars mark the greedy pass@1 performance of the ~14x larger model at three positions along the x-axis corresponding to *R* << 1, *R* ~ 1, and *R* >> 1. Three vertical dashed lines indicate these *R* values. On easy questions or with *R* << 1, the test-time compute lines are above the stars; on hard questions or with *R* >> 1, the stars are above the lines. Right panel: "PRM Search." Same axes and layout. Similar trends, though the curves differ in absolute values.

**Takeaways for exchanging pretraining and test-time compute** (boxed text, p. 15):
> "Test-time and pretraining compute are not 1-to-1 'exchangeable'. On easy and medium questions, which are within a model's capabilities, or in settings with small inference requirement, test-time compute can easily cover up for additional pretraining. However, on challenging questions which are outside a given base model's capabilities or under higher inference requirement, pretraining is likely more effective for improving performance." [p. 15]
