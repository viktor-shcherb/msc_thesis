# 5. Scaling Test-Time Compute via Verifiers [p. 6]

[p. 6] This section analyzes how test-time compute can be scaled by optimizing a verifier, as effectively as possible. The authors study different approaches for performing test-time search with process verifiers (PRMs) and analyze the test-time compute scaling properties of these different approaches.

## 5.1. Training Verifiers Amenable to Search

[p. 6-7] **PRM training.** Originally PRM training [22, 42] used human crowd-worker labels. While Lightman et al. [22] released their PRM training data (i.e., the PRM800k dataset), the authors found this data to be largely ineffective for them. It was easy to exploit a PRM trained on this dataset via even naive strategies such as best-of-N sampling. The authors hypothesize that this is likely a result of the distribution shift between the GPT-4 generated samples in the Lightman et al. dataset and their PaLM 2 models. [p. 7]

Rather than proceeding with the expensive process of collecting crowd-worker PRM labels for their PaLM 2 models, the authors instead apply the approach of Wang et al. [45] to supervise PRMs without human labels, using estimates of per-step correctness obtained from running **Monte Carlo rollouts** from each step in the solution. The PRM's per-step predictions therefore correspond to value estimates of reward-to-go for the base model's sampling policy, similar to recent work [31, 45]. They also compared to an ORM baseline (Appendix F) but found that their PRM consistently outperforms the ORM. Hence, all of the search experiments in this section use a PRM model. Additional details on PRM training are shown in Appendix D. [p. 7]

**Answer aggregation.** At test time, process-based verifiers can be used to score each individual step in a set of solutions sampled from the base model. To select the best-of-N answers with the PRM, a function is needed that can aggregate across all the per-step scores for each answer to determine the best candidate for the correct answer. This is done by first aggregating each individual answer's per-step scores to obtain a final score for the full answer (step-wise aggregation), then aggregating across answers to determine the best answer (inter-answer aggregation). [p. 7]

- **Step-wise aggregation.** Rather than aggregating the per-step scores by taking the product or minimum [22, 45], the authors instead use the PRM's prediction at the last step as the full-answer score. They found this to perform the best out of all aggregation methods studied (see Appendix E). [p. 7]
- **Inter-answer aggregation.** Following Li et al. [21], the authors apply "best-of-N weighted" selection rather than standard best-of-N. Best-of-N weighted selection marginalizes the verifier's correctness scores across all solutions with the same final answer, selecting the final answer with the greatest total sum. [p. 7]

## 5.2. Search Methods Against a PRM

[p. 7] The authors optimize the PRM at test time via search methods. Three search approaches are studied that sample outputs from a few-shot prompted base LLM (see Appendix G). An illustration is shown in Figure 2.

**Best-of-N weighted.** N answers are sampled independently from the base LLM and then the best answer is selected according to the PRM's final answer judgement. [p. 7]

**Beam search.** Beam search optimizes the PRM by searching over its per-step predictions. The implementation is similar to BFS-V [10, 48]. Concretely, a fixed number of beams *N* and a beam width *M* are considered. The algorithm proceeds as follows: [p. 7]

1. Sample *N* initial predictions for the first step in the solution
2. Score the generated steps according to the PRM's predicted step-wise reward-to-go estimate (which also corresponds to the total reward from the prefix since the reward is sparse in this setting)
3. Filter for only the top *N/M* highest scoring steps
4. From each candidate, sample *M* proposals from the next step, resulting in a total of *N/M* x *M* candidate prefixes. Then repeat steps 2-4.

The algorithm runs until the end of a solution or the maximum number of rounds of beam expansion are attained (40 in their case). The search concludes with N final answer candidates, to which best-of-N weighted selection is applied to make the final answer prediction. [p. 7]

**Figure 2** (p. 8): *"Comparing different PRM search methods. Left: Best-of-N samples N full answers and then selects the best answer according to the PRM final score. Center: Beam search samples N candidates at each step, and selects the top M according to the PRM to continue the search from. Right: lookahead-search extends each step in beam-search to utilize a k-step lookahead while assessing which steps to retain and continue the search from. Thus lookahead-search needs more compute."*

The figure shows three tree diagrams side-by-side illustrating the three search methods. Best-of-N generates complete solutions in parallel and selects the best. Beam search prunes at each step keeping top N/M candidates then expands M ways. Lookahead search adds rollout simulations at each step before pruning. A key shows: circle-with-lines = Apply Verifier, yellow circle = Full Solution, orange circle = Intermediate solution step, green circle = Selected by verifier, red circle = Rejected by verifier.

[p. 8] **Lookahead search.** Lookahead search modifies how beam search evaluates individual steps. It uses lookahead rollouts to improve the accuracy of the PRM's value estimation in each step of the search process. Specifically, at each step in the beam search, rather than using the PRM score at the current step to select the top candidates, lookahead search performs a simulation, rolling out up to *k* steps further while stopping early if the end of the solution is reached. To minimize variance in the simulation rollout, rollouts are performed using temperature 0. The PRM's prediction at the end of this rollout is then used to score the current step in the beam search. That is, beam search can be viewed as a special case of lookahead search with *k* = 0. Given an accurate PRM, increasing *k* should improve the accuracy of the per-step value estimates at the cost of additional compute. [p. 8]

The authors also note that this version of lookahead search is a special case of MCTS [38], wherein the stochastic elements of MCTS, designed to facilitate exploration, are removed since the PRM is already trained and is frozen. These stochastic elements are largely useful for learning the value function (which they have already learned with their PRM), but less useful at test-time when the goal is to exploit rather than explore. Therefore, lookahead search is largely representative of how MCTS-style methods would be applied at test-time. [p. 8]

## 5.3. Analysis Results: Test-Time Scaling for Search with Verifiers

[p. 8-9] **Comparing search algorithms.** The authors first conduct a sweep over various search settings. In addition to the standard best-of-N approach, they sweep over the two main parameters that distinguish different tree-search methods: beam-width *M* and number of lookahead steps *k*. While not able to extensively sweep every single configuration, they sweep over the following settings with a maximum budget of 256: [p. 8-9]

1. Beam search with the beam width set to sqrt(*N*), where *N* is the generation budget.
2. Beam search with a fixed beam width of 4.
3. Lookahead search with *k* = 3 applied to both beam-search settings 1) and 2).
4. Lookahead search with *k* = 1 applied to beam-search setting 1).

[p. 9] To compare search methods as a function of generation budget fairly, a protocol for estimating the cost of each method is built. A generation is considered to be a sampled answer from the base LLM. For beam search and best-of-N the generation budget corresponds to the number of beams and *N* respectively. Lookahead search, however, utilizes additional compute: at each step of the search, *k* additional steps are sampled ahead. Therefore, the cost of lookahead-search is defined to be *N* x (*k* + 1) samples. [p. 9]

**Figure 3** (p. 9): *"Left: Comparing different methods for conducting search against PRM verifiers. We see that at low generation budgets, beam search performs best, but as we scale the budget further the improvements diminish, falling below the best-of-N baseline. Lookahead-search generally underperforms other methods at the same generation budget. Right: Comparing beam search and best-of-N binned by difficulty level. The four bars in each difficulty bin correspond to increasing test-time compute budgets (4, 16, 64, and 256 generations). On the easier problems (bins 1 and 2), beam search shows signs of over-optimization with higher budgets, whereas best-of-N does not. On the medium difficulty problems (bins 3 and 4), we see beam search demonstrating consistent improvements over best-of-N."*

Left panel: A line plot with x-axis "Generation Budget" (2^1 to 2^8) and y-axis "MATH Test Accuracy (%)" (10 to 40). Lines shown: Best-of-N Weighted, Majority, Beam M = sqrt(N), Beam M = 4, 1 Step Lookahead M = sqrt(N), 3 Step Lookahead M = sqrt(N), 3 Step Lookahead M = 4. At low budgets, beam search (both M settings) leads; at high budgets (~256), best-of-N weighted catches up and surpasses beam search variants. Lookahead methods generally underperform. Right panel: A grouped bar chart with x-axis "Test Questions Binned by Increasing Difficulty Level" (1 to 5) and y-axis "MATH Test Accuracy (%)" (0 to 80). Three bar colors: Beam Search, Best-of-N Weighted, Majority. Each difficulty bin has four bars per method for budgets 4, 16, 64, 256. On easy bins (1-2), beam search degrades at higher budgets. On medium bins (3-4), beam search consistently outperforms best-of-N.

[p. 9] **Results.** As shown in Figure 3 (left), with smaller generation budgets, beam search significantly outperforms best-of-N. However, as the budget is scaled up, these improvements greatly diminish, with beam search often underperforming the best-of-N baseline. Lookahead-search generally underperforms other methods at the same generation budget, likely due to the additional computation inducted by simulating the lookahead rollouts. The diminishing returns from search are likely due to exploitation of the PRM's predictions. For example, in some instances (such as in Figure 29), search causes the model to generate low-information repetitive steps at the end of a solution. In other cases, over-optimizing search can result in overly short solutions consisting of just 1-2 steps. This explains why the most powerful search method (i.e., lookahead search) underperforms the most. Several of these examples are included in Appendix M. [p. 9]

**Which problems does search improve?** To understand how to compute-optimally scale search methods, a difficulty bin analysis is conducted. Specifically, beam-search (*M* = 4) is compared against best-of-N. In Figure 3 (right), while in aggregate beam search and best-of-N perform similarly with a high generation budget, evaluating their efficacy over difficulty bins reveals very different trends. On the easy questions (levels 1 and 2), the stronger optimizer of the two approaches, beam search, degrades performance as the generation budget increases, suggesting signs of exploitation of the PRM signal. In contrast, on the harder questions (levels 3 and 4), beam search consistently outperforms best-of-N. On the most difficult questions (level 5), no method makes much meaningful progress. [p. 9]

[p. 10] These findings match intuition: on the easy questions, the verifier will make mostly correct assessments of correctness. Therefore, by applying further optimization via beam search, we only further amplify any spurious features learned by the verifier, causing performance degradation. On the more difficult questions, the base model is much less likely to sample the correct answer in the first place, so search can serve to help guide the model towards producing the correct answer more often. [p. 10]

**Compute-optimal search.** Given the above results, it is clear that question difficulty can be a useful statistic to predict the optimal search strategy to use at a given compute budget. Additionally, the best choice of search strategy can vary drastically as a function of this difficulty statistic. The authors visualize the "compute-optimal" scaling trend, as represented by the best performing search strategy at each difficulty level in Figure 4. [p. 10]

In the low generation budget regime, using both the oracle and predicted difficulty, **compute-optimal scaling can nearly outperform best-of-N using up to 4x less test-time compute** (e.g. 16 versus 64 generations). While in the higher budget regime, some of these benefits diminish with the use of predicted difficulty, with oracle bins they still see continued improvements from optimally scaling test-time compute. This result demonstrates the performance gains that could be obtained by adaptively allocating test-time compute during search. [p. 10]

**Figure 4** (p. 10): *"Comparing compute-optimal test-time compute allocation against baselines with PRM search. By scaling test time compute per the notion of question difficulty, we find that we can nearly outperform PRM best-of-N using up to 4x less test-time compute (e.g. 16 verses 64 generations). 'Compute-optimal oracle' refers to using oracle difficulty bins derived from the groundtruth correctness information, and 'compute-optimal predicted' refers to using the PRM's predictions to generate difficulty bins. Observe that the curves with either type of difficulty bins largely overlap with each other."*

The figure is a line plot titled "Compute Optimal Search" with x-axis "Generation Budget" (2^1 to 2^8) and y-axis "MATH Test Accuracy (%)" (10 to 40). Five lines are shown: Majority, ORM Best-of-N Weighted, PRM Best-of-N Weighted, PRM Compute Optimal Oracle, PRM Compute Optimal Predicted. The two compute-optimal curves (oracle and predicted) largely overlap and rise steeply at low budgets, nearly matching or exceeding the PRM Best-of-N Weighted line at approximately 4x fewer generations. All PRM-based methods outperform ORM Best-of-N Weighted and Majority.

**Takeaways for compute-optimal scaling of verifiers** (boxed text, p. 10):
> "We find that the efficacy of any given verifier search method depends critically on both the compute budget and the question at hand. Specifically, beam-search is more effective on harder questions and at lower compute budgets, whereas best-of-N is more effective on easier questions and at higher budgets. Moreover, by selecting the best search setting for a given question difficulty and test-time compute budget, we can nearly outperform best-of-N using up to 4x less test-time compute." [p. 10]
