# Appendix D. PRM Training Details [p. 22]

[p. 22] The authors finetune their PRM as a binary classifier, where the model predicts a value between 0 and 1 at each step in the solution. They train the model with soft values obtained from the monte-carlo rollouts, using a binary cross entropy loss function (e.g. $-(y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}))$ where $y$ corresponds to the soft ground-truth value and $\hat{y}$ the model prediction). They finetune the model base using the AdamW optimizer, with lr 3e-5, batch size 128, dropout 0.05, and Adam betas (0.9, 0.95). They conduct early stopping, selecting the checkpoint with the lowest validation loss on a random held-out validation set, consisting of 10% of the questions in the original PRM800k training split.

[p. 22] The authors finetune the PRM on 16 samples per question from the corresponding few-shot prompted base model. At each step, they use 16 monte-carlo rollouts, using the same base model and prompt, to estimate the step-level value. They filter out all samples which fail to output a valid, parsable final answer from the training data, as they found these to hurt PRM performance in initial experiments.
