# Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters

**Authors:** Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar
**Affiliations:** Charlie Snell (UC Berkeley, ♦work done during an internship at Google DeepMind); Jaehoon Lee (Google DeepMind); Kelvin Xu (♣Google DeepMind, equal advising); Aviral Kumar (♣Google DeepMind, equal advising)

## Publication Status

- **arXiv preprint:** August 2024, arXiv:2408.03314
- **Peer-reviewed:** Yes
- **Conference:** International Conference on Learning Representations (ICLR), 2025 (Oral presentation)
- **Status:** Published conference paper

## Preferred Citation

Cite the ICLR 2025 version:

> Snell, C., Lee, J., Xu, K., & Kumar, A. (2025). Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters. In International Conference on Learning Representations.

## Links

- arXiv: https://arxiv.org/abs/2408.03314
- OpenReview: https://openreview.net/forum?id=4FWAwZtd2n
- ICLR Virtual: https://iclr.cc/virtual/2025/oral/31924
