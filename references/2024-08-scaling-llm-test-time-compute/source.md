# Scaling LLM Test-Time Compute Optimally Can Be More Effective Than Scaling Parameters for Reasoning

**Authors:** Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar
**Affiliations:** Charlie Snell (UC Berkeley, *work done during an internship at Google DeepMind); Jaehoon Lee (Google DeepMind); Kelvin Xu (Google DeepMind, equal advising); Aviral Kumar (CMU / Google DeepMind, equal advising)

## Publication Status

- **arXiv preprint:** August 2024, arXiv:2408.03314
- **Peer-reviewed:** Yes
- **Conference:** International Conference on Learning Representations (ICLR), 2025, April 24--28, Singapore (Oral presentation)
- **Status:** Published conference paper

## Preferred Citation

Cite the ICLR 2025 version:

> Snell, C., Lee, J., Xu, K., & Kumar, A. (2025). Scaling LLM Test-Time Compute Optimally Can Be More Effective Than Scaling Parameters for Reasoning. In International Conference on Learning Representations.

## Links

- arXiv: https://arxiv.org/abs/2408.03314
- OpenReview: https://openreview.net/forum?id=4FWAwZtd2n
- Proceedings: https://proceedings.iclr.cc/paper_files/paper/2025/file/1b623663fd9b874366f3ce019fdfdd44-Paper-Conference.pdf
- ICLR Virtual: https://iclr.cc/virtual/2025/oral/31924
