# Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters

**Authors:** Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar
**Affiliations:** UC Berkeley, Google DeepMind

## Publication Status

- **arXiv preprint:** August 2024, arXiv:2408.03314
- **Peer-reviewed:** Yes
- **Conference:** International Conference on Learning Representations (ICLR), 2025
- **Status:** Published conference paper

## Preferred Citation

Cite the ICLR 2025 version:

> Snell, C., Lee, J., Xu, K., & Kumar, A. (2025). Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters. In International Conference on Learning Representations.

## Notes

This paper is foundational for understanding o1-style reasoning approaches. It demonstrates that compute-optimal test-time scaling can achieve more than 4x the efficiency of best-of-N sampling, and that smaller models with optimally allocated test-time compute can match or exceed larger models on certain tasks.

## Links

- arXiv: https://arxiv.org/abs/2408.03314
- OpenReview: https://openreview.net/forum?id=KOUASr4W9T
