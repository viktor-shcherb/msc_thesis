# Conclusion [p. 13]

[p. 13] The authors present a series of long-context LLMs that leverage a simple yet necessary position encoding refinement and continual pretraining to achieve strong long-context performance. The long context scaling is performed by continually pretraining from LLAMA 2 with additional 400B tokens and outperforms LLAMA 2 on both short and long-context tasks. The models also demonstrate superior performance compared to existing open-source long-context models and compare favorably against `gpt-3.5-turbo-16k` on a suite of long-context tasks after a simple instruction finetuning procedure without human supervision. The results are complemented with a comprehensive analysis, providing insights on the influences of various factors including the nuances of position encodings, the data mix, and the pretraining curriculum on the final performance. The authors hope their study could make long-context LLMs more accessible and facilitate further advancements in this field.
