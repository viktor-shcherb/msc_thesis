# Method [p. 2-3]

## Continual Pretraining [p. 2-3]

[p. 2] Training with longer sequence lengths introduces significant computational overhead due to quadratic attention calculations. The main motivation for continual pretraining is the hypothesis that similar long-context capabilities can be learned by continually pretraining from a short-context model (validated later in Section 4.4). The original LLAMA 2 architecture is kept nearly intact for continual pretraining, with only a necessary modification to the positional encoding.

[p. 3] The authors also choose not to apply sparse attention (Child et al., 2019) since given LLAMA 2 70B's model dimension (h = 8192), the cost of attention matrix calculation and value aggregation only becomes a computation bottleneck when the sequence length exceeds 49,152 (6h) tokens (Narayanan et al., 2021).

### Positional Encoding

[p. 3] Through early experiments at the 7B scale, the authors identified a key limitation of LLAMA 2's positional encoding (PE) that prevents the attention module from aggregating information of distant tokens. They adopt a minimal yet necessary modification on the RoPE positional encoding (Su et al., 2022) for long-context modeling -- decreasing the rotation angle (controlled by the hyperparameter "base frequency b"), which reduces the decaying effect of RoPE for distant tokens. In Section 4.1, they show this simple method outperforms a concurrent approach (Chen et al., 2023) and provide a theoretic explanation of its superiority.

### Data Mix

[p. 3] On top of the working model with the modified PE, the authors further explored different pretrain data mixes in Section 4.2 for improving long-context abilities, either by adjusting the ratio of LLAMA 2's pretraining data or adding new long text data. They found that often the quality of the data plays a more critical role than the length of texts for long-context continual pretraining.

### Optimization Details

[p. 3] Continual pretraining of LLAMA 2 checkpoints with increased sequence length while keeping the same number of tokens per batch as in LLAMA 2:

- **Total training:** 400B tokens over 100,000 steps for all models
- **FlashAttention** (Dao et al., 2022): negligible GPU memory overhead as sequence length increases; ~17% speed loss when increasing sequence length from 4,096 to 16,384 for the 70B model
- **7B/13B models:** learning rate 2e-5, cosine learning rate schedule with 2000 warm-up steps
- **34B/70B models:** smaller learning rate (1e-5) found important to get monotonically decreasing validation losses

## Instruction Tuning [p. 3]

[p. 3] Collecting human demonstration and preference labels for LLM alignment is cumbersome and expensive (Ouyang et al., 2022; Touvron et al., 2023). The challenge and cost are more pronounced under long-context scenarios. Most existing open-source instruction datasets (Conover et al., 2023; Kopf et al., 2023) predominantly consist of short samples.

### Data Generation Approach

[p. 3] The authors found that a simple and cheap approach leveraging a pre-built large and diverse short-prompt dataset works surprisingly well on long-context benchmarks:

1. Take the RLHF dataset used in LLAMA 2 CHAT
2. Augment it with synthetic self-instruct (Wang et al., 2022) long data generated by LLAMA 2 CHAT itself
3. The model can learn a diverse set of skills through the large amount of RLHF data and transfer that knowledge to long-context scenarios via self-instruct data

The data generation process focuses on QA-format tasks:
- Start from a long document in the pretraining corpus
- Select a random chunk and prompt LLAMA 2 CHAT to write question-answer pairs based on information in the text chunk
- Collect both long and short form answers with different prompts
- A self-critique step where LLAMA 2 CHAT is prompted to verify the model-generated answers
- Given a generated QA pair, use the original long document (truncated to fit the model's maximum context length) as the context to construct a training instance

### Training Details

[p. 3] For short instruction data: concatenated as 16,384-token sequences. For long instruction data: padding tokens added on the right so models can process each long instance individually without truncation.

> While standard instruction tuning only calculates loss on the output tokens, the authors "find it particularly beneficial to also calculate the language modeling loss on the long input prompts, which gives consistent improvements on downstream tasks (Section 4.3)." [p. 3]

### Footnote

[p. 3] Footnote 1: While sparse attention might be useful for reducing the key/value cache size at inference time when trading off performance, it can complicate the inference pipeline and the improvements can also be offset by quantization methods.
