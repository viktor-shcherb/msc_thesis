# References

## Cited works from notes

- **Baichuan (2023)** -- *Baichuan 2: Open large-scale language models.* Cited in `01_introduction.md` and `05_analysis.md` as a RoPE-based model family used for PoSE compatibility checks.
- **Bisk et al. (2020)** -- *PIQA: Reasoning about physical commonsense in natural language.* Cited in `05_analysis.md` as one of the zero-shot benchmarks in Table 3.
- **Brown et al. (2020)** -- *Language Models are Few-Shot Learners.* Cited in `01_introduction.md` for LLM capability context.
- **Bulatov et al. (2022)** -- *Recurrent Memory Transformer.* Cited in `02_related-work.md` under recurrence-based memory approaches.
- **Chen et al. (2023a)** -- *Extending Context Window of Large Language Models via Positional Interpolation.* Cited in `01_introduction.md`, `02_related-work.md`, `03_methodology.md` as primary PI baseline.
- **Chen et al. (2023b)** -- *LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models.* Cited in `02_related-work.md` as an alternative long-context fine-tuning strategy.
- **Clark et al. (2018)** -- *ARC Challenge.* Cited in `05_analysis.md` for ARC-C benchmark in Table 3.
- **Clark et al. (2019)** -- *BoolQ.* Cited in `05_analysis.md` for benchmark setup.
- **Dai et al. (2019)** -- *Transformer-XL.* Cited in `02_related-work.md` as recurrence-memory prior work.
- **Dao (2023)** -- *FlashAttention-2.* Cited in `04_experiments.md` and `01_introduction.md` for efficient long-context training/evaluation support.
- **Gao et al. (2020)** -- *The Pile.* Cited in `04_experiments.md` as fine-tuning data source.
- **Haviv et al. (2022)** -- *Transformers without positional encodings still learn positional information.* Cited in `02_related-work.md` under positional schemes.
- **Huang et al. (2021)** -- *Efficient attentions for long document summarization.* Cited in `01_introduction.md` and `04_experiments.md` (GovReport dataset).
- **kaiokendev (2023)** -- *Things I'm learning while training superhot.* Cited in `01_introduction.md`, `02_related-work.md`, `03_methodology.md` as Linear PI-related practical reference.
- **Kwon et al. (2023)** -- *PagedAttention / vLLM memory management.* Cited in `01_introduction.md` for inference-system progress.
- **Li et al. (2023)** -- *In-context learning with many demonstration examples.* Cited in `01_introduction.md` as long-input application case.
- **Lin et al. (2022)** -- *TruthfulQA.* Cited in `05_analysis.md` for benchmark details.
- **Mohtashami and Jaggi (2023)** -- *Landmark Attention.* Cited in `02_related-work.md` and `04_experiments.md` for passkey retrieval and long-context attention comparison.
- **Peng and Quesnelle (2023)** -- *NTK-aware scaled RoPE (Reddit post).* Cited in `03_methodology.md`, `05_analysis.md`, `10_appendix-c-sliding-window-ppl-interpolation.md` for NTK interpolation behavior.
- **Peng et al. (2023)** -- *YaRN: Efficient Context Window Extension of Large Language Models.* Cited in `01_introduction.md`, `02_related-work.md`, `03_methodology.md`, `10_appendix-c-sliding-window-ppl-interpolation.md`.
- **Press et al. (2021)** -- *Train Short, Test Long (ALiBi).* Cited in `02_related-work.md` and `04_experiments.md` (sliding-window evaluation reference).
- **Ruoss et al. (2023)** -- *Randomized positional encodings (RandPos).* Cited in `02_related-work.md`, `04_experiments.md` and appendix discussion.
- **Su et al. (2021)** -- *RoFormer / RoPE.* Cited in `03_methodology.md` as the positional-embedding foundation.
- **Sun et al. (2023)** -- *A Length-Extrapolatable Transformer (xPos).* Cited in `02_related-work.md` under length-extrapolatable positional methods.
- **Touvron et al. (2023a)** -- *LLaMA.* Cited in `01_introduction.md`, `05_analysis.md` as principal base model.
- **Touvron et al. (2023b)** -- *Llama 2.* Cited in `01_introduction.md`, `05_analysis.md`.
- **Tworkowski et al. (2023)** -- *Focused Transformer.* Cited in `02_related-work.md` under retrieval-based memory lines.
- **Wang and Komatsuzaki (2021)** -- *GPT-J-6B.* Cited in `01_introduction.md` and `05_analysis.md`.
- **Wang et al. (2023)** -- *Augmenting language models with long-term memory.* Cited in `02_related-work.md` memory-transformer context.
- **Wu et al. (2022)** -- *Memorizing Transformers.* Cited in `02_related-work.md` memory-transformer context.
- **Zellers et al. (2019)** -- *HellaSwag.* Cited in `05_analysis.md` benchmark details.
- **Zhangir et al. (2022)** -- *Proof-pile.* Cited in `04_experiments.md` for evaluation dataset.
- **Zhou et al. (2022)** -- *Fine-grained distillation for long document retrieval.* Cited in `01_introduction.md` as long-input motivation.
