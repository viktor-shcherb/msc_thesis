# 6 Related work [p. 9–10]

[p. 9–10] The paper examines recent advances in efficient Transformer variants (Sukhbaatar et al., 2019; Kitaev et al., 2020; Choromanski et al., 2021; Tay et al., 2021; Katharopoulos et al., 2020; Wang et al., 2020; Wu et al., 2020) that accept longer sequences than prior approaches. Longer effective context size is often achieved by sparse attention (Child et al., 2019), recurrence (Dai et al., 2019), and cached memory (Weston et al., 2015; Rae et al., 2020). The work is also related to methods that incorporate long context (Wang and Cho, 2016) as well as document-level tasks that inherently require modeling long-range context (Zhang et al., 2018; Hofstatter et al., 2020; Zhang et al., 2020). [p. 9]

[p. 10] The work is also similar to other analysis of language models, especially for long-range context. Khandelwal et al. (2018) analyze the usage of long-term context of smaller LSTM LMs. Sharan et al. (2018) prove long-term context is not needed for HMM LM due to teacher forcing. Rae and Razavi (2020) conduct an analysis exclusively for the Transformer-XL (Dai et al., 2019) model. Rae et al. (2020) show that Compressive Transformer improves the performance of infrequent tokens. The work also relates to that of Lai et al. (2020), who investigate the impact of context for pretrained masked LMs. More recently, Press et al. (2020) also observe negligible benefits of long-term context; the authors step further in this direction by exploring larger models with more fine-grained analysis. [p. 10]
