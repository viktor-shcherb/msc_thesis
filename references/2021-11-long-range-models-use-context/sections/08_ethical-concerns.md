# Ethical concerns [p. 10]

## Misuse of language models [p. 10]

[p. 10] The two large language models evaluated in this work share common ethical concerns with works on language models and language generation. These pre-trained LMs can be used maliciously to generate unfaithful, hallucinated, and biased output. The reported results do not include any kind of generation. [p. 10]

## Energy costs [p. 10]

[p. 10] All analysis experiments are conducted on RTX8000 GPUs. Although this work does not include training large language models, the energy costs of evaluating large pre-trained LMs, such as the Routing Transformer, should not be ignored. Each example of 8K tokens long takes around 1.3s ~ 1.4s to run one forward pass with the RT model. The authors hope the analysis can shed light on more efficient and effective methods to encode long-term context. [p. 10]
