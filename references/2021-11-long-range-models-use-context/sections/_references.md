# References

Only references that are cited in the section notes are included below.

---

**Beltagy et al. (2020)**
Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer.
Cited in 02_background-and-setup.md as a model without a publicly-available PG-19 checkpoint.

**Brown et al. (2020)**
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. *arXiv preprint arXiv:2005.14165*.
Cited in 01_introduction.md as part of the backbone of state-of-the-art NLP systems.

**Child et al. (2019)**
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers.
Cited in 02_background-and-setup.md for local attention and in 06_related-work.md for sparse attention.

**Choromanski et al. (2021)**
Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. 2021. Rethinking attention with performers. In *International Conference on Learning Representations*.
Cited in 06_related-work.md as an efficient Transformer variant.

**Correia et al. (2019)**
Goncalo M. Correia, Vlad Niculae, and Andre F. T. Martins. 2019. Adaptively sparse transformers. In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*, pages 2174-2184.
Cited in 01_introduction.md for sparse attention methods.

**Dai et al. (2019)**
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive language models beyond a fixed-length context. In *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*, pages 2978-2988.
Cited in 06_related-work.md for recurrence-based long context and as the model analyzed by Rae and Razavi (2020).

**Devlin et al. (2019)**
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding.
Cited in 01_introduction.md as part of the backbone of state-of-the-art NLP systems.

**Grimshaw (1990)**
Jane Grimshaw. 1990. *Argument structure*. the MIT Press.
Cited in 01_introduction.md for argument structure as a discourse-level phenomenon.

**Grosz et al. (1995)**
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. *Computational Linguistics*, 21(2):203-225.
Cited in 01_introduction.md for anaphora modeling.

**Hobbs (1979)**
Jerry R Hobbs. 1979. Coherence and coreference. *Cognitive science*, 3(1):67-90.
Cited in 01_introduction.md for anaphora modeling.

**Hofstatter et al. (2020)**
Sebastian Hofstatter, Hamed Zamani, Bhaskar Mitra, Nick Craswell, and Allan Hanbury. 2020. Local self-attention over long text for efficient document retrieval.
Cited in 06_related-work.md for document-level tasks requiring long-range context.

**Holtzman et al. (2020)**
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In *International Conference on Learning Representations*.
Cited in 05_sequence-level-analysis.md for noting different behavior in non-teacher-forced generation settings.

**Ji et al. (2015)**
Yangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer, and Jacob Eisenstein. 2015. Document context language models. *arXiv preprint arXiv:1511.03962*.
Cited in 01_introduction.md for adding document-level context to neural LMs.

**Katharopoulos et al. (2020)**
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In *Proceedings of the 37th International Conference on Machine Learning*.
Cited in 06_related-work.md as an efficient Transformer variant.

**Khandelwal et al. (2018)**
Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. 2018. Sharp nearby, fuzzy far away: How neural language models use context. In *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 284-294.
Cited in 01_introduction.md, 02_background-and-setup.md, 03_effect-of-longer-context.md, and 06_related-work.md as the primary inspiration for the context analysis methodology and for analyzing LSTM LM context usage.

**Kitaev et al. (2020)**
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer. In *International Conference on Learning Representations*.
Cited in 06_related-work.md as an efficient Transformer variant.

**Krishna et al. (2021)**
Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021. Hurdles to progress in long-form question answering. In *North American Association for Computational Linguistics*.
Cited in 02_background-and-setup.md for state-of-the-art results using sparse self-attention.

**Labov and Waletzky (1997)**
William Labov and Joshua Waletzky. 1997. Narrative analysis: Oral versions of personal experience.
Cited in 01_introduction.md for narrative scripts and trajectories.

**Lai et al. (2020)**
Yi-An Lai, Garima Lalwani, and Yi Zhang. 2020. Context analysis for pre-trained masked language models. In *Findings of the Association for Computational Linguistics: EMNLP 2020*, pages 3789-3804.
Cited in 06_related-work.md for investigating the impact of context for pretrained masked LMs.

**Luong et al. (2015)**
Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective approaches to attention-based neural machine translation. In *Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing*, pages 1412-1421.
Cited in 02_background-and-setup.md for Local Transformer receptive field scaling.

**Mooney and DeJong (1985)**
Raymond J Mooney and Gerald DeJong. 1985. Learning schemata for natural language processing. In *IJCAI*, pages 681-687.
Cited in 01_introduction.md for causal links between concepts.

**Press et al. (2020)**
Ofir Press, Noah A. Smith, and Mike Lewis. 2020. Shortformer: Better language modeling using shorter inputs.
Cited in 03_effect-of-longer-context.md (footnote 9) and 06_related-work.md for observing negligible benefits of long-term context.

**Radford et al. (2019)**
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. *OpenAI blog*, 1(8):9.
Cited in 01_introduction.md for GPT-2's 1024-token context.

**Rae and Razavi (2020)**
Jack Rae and Ali Razavi. 2020. Do transformers need deep long-range memory? In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*, pages 7524-7529.
Cited in 06_related-work.md for analyzing the Transformer-XL model.

**Rae et al. (2020)**
Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. 2020. Compressive transformers for long-range sequence modelling. In *International Conference on Learning Representations*.
Cited in 01_introduction.md, 02_background-and-setup.md, and 06_related-work.md for the PG-19 dataset, cached memory, and Compressive Transformer.

**Rosenfeld (1996)**
Roni Rosenfeld. 1996. A maximum entropy approach to adaptive statistical language modeling.
Cited in 01_introduction.md for incorporating distant token statistics into n-gram models.

**Roy et al. (2021)**
Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2021. Efficient content-based sparse attention with routing transformers. *Transactions of the Association for Computational Linguistics*, 9:53-68.
Cited in 01_introduction.md and 02_background-and-setup.md as the Routing Transformer model under analysis.

**Schank and Abelson (1977)**
Roger C Schank and Robert P Abelson. 1977. Scripts, plans, goals, and understanding: an inquiry into human knowledge structures.
Cited in 01_introduction.md for narrative scripts and trajectories.

**Sharan et al. (2018)**
Vatsal Sharan, Sham Kakade, Percy Liang, and Gregory Valiant. 2018. Prediction with a short memory. In *Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing*, STOC 2018, page 1074-1087.
Cited in 06_related-work.md for proving long-term context is not needed for HMM LM due to teacher forcing.

**Sukhbaatar et al. (2019)**
Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. 2019. Adaptive attention span in transformers. In *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*, pages 331-335.
Cited in 02_background-and-setup.md and 06_related-work.md for adaptive attention span and as an efficient Transformer variant.

**Tay et al. (2021)**
Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. 2021. Synthesizer: Rethinking self-attention for transformer models.
Cited in 06_related-work.md as an efficient Transformer variant.

**Vaswani et al. (2017)**
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. In *Advances in Neural Information Processing Systems*, volume 30. Curran Associates, Inc.
Cited in 01_introduction.md and 02_background-and-setup.md as the original Transformer architecture.

**Wang and Cho (2016)**
Tian Wang and Kyunghyun Cho. 2016. Larger-context language modelling with recurrent neural network. In *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 1319-1329.
Cited in 06_related-work.md for methods incorporating long context.

**Wang and Sennrich (2020)**
Chaojun Wang and Rico Sennrich. 2020. On exposure bias, hallucination and domain shift in neural machine translation. In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*, pages 3544-3552.
Cited in 05_sequence-level-analysis.md for noting different behavior in non-teacher-forced settings.

**Wang et al. (2020)**
Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Linformer: Self-attention with linear complexity.
Cited in 06_related-work.md as an efficient Transformer variant.

**Welleck et al. (2020)**
Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. 2020. Neural text generation with unlikelihood training. In *International Conference on Learning Representations*.
Cited in 05_sequence-level-analysis.md for noting different behavior in non-teacher-forced settings.

**Weston et al. (2015)**
Jason Weston, Sumit Chopra, and Antoine Bordes. 2015. Memory networks.
Cited in 06_related-work.md for cached memory approaches.

**Wu et al. (2020)**
Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. 2020. Lite transformer with long-short range attention.
Cited in 06_related-work.md as an efficient Transformer variant.

**Zaheer et al. (2020)**
Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big bird: Transformers for longer sequences. *Advances in Neural Information Processing Systems*, 33.
Cited in 01_introduction.md for 4096-token context.

**Zellers et al. (2018)**
Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. SWAG: A large-scale adversarial dataset for grounded commonsense inference. In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing*, pages 93-104.
Cited in 05_sequence-level-analysis.md for the SWAG multiple choice task setting.

**Zhang et al. (2018)**
Jiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei Zhai, Jingfang Xu, Min Zhang, and Yang Liu. 2018. Improving the transformer translation model with document-level context. In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing*, pages 533-542.
Cited in 06_related-work.md for document-level tasks requiring long-range context.

**Zhang et al. (2020)**
Pei Zhang, Boxing Chen, Niyu Ge, and Kai Fan. 2020. Long-short term masking transformer: A simple but effective baseline for document-level neural machine translation. In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing*, pages 1081-1087.
Cited in 06_related-work.md for document-level tasks requiring long-range context.
