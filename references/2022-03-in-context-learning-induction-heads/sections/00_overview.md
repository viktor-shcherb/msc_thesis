# Overview

**Title:** In-context Learning and Induction Heads

**Authors:** Catherine Olsson\*, Nelson Elhage\*, Neel Nanda\*, Nicholas Joseph\*, Nova DasSarma\*, Tom Henighan\*, Ben Mann\*, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, Chris Olah\*

(\* Core Research Contributor; \* Core Infrastructure Contributor; \* Correspondence to colah@anthropic.com)

**Affiliation:** Anthropic

**Venue:** Transformer Circuits Thread

**Published:** Mar 8, 2022

## Abstract

> "Induction heads" are attention heads that implement a simple algorithm to complete token sequences like [A][B] ... [A] -> [B]. In this work, we present preliminary and indirect evidence for a hypothesis that induction heads might constitute the mechanism for the majority of all "in-context learning" in large transformer models (i.e. decreasing loss at increasing token indices). We find that induction heads develop at precisely the same point as a sudden sharp increase in in-context learning ability, visible as a bump in the training loss. We present six complementary lines of evidence, arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size. For small attention-only models, we present strong, causal evidence; for larger models with MLPs, we present correlational evidence." [p. 1]

## Section headings

- Introduction (unnumbered) [p. 1-3]
- Key Concepts [p. 4-7]
  - In-context Learning [p. 4-5]
  - Induction Heads [p. 5-6]
  - Per-Token Loss Analysis [p. 6-7]
- Arguments that induction heads are the mechanism for the majority of in-context learning [p. 8-9]
- Argument 1: Transformer language models undergo a "phase change" during training, during which induction heads form and simultaneously in-context learning improves dramatically [p. 9-15]
  - Abrupt onset of in-context learning [p. 10]
  - Derivative of loss with respect to log token index [p. 10-11]
  - Co-occurrence with induction head formation [p. 11]
  - Connection to overall training dynamics [p. 11]
  - Loss curves diverge during phase change [p. 12]
  - Per-token loss principal component analysis [p. 12]
  - Summary of co-occurring changes [p. 13]
  - Looking at the Phase Change More Closely [p. 13-14]
  - Assessing the Evidence [p. 15]
- Argument 2: When we change the transformer architecture in a way that shifts when induction heads form or whether they can form, the dramatic improvement in in-context learning shifts in a precisely matching way [p. 16-17]
  - Smeared key architecture [p. 16]
  - Results [p. 17]
  - Caveats for large models [p. 17]
- Argument 3: When we directly "knock out" induction heads in small models at test-time, the amount of in-context learning greatly decreases [p. 18-19]
  - Ablation results [p. 18-19]
  - Interpreting the ablation results [p. 19]
- Argument 4: Despite being defined narrowly as copying random sequences, induction heads can implement surprisingly abstract types of in-context learning [p. 19-24]
  - Behavior 1: Literal sequence copying [p. 20-21]
  - Behavior 2: Translation [p. 21-22]
  - Behavior 3: Pattern matching [p. 22-23]
  - What's going on with more abstract heads that are also induction heads? [p. 24]
- Argument 5: For small models, we can explain mechanistically how induction heads work, and can show they contribute to in-context learning [p. 25-27]
  - Summary of Reverse Engineering Induction Heads [p. 26-27]
  - What About More Complex Induction Heads? [p. 27]
- Argument 6: Extrapolation from small models suggests induction heads are responsible for the majority of in-context learning in large models [p. 28-29]
- Model Analysis Table [p. 29-38]
  - Small Attention-Only Transformers [p. 30-31]
  - Small Transformers (with MLPs) [p. 32-33]
  - Full-Scale Transformers [p. 34-36]
  - "Smeared Key" Architecture Modification + Controls [p. 36]
  - Small Attention-Only Transformers (Different Dataset) [p. 37]
  - Small Transformers (with MLPs) (Different Dataset) [p. 38]
- Model Details [p. 39-40]
  - Small Models [p. 39]
  - Full-Scale Models [p. 39-40]
  - Table of Model Properties for Full-Scale Models [p. 40]
  - Smeared Key Models [p. 40]
- Unexplained Curiosities [p. 40-42]
  - Seemingly Constant In-Context Learning Score [p. 40-41]
  - Phase Change Effect on Loss Derivatives [p. 41-42]
  - Additional Curiosities [p. 42]
- Discussion [p. 43]
  - Safety Implications [p. 43]
  - Linking Learning Dynamics, Scaling Laws, and Mechanistic Interpretability [p. 43]
- Related Work [p. 44-47]
  - In-Context Learning [p. 44]
  - Scaling Laws [p. 44-45]
  - Phase Changes & Discontinuous Model Behavior [p. 45]
  - Learning Dynamics [p. 46]
  - Universality [p. 46-47]
  - Attention Patterns in Translation-Like Tasks [p. 47]
- Comments & Replications [p. 48-49]
  - Replication: Adam Scherlis (Redwood Research) [p. 48]
  - Replication: Tom Lieberum (University of Amsterdam) [p. 48-49]
- Change Log [p. 49]
- Footnotes [p. 49-51]
- Acknowledgments [p. 53]
- Author Contributions [p. 53-54]
- References (bibliography) [p. 51-53]
- Citation Information [p. 54]
- Where Induction Heads Form [p. 54-56]
  - Small Attention-Only Models [p. 55]
  - Small Models with MLPs [p. 55]
  - Full-Scale Models [p. 55-56]
- Distribution of Scores [p. 56]
- Validating Head Activation Evaluators [p. 56-57]
  - Copying [p. 56-57]
  - Prefix matching [p. 57]
- Data Collection [p. 58-60]
  - (1) Per-token losses (unaltered models) [p. 58]
  - (2) Per-token losses (attention head ablations) [p. 58-59]
  - (3) Head activation evaluators [p. 60]
  - (4) Trace of QK eigenvalues (for previous-token QK-circuit term) [p. 60-61]
- Analyses [p. 61]
  - Per-token losses [p. 61]
  - Attention head measurements [p. 61]
  - Ablation attribution to phase change [p. 61]
