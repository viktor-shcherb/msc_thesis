# References

This file contains only the references that are cited in the section notes.

## [1] Language models are few-shot learners
Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A. and others., 2020. arXiv preprint arXiv:2005.14165.
- Cited in 01_introduction.md (scaling/real-world use of transformers; abrupt capability emergence), 02_key-concepts.md (emergent in-context learning; few-shot learning conception), 14_related-work.md (demonstrated emergent in-context learning; task-specific abrupt changes; "few-shot" framing)

## [2] LaMDA: our breakthrough conversation technology
Collins, E. and Ghahramani, Z., 2021.
- Cited in 01_introduction.md (scaling/real-world use of transformers)

## [3] Evaluating large language models trained on code
Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H.P.d.O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G. and others., 2021. arXiv preprint arXiv:2107.03374.
- Cited in 01_introduction.md (scaling/real-world use of transformers)

## [4] Towards a human-like open-domain chatbot
Adiwardana, D., Luong, M., So, D.R., Hall, J., Fiedel, N., Thoppilan, R., Yang, Z., Kulshreshtha, A., Nemade, G., Lu, Y. and others., 2020. arXiv preprint arXiv:2001.09977.
- Cited in 01_introduction.md (scaling/real-world use of transformers)

## [5] Scaling Language Models: Methods, Analysis & Insights from Training Gopher
Rae, J.W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A., Powell, R., Driessche, G.v.d., Hendricks, L.A., Rauh, M., Huang, P., Glaese, A., Welbl, J., Dathathri, S., Huang, S., Uesato, J., Mellor, J., Higgins, I., Creswell, A., McAleese, N., Wu, A., Elsen, E., Jayakumar, S., Buchatskaya, E., Budden, D., Sutherland, E., Simonyan, K., Paganini, M., Sifre, L., Martens, L., Li, X.L., Kuncoro, A., Nematzadeh, A., Gribovskaya, E., Donato, D., Lazaridou, A., Mensch, A., Lespiau, J., Tsimpoukelli, M., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M., Pohlen, T., Gong, Z., Toyama, D., d'Autume, C.d.M., Li, Y., Terzi, T., Mikulik, V., Babuschkin, I., Clark, A., Casas, D.d.L., Guy, A., Jones, C., Bradbury, J., Johnson, M., Hechtman, B., Weidinger, L., Gabriel, I., Isaac, W., Lockhart, E., Osindero, S., Rimell, L., Dyer, C., Vinyals, O., Ayoub, K., Stanway, J., Bennett, L., Hassabis, D., Kavukcuoglu, K. and Irving, G., 2021. Preprint.
- Cited in 01_introduction.md (scaling/real-world use of transformers)

## [6] Thread: Circuits
Cammarata, N., Carter, S., Goh, G., Olah, C., Petrov, M., Schubert, L., Voss, C., Egan, B. and Lim, S.K., 2020. Distill.
- Cited in 01_introduction.md (mechanistic interpretability on CNN vision models), 14_related-work.md (prior work on circuits)

## [7] A Mathematical Framework for Transformer Circuits
Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., DasSarma, N., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S. and Olah, C., 2021. Transformer Circuits Thread.
- Cited in 01_introduction.md (prior work developing mathematical framework for transformers; discovery of induction heads in 2-layer models), 02_key-concepts.md (previous paper defining induction heads)

## [8] Grokking: Generalization beyond overfitting on small algorithmic datasets
Power, A., Burda, Y., Edwards, H., Babuschkin, I. and Misra, V., 2022. arXiv preprint arXiv:2201.02177.
- Cited in 01_introduction.md (abrupt capability emergence; phase changes), 14_related-work.md (grokking phenomenon, discontinuous jump to perfect generalization)

## [9] The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models
Pan, A., Bhatia, K. and Steinhardt, J., 2022. International Conference on Learning Representations.
- Cited in 01_introduction.md (reward hacking emerging in a phase change)

## [10] Language Models are Unsupervised Multitask Learners
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D. and Sutskever, I., 2019.
- Cited in 02_key-concepts.md (GPT-2; emergent in-context learning noted), 08_argument-5.md (GPT-2 pointer-arithmetic mechanism for induction heads)

## [11] Calibrate Before Use: Improving Few-Shot Performance of Language Models
Zhao, T.Z., Wallace, E., Feng, S., Klein, D. and Singh, S., 2021.
- Cited in 02_key-concepts.md (prompt engineering to leverage in-context learning), 14_related-work.md (prompt engineering)

## [12] Making Pre-trained Language Models Better Few-shot Learners
Gao, T., Fisch, A. and Chen, D., 2021.
- Cited in 02_key-concepts.md (prompt engineering to leverage in-context learning), 14_related-work.md (prompt engineering)

## [13] Scaling laws for neural language models
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J. and Amodei, D., 2020. arXiv preprint arXiv:2001.08361.
- Cited in 02_key-concepts.md (origin of the loss-at-different-token-indices formalism for in-context learning), 14_related-work.md (scaling laws; origin of per-token-index approach; 1-layer transformers not following same scaling laws), 24_analyses.md (technique borrowed for highlighting in-context learning in 2D plots)

## [14] Risks from Learned Optimization in Advanced Machine Learning Systems
Hubinger, E., Merwijk, C.v., Mikulik, V., Skalse, J. and Garrabrant, S., 2021.
- Cited in 02_key-concepts.md (mesa-optimization / inner alignment as safety concern), 13_discussion.md (mesa-optimization concern)

## [15] Why does unsupervised pre-training help deep learning?
Erhan, D., Courville, A., Bengio, Y. and Vincent, P., 2010. Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 201--208.
- Cited in 02_key-concepts.md (origin of per-token loss analysis method / "function spaces")

## [16] The Scaling Hypothesis
Branwen, G., 2020.
- Cited in 17_footnotes.md (footnote 13, referenced regarding near-perfect loss and human-level understanding)

## [17] Shortformer: Better language modeling using shorter inputs
Press, O., Smith, N.A. and Lewis, M., 2020. arXiv preprint arXiv:2012.15832.
- Cited in 08_argument-5.md (unusual positional mechanism in GPT-2), 11_model-details.md (small model positional embeddings variant)

## [18] A General Language Assistant as a Laboratory for Alignment
Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N. and others., 2021. arXiv preprint arXiv:2112.00861.
- Cited in 10_model-analysis-table.md (dataset description), 11_model-details.md (full-scale models description)

## [19] Common Crawl
Foundation, T.C.C.
- Cited in 10_model-analysis-table.md (filtered common crawl data for training)

## [20] The Pile: An 800GB Dataset of Diverse Text for Language Modeling
Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S. and Leahy, C., 2020.
- Cited in 10_model-analysis-table.md (smaller data distributions included in training data)

## [21] Adversarial Reprogramming of Neural Networks
Elsayed, G.F., Goodfellow, I. and Sohl-Dickstein, J., 2018.
- Cited in 13_discussion.md (adversarial reprogramming showing model behavior can change at inference)

## [22] Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm
Reynolds, L. and McDonell, K., 2021.
- Cited in 13_discussion.md ("locating" an already-learned behavior vs. learning something new)

## [23] Exact solutions to the nonlinear dynamics of learning in deep linear neural networks
Saxe, A.M., McClelland, J.L. and Ganguli, S., 2014.
- Cited in 13_discussion.md (learning dynamics as a bridge topic), 14_related-work.md (closed-form solutions to learning dynamics for linear networks)

## [24] Tensor2tensor transformer visualization
Jones, L., 2017.
- Cited in 14_related-work.md (prior work on attention head analysis, referenced via the framework paper)

## [25] A multiscale visualization of attention in the transformer model
Vig, J., 2019. arXiv preprint arXiv:1906.05714.
- Cited in 14_related-work.md (prior work on attention head analysis, referenced via the framework paper)

## [26] Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned
Voita, E., Talbot, D., Moiseev, F., Sennrich, R. and Titov, I., 2019. arXiv preprint arXiv:1905.09418.
- Cited in 14_related-work.md (prior work on attention head analysis; "previous token" attention heads forming across models)

## [27] What does bert look at? an analysis of bert's attention
Clark, K., Khandelwal, U., Levy, O. and Manning, C.D., 2019. arXiv preprint arXiv:1906.04341.
- Cited in 14_related-work.md (prior work on attention head analysis; "previous token" attention heads forming across models)

## [28] Do attention heads in bert track syntactic dependencies?
Htut, P.M., Phang, J., Bordia, S. and Bowman, S.R., 2019. arXiv preprint arXiv:1911.12246.
- Cited in 14_related-work.md (prior work on attention head analysis, referenced via the framework paper)

## [29] Attention is not all you need: Pure attention loses rank doubly exponentially with depth
Dong, Y., Cordonnier, J. and Loukas, A., 2021. arXiv preprint arXiv:2103.03404.
- Cited in 14_related-work.md (related mathematical analysis, referenced via the framework paper)

## [30] What Context Features Can Transformer Language Models Use?
O'Connor, J. and Andreas, J., 2021. arXiv preprint arXiv:2106.08367.
- Cited in 14_related-work.md (studying how/when in-context learning occurs; word order importance; some experiments in tension with induction head hypothesis)

## [31] An Explanation of In-context Learning as Implicit Bayesian Inference
Xie, S.M., Raghunathan, A., Liang, P. and Ma, T., 2021. arXiv preprint arXiv:2111.02080.
- Cited in 14_related-work.md (studying how/when in-context learning occurs; LSTMs outperforming Transformers on HMM-based synthetic data)

## [32] Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?
Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H. and Zettlemoyer, L., 2022. arXiv preprint arXiv:2202.12837.
- Cited in 14_related-work.md (studying how/when in-context learning occurs)

## [33] Reconciling modern machine-learning practice and the classical bias-variance trade-off
Belkin, M., Hsu, D., Ma, S. and Mandal, S., 2019. Proceedings of the National Academy of Sciences, Vol 116(32), pp. 15849--15854. National Acad Sciences.
- Cited in 14_related-work.md (Double Descent phenomenon)

## [34] Deep double descent: Where bigger models and more data hurt
Nakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B. and Sutskever, I., 2021. Journal of Statistical Mechanics: Theory and Experiment, Vol 2021(12), pp. 124003. IOP Publishing.
- Cited in 14_related-work.md (generalizations of double descent with respect to parameter size, dataset size, or training)

## [35] Future ML Systems Will Be Qualitatively Different
Steinhardt, J., 2021. Bounded Regret.
- Cited in 14_related-work.md (general discussion of phase change phenomena)

## [36] A mathematical theory of semantic development in deep neural networks
Saxe, A.M., McClelland, J.L. and Ganguli, S., 2019. Proceedings of the National Academy of Sciences, Vol 116(23), pp. 11537--11546. National Acad Sciences.
- Cited in 14_related-work.md (Saxe et al. follow-up work on learning dynamics and semantic information)

## [37] Qualitatively characterizing neural network optimization problems
Goodfellow, I.J., Vinyals, O. and Saxe, A.M., 2014. arXiv preprint arXiv:1412.6544.
- Cited in 14_related-work.md (geometry of neural network loss surfaces)

## [38] Analyzing monotonic linear interpolation in neural network loss landscapes
Lucas, J., Bae, J., Zhang, M.R., Fort, S., Zemel, R. and Grosse, R., 2021. arXiv preprint arXiv:2104.11044.
- Cited in 14_related-work.md (geometry of neural network loss surfaces)

## [39] Geometry of neural network loss surfaces via random matrix theory
Pennington, J. and Bahri, Y., 2017. International Conference on Machine Learning, pp. 2798--2806.
- Cited in 14_related-work.md (geometry of neural network loss surfaces)

## [40] Visualizing the loss landscape of neural nets
Li, H., Xu, Z., Taylor, G., Studer, C. and Goldstein, T., 2018. Advances in neural information processing systems, Vol 31.
- Cited in 14_related-work.md (geometry of neural network loss surfaces)

## [41] Zoom In: An Introduction to Circuits
Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M. and Carter, S., 2020. Distill. DOI: 10.23915/distill.00024.001
- Cited in 14_related-work.md (universality / "periodic table of visual features" quote; extending universality from features to circuits)

## [42] Convergent learning: Do different neural networks learn the same representations?
Li, Y., Yosinski, J., Clune, J., Lipson, H., Hopcroft, J.E. and others., 2015. FE@ NIPS, pp. 196--212.
- Cited in 14_related-work.md ("convergent learning"; neurons highly correlated across retrained versions)

## [43] SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability
Raghu, M., Gilmer, J., Yosinski, J. and Sohl-Dickstein, J., 2017. Advances in Neural Information Processing Systems 30, pp. 6078--6087. Curran Associates, Inc.
- Cited in 14_related-work.md (neural networks develop shared representations)

## [44] Similarity of neural network representations revisited
Kornblith, S., Norouzi, M., Lee, H. and Hinton, G., 2019. arXiv preprint arXiv:1905.00414.
- Cited in 14_related-work.md (neural networks develop shared representations)

## [45] High-Low Frequency Detectors
Schubert, L., Voss, C., Cammarata, N., Goh, G. and Olah, C., 2021. Distill. DOI: 10.23915/distill.00024.005
- Cited in 14_related-work.md (same circuits appear to implement similar features across models)

## [46] Performance-optimized hierarchical models predict neural responses in higher visual cortex
Yamins, D.L., Hong, H., Cadieu, C.F., Solomon, E.A., Seibert, D. and DiCarlo, J.J., 2014. Proceedings of the National Academy of Sciences, Vol 111(23), pp. 8619--8624. National Acad Sciences.
- Cited in 14_related-work.md (biological and artificial neural networks learn similar representations)

## [47] Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream
Guclu, U. and van Gerven, M.A., 2015. Journal of Neuroscience, Vol 35(27), pp. 10005--10014. Soc Neuroscience.
- Cited in 14_related-work.md (biological and artificial neural networks learn similar representations)

## [48] Seeing it all: Convolutional network layers map the function of the human visual system
Eickenberg, M., Gramfort, A., Varoquaux, G. and Thirion, B., 2017. NeuroImage, Vol 152, pp. 184--194. Elsevier.
- Cited in 14_related-work.md (biological and artificial neural networks learn similar representations)

## [49] Multimodal Neurons in Artificial Neural Networks
Goh, G., Cammarata, N., Voss, C., Carter, S., Petrov, M., Schubert, L., Radford, A. and Olah, C., 2021. Distill. DOI: 10.23915/distill.00030
- Cited in 14_related-work.md (multimodal "concept" neurons found in humans also occur in neural networks)

## [50] Neural machine translation by jointly learning to align and translate
Bahdanau, D., Cho, K. and Bengio, Y., 2014. arXiv preprint arXiv:1409.0473.
- Cited in 14_related-work.md (attention patterns in literal translation attending to token about to be translated)

## [51] Listen, attend and spell
Chan, W., Jaitly, N., Le, Q.V. and Vinyals, O., 2015. arXiv preprint arXiv:1508.01211.
- Cited in 14_related-work.md (attention in voice recognition attending to portion of audio about to be transcribed)
