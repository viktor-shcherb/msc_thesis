# Related Work [p. 44-47]

The general approach of the paper to reverse engineering transformers is based heavily on the previous paper, *A Mathematical Framework for Transformer Circuits*. Rather than repeating the discussion of how that framework relates to other work in interpretability, the authors refer readers to Related Work in that previous paper, especially discussion of the relationship to circuits [6], to analysis of attention heads (e.g. [24, 25, 26, 27, 28]), and to related mathematical analysis (e.g. [29]). [p. 44]

Building on that perspective, this section focuses on how aspects of the paper raise new connections to the machine learning literature, separate from the connections raised simply by the underlying framework. [p. 44]

## In-Context Learning [p. 44]

Emergent in-context learning was compellingly demonstrated in GPT-3 [1]. A number of papers have studied how to effectively leverage in-context learning, especially with "prompt engineering" [11, 12]. But of particular importance to the authors, several papers have tried to study how and when in-context learning occurs (e.g. [13, 30, 31, 32]). [p. 44]

Some of the findings of these papers are consistent with the induction head hypothesis, or support the methodology: [p. 44]

- Kaplan *et al.* [13] is the origin of the approach for using loss at different token indices as a formalism for studying in-context learning. [p. 44]
- O'Connor & Andreas [30] find that preserving word order in contexts is important, as the induction head hypothesis would suggest. [p. 44]

However, there are also places where experiments in these papers seem in tension with the induction head hypothesis: [p. 44]

- O'Connor & Andreas [30] have some experiments suggesting that removing all words except nouns can improve loss. This seems inconsistent with the induction head hypothesis. However, they only find this for experiments where they retrain the model on modified data. This seems both less directly related to the current work (because the authors aim to study models trained on natural data) and subtle to interpret (because retraining models introduces the possibility of run-to-run loss variation, and the measured loss differences are small). The experiments where they do not retrain models on modified data seem consistent with the induction head hypothesis. [p. 44]

- Xie *et al.* [31] finds that LSTMs outperform Transformers when fit to synthetic data generated by a Hidden Markov Model (HMM) designed to isolate a particular theoretical model of in-context learning. The authors generally expect Transformers to outperform LSTMs at in-context learning on natural text (as seen in Kaplan *et al.* [13]), with induction heads as a major explanation. But in the case of the Xie *et al.* experiments (which do not use natural text), they suspect that the structure of the synthetic data does not benefit from Transformers, and that LSTMs are perhaps better at simulating HMMs. [p. 44]

Note that the authors are using a broader conception of "in-context learning", rather than something as specific as "few-shot learning". This is in contrast with Brown *et al.* [1], which describes that a language model "develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task," with examples of tasks such as few-digit addition and typo correction. In the authors' conception of "in-context learning", they refer to all the ways that a model rapidly adapts to or recognizes what is going on in the context, even if "what is going on in the context" is not well-conceived-of as multiple "shots" of some other specific repeated task. [p. 44]

## Scaling Laws [p. 44-45]

Over the last few years, the observation that machine learning models change in smooth, predictable ways described by scaling laws [13] has emerged as a useful tool for modelling the properties of models before training them. [p. 44]

The relationship between scaling laws and mechanistic interpretability might be seen as analogous to the relationship between thermodynamics and the physics of individual particles. For both thermodynamics and scaling laws, even though the underlying system is very complicated, we are able to find simple relationships between the variables -- for thermodynamics: entropy, temperature, volume and pressure; for neural networks: loss, compute, parameters, and data. In contrast, mechanistic interpretability studies the individual circuits underlying our models, vaguely analogous to how one might carefully study individual particles in physics. In physics, these two layers of abstraction were bridged by statistical physics. [p. 45]

Can we bridge these two levels of abstraction in machine learning? The induction head phase change is the first time the authors are aware of a bridge. They give us a phenomenon at the level of macroscopic properties at loss which can be explained at the level of circuits and mechanistic interpretability. [p. 45]

In fact, induction heads may be able to explain previous observed exceptions to scaling laws. In the authors' work, 1-layer transformers seem very different from deeper transformers. This was previously observed by Kaplan *et al.* [13] who found that 1-layer transformers do not follow the same scaling laws as larger transformers. It seems quite plausible that the reason why the scaling laws are different for one-layer models is that they do not have induction heads. [p. 45]

## Phase Changes & Discontinuous Model Behavior [p. 45]

In the previous section, the authors discussed how scaling laws describe smooth predictable relationships between model loss and properties like scale. However, a more recent set of results have made the situation seem more subtle. While models' losses often scale in predictable ways, there are cases where behavior is more complex: [p. 45]

- Brown *et al.* [1] find that, while aggregate loss scales predictably with model size, models' ability to perform specific tasks like arithmetic can change abruptly. [p. 45]

- Power *et al.* [8] observe a phenomenon they call "grokking" where models discontinuously jump from random chance to perfect generalization as they train. [p. 45]

- Double Descent [33] is a phenomenon where model performance first gets worse due to overfitting as one makes a model larger (the "classical" regime), but then gets better again past a certain point (the "modern" regime). Generalizations of double descent can occur with respect to parameter size, dataset size, or amount of training [34]. These phenomena are not discontinuous in loss, but they are surprising trend reversals, and perhaps discontinuous in derivatives. [p. 45]

For more general discussion of these phase change phenomena, see a recent blog post by Steinhardt [35]. [p. 45]

The discontinuous phase change behavior the authors observe with induction heads is most analogous to Power *et al.* [8]'s "grokking", in that it occurs over the course of training. The authors think their main contribution to this literature is linking the changes they observe to the formation of induction heads and a parameter-level understanding of the circuits involved. As far as they know, induction heads are the first case where a mechanistic account has been provided for a phase change in machine learning. [p. 45]

## Learning Dynamics [p. 46]

If neural networks can genuinely be understood mechanistically, in terms of circuits, it seems like there almost *has to* be some way to understand the learning process in terms of the dynamics of circuits changing. Induction heads offer an interesting preliminary bridge between these topics, and are a source of optimism for such a connection. This section briefly reviews some strands of work on the learning dynamics side, which seem particularly promising to think about if one wanted to pursue such a connection further. [p. 46]

One remarkable result in learning dynamics, by Saxe *et al.* [23], has been the discovery of *closed form* solutions to learning dynamics for linear neural networks without activation functions. The exciting thing about this work is that it actually provides a simple way to conceptually think about neural network learning in a simplified case. (In follow up work, Saxe *et al.* also explore connections between this framework and models learning to represent semantic information [36].) The authors are unable to provide a detailed review of this work, but they note that Saxe *et al.*'s framework could naturally suggest a circuit lens for thinking about learning dynamics. Very roughly, they find that linear neural networks can be understood in terms of the evolution of independent paths through the network, with each path corresponding to a principal component of the data. These paths might be thought of as circuits. [p. 46]

Another interesting line of work has been the study of the geometry of neural network loss surfaces (e.g. [37, 38, 39, 40]). The authors' thoughts on the connection are more superficial, but it seems like there must be some way in which aspects of the loss surface connect to the formation of circuits. Very concretely, it seems like the phase change they have described in this paper must correspond to some very large feature in the loss landscape of transformers. [p. 46]

## Universality [p. 46]

In the context of interpretability and circuits, "universality" [41] or "convergent learning" [42] is when multiple models develop the same features and circuits. Universality might seem like an intellectual curiosity, but the circuits thread argues that universality plays a critical role in what kind of interpretability makes sense: [p. 46]

> "[I]magine the study of anatomy in a world where every species of animal had a completely unrelated anatomy: would we seriously study anything other than humans and a couple domestic animals? In the same way, the universality hypothesis determines what form of circuits research makes sense. If it was true in the strongest sense, one could imagine a kind of 'periodic table of visual features' which we observe and catalogue across models. On the other hand, if it was mostly false, we would need to focus on a handful of models of particular societal importance and hope they stop changing every year." [41] [p. 46]

Research on universality began with Li *et al.* [42] who showed that many neurons are highly correlated with neurons in retrained versions of the same model. More recently, a number of papers have shown that in aggregate, neural networks develop representations with a lot of shared information (e.g. [43, 44]). The Circuits thread tried to extend this notion of universality from features to *circuits*, finding that not only do at least some families of well-characterized neurons reoccur across multiple networks of different architectures and that the same circuits [41], but the same circuits appear to implement them [45]. [p. 46]

---
[p. 47 continued]

Certain kinds of universality are often implicitly assumed in the language model attention head interpretability literature. For example, it seems widely accepted that "previous token" attention heads form across many transformer language models (e.g. [26, 27]). The implicit hypothesis of universal attention heads -- that is, attention heads with the same attention patterns in different models -- isn't exactly the same thing as the kind of feature universality studied in the vision context, but is kind of analogous. [p. 47]

The authors' work in this paper has analogies to many of these strands of prior work. Like the previous attention head papers, they describe the induction head pattern as a universal attention pattern. However, their analysis of these heads' OV and QK circuits extends this claim of universality to the circuit level, similar to the original Circuits thread. And a corollary of their analysis of the OV circuit is a claim about what feature the attention head computes (roughly: the token embedding of the token following a previous copy of the present token) which is more similar to the traditional work on universality. [p. 47]

Separate from all of this, it's worth mentioning that increasingly there's evidence for a particularly extreme kind of universality at the intersection of neuroscience and deep learning. Increasingly, research suggests that biological and artificial neural networks learn similar representations (e.g. [46, 47, 48]). In fact, Goh *et al.* [49] find that multimodal "concept" neurons found in humans (such as the famous "Jennifer Anniston neuron") occur in neural networks. [p. 47]

## Attention Patterns in Translation-Like Tasks [p. 47]

In Argument 4, the authors saw an induction head that helps implement translation. Although they are not aware of anything quite so general in the prior literature, there are reports of attention patterns which, in retrospect, seem somewhat similar. Often, in translation-like tasks, attention attends to the token which is *about to be translated*. This is seen in literal translation (e.g. [50]) and also in voice recognition (e.g. [51]) where the model attends to the portion of the audio about to be transcribed. Visualizations of this in the encoder-decoder context often slightly obscure the induction-like nature of the attention patterns, because the decoder is visualized in terms of the output tokens predicted per time step rather than its input tokens. [p. 47]
