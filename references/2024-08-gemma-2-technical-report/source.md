# Gemma 2: Improving Open Language Models at a Practical Size

**Authors:** Gemma Team (Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, et al.)
**Affiliation:** Google DeepMind

## Publication Status

- **arXiv preprint:** August 2024, arXiv:2408.00118
- **Peer-reviewed:** No
- **Conference/Journal:** None
- **Status:** Preprint

Gemma 2 is the second generation of Google DeepMind's open-weight language models, released in sizes of 2B, 9B, and 27B parameters. The models were released under permissive licenses (Gemma License). The report introduces knowledge distillation as a training strategy for smaller models (2B and 9B), along with architectural modifications including interleaved local-global attention and logit soft-capping.

## Preferred Citation

> Gemma Team. (2024). Gemma 2: Improving Open Language Models at a Practical Size. arXiv:2408.00118.

## Links

- arXiv: https://arxiv.org/abs/2408.00118
- Models: https://huggingface.co/google/gemma-2-27b
- Code: https://github.com/google-deepmind/gemma
- Blog: https://blog.google/technology/developers/google-gemma-2/
