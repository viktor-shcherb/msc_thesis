# 4. Post-Training [p. 4]

For post-training, pre-trained models are fine-tuned into instruction-tuned models. The process involves: [p. 4]

1. Supervised fine-tuning (SFT) on a mix of text-only, English-only synthetic and human-generated prompt-response pairs.
2. RLHF on top of the SFT models with the reward model trained on labelled English-only preference data and the policy based on the same prompts as the SFT phase.
3. Averaging the models obtained after each phase to improve overall performance.

The final data mixtures and post-training recipe, including tuned hyperparameters, were chosen on the basis of improving helpfulness while minimizing model harms related to safety and hallucinations. [p. 4]

The post-training data was extended from Gemma 1.1 with a mixture of internal and external public data. In particular, the prompts but not the answers from LMSYS-chat-1M (Zheng et al., 2023) are used. All data goes through a filtering stage. [p. 4]

## Supervised Fine-Tuning (SFT)

Behavioral cloning is run on synthetic and real prompts, with responses predominantly synthetically generated by the teacher (a larger model). Distillation from the teacher on the student's distribution is also run (Agarwal et al., 2024; Gu et al., 2024). [p. 4]

## Reinforcement Learning from Human Feedback (RLHF)

A similar RLHF algorithm as Gemma 1.1 (Gemma Team, 2024) is used but with a different reward model, which is an order of magnitude larger than the policy. The new reward model is also oriented more towards conversational capabilities, specifically multi-turn. [p. 4]

## Model Merging

Different models obtained by running the pipeline with different hyperparameters are averaged (Ram√© et al., 2024). [p. 4]

## Data Filtering

When using synthetic data, several stages of filtering are run to remove examples that show certain personal information, unsafe or toxic model outputs, mistaken self-identification data, and duplicated examples. Following Gemini, the authors find that including subsets of data that encourage better in-context attribution, hedging, and refusals to minimize hallucinations improves performance on factuality metrics, without degrading model performance on other metrics. [p. 4]

## Formatting

Gemma 2 models are fine-tuned with the same control tokens as Gemma 1 models, as detailed in Table 4, but a different formatting schema. The model explicitly ends generations with `<end_of_turn><eos>` tokens, while previously it only generated `<eos>`. For the motivation behind this formatting structure, see Gemma 1. [p. 4]

## Table 5 | Example dialogue with user and model control tokens [p. 4]

**First turn:**

| Role | Content |
|---|---|
| User: | `<start_of_turn>user` Knock knock.`<end_of_turn>` `<start_of_turn>model` |
| Model: | Who's there?`<end_of_turn><eos>` |

**Second turn:**

| Role | Content |
|---|---|
| User: | `<start_of_turn>user` Knock knock.`<end_of_turn>` `<start_of_turn>model` |
| Model: | Who's there?`<end_of_turn>` |
| User: | `<start_of_turn>user` Gemma.`<end_of_turn>` `<start_of_turn>model` |
| Model: | Gemma who?`<end_of_turn><eos>` |

To proceed with multi-turn, remove the model-outputted `<eos>`, add back the usual user turn's control tokens and continue with the following turn's chat template. [p. 4]
