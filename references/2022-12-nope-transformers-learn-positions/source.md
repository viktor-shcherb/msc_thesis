# Transformer Language Models without Positional Encodings Still Learn Positional Information

**Authors:** Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, Omer Levy
**Affiliations:** Tel Aviv University, University of Washington, Intel Labs, Meta AI

## Publication Status

- **arXiv preprint:** March 2022, arXiv:2203.16634
- **Peer-reviewed:** Yes
- **Conference/Journal:** Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1382--1390, Abu Dhabi, United Arab Emirates, December 2022
- **Status:** Published conference paper (Findings)

## Preferred Citation

Cite the Findings of EMNLP 2022 version:

> Haviv, A., Ram, O., Press, O., Izsak, P., & Levy, O. (2022). Transformer Language Models without Positional Encodings Still Learn Positional Information. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1382--1390.

## Links

- arXiv: https://arxiv.org/abs/2203.16634
- Proceedings: https://aclanthology.org/2022.findings-emnlp.99/
- Code: https://github.com/adihaviv/NoPos
