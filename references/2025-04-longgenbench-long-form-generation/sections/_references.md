# References

This file contains only the references that were cited in the section notes.

## A

**An et al., 2024**
Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, and Jian-Guang Lou. Make your llm fully utilize the context. arXiv preprint arXiv:2404.16811, 2024.
- Cited in 03_experiments.md (Table 3) as FILM-7B model

**Achiam et al., 2023**
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
- Cited in 01_introduction.md as GPT-4 model with 128K token context window

**Anthropic, 2024a**
Anthropic. Introducing claude 2.1, 2024a. URL https://www.anthropic.com/index/claude-2-1. Accessed: 2024-01-23.
- Cited in 01_introduction.md as Claude 2.1 with 128K token context window

**Anthropic, 2024b**
Anthropic. Introducing the next generation of claude, 2024b. URL https://www.anthropic.com/news/claude-3-family. Accessed: 2024-03-27.
- Cited in 01_introduction.md as Claude 3 series handling inputs exceeding 1 million tokens

## B

**Bai et al., 2023**
Yushi Bai et al. LongBench: A bilingual, multitask benchmark for long context understanding. arXiv:2308.14508, 2023.
- Cited in 01_introduction.md and 05_related-work.md as a long-context benchmark
- Cited in Table 1 as a retrieval benchmark with ~8k average length

**Bai et al., 2024**
Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longwriter: Unleashing 10,000+ word generation from long context llms. arXiv preprint arXiv:2408.07055, 2024.
- Cited in 01_introduction.md for long-form text generation
- Cited in 03_experiments.md for LongWriter model excelling at word count requirements
- Cited in Table 1 as LongWrite benchmark with ~2.7K average length

## C

**Chan et al., 2023**
Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201, 2023.
- Cited in 05_related-work.md regarding improving LLM evaluations

**Chiang & Lee, 2023**
David Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human evaluations? In Anna Rogers Papers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 15607–15621. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-LONG.870.
- Cited in 05_related-work.md regarding long-form text generation evaluation methods

**Chiang et al., 2023**
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.
- Cited in 05_related-work.md regarding instruction tuning models

## D

**Dasigi et al., 2021**
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A dataset of information-seeking questions and answers anchored in research papers. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tür, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pp. 4599–4610. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.NAACL-MAIN.365.
- Cited in 01_introduction.md for comprehensive question answering with shorter outputs

**Dubey et al., 2024**
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.
- Cited in 01_introduction.md as LLaMa-3.2 with 128K token context window
- Cited in 03_experiments.md (Table 3) as LLama3.1-8B-instruction and LLama3.1-70B models

## F

**Fan et al., 2018**
Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Iryna Gurevych and Yusuke Miyao (eds.), Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 889–898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082.
- Cited in 01_introduction.md for shorter text outputs in existing benchmarks

**Fan et al., 2019a**
Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: long form question answering. In Anna Korhonen, David R. Traum, and Lluís Màrquez (eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pp. 3558–3567. Association for Computational Linguistics, 2019a. doi: 10.18653/V1/P19-1346.
- Cited in 01_introduction.md and 05_related-work.md for long-form question answering

**Fan et al., 2019b**
Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In Anna Korhonen, David Traum, and Lluís Màrquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3558–3567, Florence, Italy, July 2019b. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346.
- Cited in Table 1 as ELI5 benchmark for generation tasks with ~0.2K average length

**Fan et al., 2019c**
Angela Fan, Mike Lewis, and Yann N. Dauphin. Strategies for structuring story generation. In Anna Korhonen, David R. Traum, and Lluís Màrquez (eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pp. 2650–2660. Association for Computational Linguistics, 2019c. doi: 10.18653/V1/P19-1254.
- Cited in 05_related-work.md for story generation research

## H

**Hsieh et al., 2024**
Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. Ruler: What's the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024.
- Cited in 03_experiments.md, 04_analysis-and-limitations.md, and 05_related-work.md as RULER benchmark
- Cited in Table 1 as a retrieval benchmark with "Any" length support

**Hu et al., 2022**
Zhe Hu, Hou Pong Chan, Jiachen Liu, Xinyan Xiao, Hua Wu, and Lifu Huang. PLANET: dynamic content planning in autoregressive transformers for long-form text generation. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 2288–2305. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.ACL-LONG.163.
- Cited in 01_introduction.md for creative writing

**Hua & Wang, 2020**
Xinyu Hua and Lu Wang. PAIR: planning and iterative refinement in pre-trained transformers for long text generation. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 781–793. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.emnlp-main.57.
- Cited in 01_introduction.md for creative writing

## K

**Kamradt, 2023**
Gregory Kamradt. Needle In A Haystack - Pressure Testing LLMs. Github, 2023. URL https://github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main.
- Cited in 01_introduction.md as NIAH benchmark
- Cited in Table 1 as NIAH retrieval benchmark

**Kang & Hovy, 2020**
Dongyeop Kang and Eduard H. Hovy. Plan ahead: Self-supervised text planning for paragraph completion task. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 6533–6543. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.EMNLP-MAIN.529.
- Cited in 05_related-work.md for story generation

**Kumar et al., 2024**
Ishita Kumar, Snigdha Viswanathan, Sushrita Yerra, Alireza Salemi, Ryan A Rossi, Franck Dernoncourt, Hanieh Deilamsalehy, Xiang Chen, Ruiyi Zhang, Shubham Agarwal, et al. Longjump: A benchmark for personalized long-form text generation. arXiv preprint arXiv:2407.11016, 2024.
- Cited in 01_introduction.md for document summarization

**Kwon et al., 2023**
Woosuk Kwon et al. Efficient memory management for large language model serving with paged attention. In Proc. of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.
- Cited in 03_experiments.md as vLLM system for inference

## L

**Lee et al., 2023**
Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, Hong-In Lee, and Moontae Lee. QASA: advanced question answering on scientific articles. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 19036–19052. PMLR, 2023.
- Cited in 01_introduction.md and 05_related-work.md for comprehensive question answering

**Li et al., 2023**
Ruosen Li, Teerth Patel, and Xinya Du. Prd: Peer rank and discussion improve large language model based evaluations. arXiv preprint arXiv:2307.02762, 2023.
- Cited in 05_related-work.md regarding improving LLM evaluations

**Li et al., 2024**
Mo Li, Songyang Zhang, Yunxin Liu, and Kai Chen. Needlebench: Can llms do retrieval and reasoning in 1 million context window? arXiv preprint arXiv:2407.11963, 2024.
- Cited in 05_related-work.md as NeedleBench introducing the Ancestral Trace Challenge

**Liu et al., 2023**
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: NLG evaluation using gpt-4 with better human alignment. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 2511–2522. Association for Computational Linguistics, 2023.
- Cited in 05_related-work.md regarding evaluation methods for long-form generation

**Liu et al., 2024**
Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulic, Anna Korhonen, and Nigel Collier. Aligning with human judgement: The role of pairwise preference in large language model evaluators. arXiv preprint arXiv:2403.16950, 2024.
- Cited in 05_related-work.md regarding evaluation methods for long-form generation

## O

**OpenAI, 2022**
OpenAI. ChatGPT, 2022. URL https://chat.openai.com.
- Cited in 05_related-work.md regarding instruction tuning models

**OpenAI, 2023**
OpenAI. GPT-4 Technical Report. CoRR, abs/2303.08774, 2023. doi: 10.48550/arXiv.2303.08774.
- Cited in 03_experiments.md (Table 3) as GPT-4o model

**OpenAI, 2024a**
OpenAI. Gpt-4o-mini: Advancing cost-efficient intelligence, 2024a. URL https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/. Accessed: 2024-08-31.
- Cited in 03_experiments.md (Table 3) as GPT-4o-mini model

**Ouyang et al., 2022**
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730–27744, 2022.
- Cited in 05_related-work.md regarding instruction tuning models

## P

**Papineni et al., 2002**
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311–318, 2002.
- Cited in 05_related-work.md as BLEU metric

**Pezeshkpour & Hruschka, 2023**
Pouya Pezeshkpour and Estevam Hruschka. Large language models sensitivity to the order of options in multiple-choice questions. arXiv preprint arXiv:2308.11483, 2023.
- Cited in 05_related-work.md regarding LLM evaluation biases

## R

**Rafailov et al., 2024**
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.
- Cited in 05_related-work.md regarding instruction tuning models

## S

**Shaham et al., 2023**
Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. ZeroSCROLLS: A zero-shot benchmark for long text understanding. In EMNLP, 2023.
- Cited in 05_related-work.md as ZeroSCROLLS benchmark for long-document QA

**Shen et al., 2023**
Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, and Lidong Bing. Large language models are not yet human-level evaluators for abstractive summarization. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pp. 4215–4233. Association for Computational Linguistics, 2023.
- Cited in 05_related-work.md regarding text length limitations (under 2000 tokens)

**Stelmakh et al., 2022**
Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. ASQA: factoid questions meet long-form answers. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 8273–8288. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.EMNLP-MAIN.566.
- Cited in 01_introduction.md and 05_related-work.md for comprehensive question answering

## T

**Tan et al., 2024**
Haochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu, Zhili Liu, Xiaoguang Li, Yasheng Wang, Lifeng Shang, Qun Liu, and Linqi Song. Proxyqa: An alternative framework for evaluating long-form text generation with large language models. arXiv preprint arXiv:2401.15042, 2024.
- Cited in 05_related-work.md regarding recent work on evaluation criteria for long-form generation

**Taori et al., 2023**
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.
- Cited in 05_related-work.md regarding instruction tuning models

## W

**Wang et al., 2024**
Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 9440–9550, Bangkok, Thailand, August 2024. Association for Computational Linguistics, 2024.
- Cited in 05_related-work.md regarding LLM evaluation biases and meta-evaluation benchmarks

## X

**Xu et al., 2020**
Peng Xu, Mostofa Patwary, Mohammad Shoeybi, Raul Puri, Pascale Fung, Anima Anandkumar, and Bryan Catanzaro. MEGATRON-CNTRL: controllable story generation with external knowledge using large-scale language models. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 2831–2845. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.EMNLP-MAIN.226.
- Cited in 05_related-work.md for story generation

**Xu et al., 2022**
Jing Xu, Arthur Szlam, and Jason Weston. Beyond goldfish memory: Long-term open-domain conversation. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 5180–5197. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.ACL-LONG.356.
- Cited in 05_related-work.md for sustained conversation

## Y

**Yang et al., 2024**
An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024.
- Cited in 03_experiments.md (Table 3) as Qwen-72B and Qwen2-7B models

## Z

**Zhang et al., 2023**
Xinhua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li. Wider and deeper llm networks are fairer llm evaluators. arXiv preprint arXiv:2308.01862, 2023.
- Cited in 05_related-work.md regarding meta-evaluation benchmarks

**Zheng et al., 2023**
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.
- Cited in 05_related-work.md regarding improving LLM evaluations and meta-evaluation benchmarks

**Zheng et al., 2024**
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024.
- Cited in 01_introduction.md regarding LLM-as-a-judge evaluation methods
