# 1 Introduction [p. 1–2]

## Context and Motivation [p. 1]

Recent advances in large language models (LLMs) have dramatically enhanced their ability to process long text sequences, supporting applications from document summarization to creative writing. Leading models such as GPT-4 (Achiam et al., 2023), LLaMa-3.2 (Dubey et al., 2024), and Claude 2.1 (Anthropic, 2024a) manage context windows of up to 128K tokens, with the Claude 3 series (Anthropic, 2024b) handling inputs exceeding 1 million tokens. However, while much attention has been given to these models' ability to retrieve and understand long input sequences, far less focus has been placed on their ability to generate coherent and high-quality long-form text outputs—a critical requirement for tasks such as design proposals and creative writing [p. 1].

Long-form text generation is crucial for real-world applications that require detailed, well-structured narratives, such as document summarization (Kumar et al., 2024), creative writing (Hua & Wang, 2020; Hu et al., 2022), and comprehensive question answering (Stelmakh et al., 2022; Lee et al., 2023; Bai et al., 2024). Despite this importance, current benchmarks are limited in their ability to evaluate long-form generation, focusing instead on shorter text outputs (≤ 2K tokens) (Fan et al., 2018, 2019a; Dasigi et al., 2021), making them unsuitable for measuring outputs of ≥16K tokens (Bai et al., 2024). The challenge is further compounded by the lack of robust methods for evaluating these long sequences. The ability to follow instructions is essential for long text generation (Reversed NIAH¹), just as effective information retrieval is fundamental for processing long-context inputs (NIAH (Kamradt, 2023)). However, current benchmarks do not adequately assess whether the

---

[p. 2]

generated text adheres to the specific directives of a prompt. For instance, a prompt may require incorporating specific information at a certain point in a lengthy document, but evaluations often fail to verify the model's compliance with these instructions. This oversight represents a significant shortcoming in benchmarking, particularly because performance under explicit constraints typically predicts outcomes in tasks with more implicit requirements, such as long-form generation or academic paper production. If a model struggles with explicit requirements, it is likely to underperform in scenarios with subtler constraints [p. 2].

Manual evaluations, while thorough, are both costly and impractical at scale. Meanwhile, automated evaluations using "LLM-as-a-judge" methods (Zheng et al., 2024) often yield results that are difficult to interpret and may not align with human judgments, raising concerns about reliability. This highlights the need for more specialized benchmarks capable of reliably assessing the quality of super-long-form text generation [p. 2].

## LongGenBench Introduction [p. 2]

To address this gap, we present LongGenBench, a novel benchmark designed to evaluate the quality of super-long-form text generated by long-context LLMs. Unlike existing benchmarks that primarily test retrieval or reasoning over long inputs, LongGenBench focuses on the model's ability to generate content that follows complex instructions over extended sequences. Our benchmark introduces tasks that reflect real-world generation challenges, such as diary writing, menu planning, and urban design, where the text must adhere to constraints provided in the prompt. These tasks assess whether models can correctly incorporate specific details at designated points in the text, ensuring the generated content meets explicit requirements. By evaluating texts up to 32K tokens, LongGenBench is the first benchmark to systematically test the ability to generate instruction-compliant long-form content over extended lengths. Table 1 summarizes the different benchmarks supporting long-context retrieval and generation tasks [p. 2].

**Table 1:** Comparison of Long-Context LLM Benchmarks [p. 2]

| Type of Task | Benchmark | Type of Data | Avg Len | Long-Length |
|--------------|-----------|--------------|---------|-------------|
| Retrieval | Longbench (Bai et al., 2023) | hybrid | ∼8k | ✓ |
| Retrieval | NIAH (Kamradt, 2023) | synthetic | Any | ✓ |
| Retrieval | Ruler (Hsieh et al., 2024) | synthetic | Any | ✓ |
| Generation | ELI5 (Fan et al., 2019b) | hybrid | ∼0.2K | ✗ |
| Generation | LongWrite (Bai et al., 2024) | synthetic | ∼2.7K | ✗ |
| Generation | LongGenBench (Ours) | synthetic | ∼20K | ✓ |

*Note: For the retrieval tasks' datasets, we measure length based on the number of processing tokens, while for the generation tasks' datasets, we calculate the average number of generation words produced by LLMs. 'Long-length' indicates if LLMs to analyze or generate text that is at least 8K token.*

The evaluation tasks are organized into four distinct scenarios: Diary Writing, Menu Design, Skyscraper Design, and Urban Planning, each varying in complexity. The scenarios involve sub-tasks such as single instance, range, and periodicity, simulating realistic constraints that a model must account for. This setup allows us to measure the model's ability to generate detailed, contextually rich outputs that satisfy a wide array of criteria [p. 2].

## Contributions [p. 2]

In summary, our major contributions are as follows:

- To the best of our knowledge, this is the first study to address the challenge of super-long-form generation in long-context models, highlighting the critical importance of generating coherent, high-quality text in extended contexts.

- We introduce LongGenBench, a comprehensive dataset that provides a diverse set of tasks specifically designed to evaluate the long-form generation capabilities of LLMs across varying token lengths (16K and 32K) and levels of text complexity.

- We perform extensive experiments on both open-source and closed-source models, revealing that despite their advanced capabilities, most models struggle significantly with super-long-form generation tasks, particularly in maintaining instruction adherence and coherence over long outputs.

---

¹Analogous to NIAH, which involves searching for a needle (retrieval) within a long input, the reversed NIAH entails placing a specific needle (instruction-following) at a designated position within a long output.
