# 1 Introduction [p. 1–2]

This work studies the internal mechanism of how long-context language models can utilize information at arbitrary locations within the input. Recent advances in long-context language modeling [1, 20, 6] show inspiring results, particularly on the Needle-in-a-Haystack test [14], which asks the model to precisely retrieve the information of a short sentence (the needle) within a long context (the haystack). Such capability is the basis of more advanced long-context tasks, which usually interleaves retrieval and reasoning in a multi-step fashion [17].

Based on extensive experiments across 4 model families, 6 model scales, and 3 types of finetuning, the authors show that across the models' attention layers, there exist a small number of retrieval heads that search the information being asked, and redirect the relevant tokens from the input to the output. Activation of retrieval heads explains whether the output is factual or hallucinated. When such heads are activated, the model behaves faithful to the input document. When they are not activated, or intentionally masked out in controlled experiments (Fig. 1), the model cannot find the relevant information and hallucinates instead.

**Figure 1** (p. 2): "Retrieval heads are the heads that redirect information from the input to the output. Left: masking out the top retrieval heads of LLaMA 2 7B 80K, its Needle-in-a-Haystack performance drops significantly, and the model hallucinates during decoding. Right: masking out random non-retrieval heads does not influence the model's needle-in-a-haystack behavior. We further note that the retrieval head only influence factuality but not the language capability that they are masked, the model hallucinates by saying "go to the beach", which is still a fluent sentence, but not factual (as the correct answer is "eating a sandwich at Dolores park.")."

Description: Two heatmaps side by side showing model performance at different context lengths (5K-100K).
- Left heatmap: "Masking out top 20 retrieval heads, accuracy 63.6" - shows significant performance degradation with many red/orange regions indicating failures
- Right heatmap: "Masking out 20 random heads, accuracy 94.7" - shows predominantly green regions indicating maintained performance
- Key elements: Y-axis shows positions (0.1, 0.5, 0.9), X-axis shows context lengths (5K, 50K, 100K)
- Notable patterns: Masking retrieval heads causes substantial accuracy drop (63.6% vs 94.7%), while masking random heads has minimal impact
- Supports claim: Retrieval heads are causally responsible for long-context factuality

## Motivation and Inspiration [p. 2]

The discovery of the retrieval head is motivated by the question of what the attention mechanism is doing when the model can or cannot find the given needle. The authors take important inspiration from two existing works: the CopyNet [10] and the Induction Head [19].

The CopyNet is essentially a single-layer, single-head attention mechanism in the age of RNNs that copy-paste tokens from the input to the output. Induction Heads [19] are special heads within a multi-layer, multi-head attention network that implements an implicit in-context program called algorithm. Combining the observation from the two works, the authors naturally hypothesize that, just like induction heads are accountable for in-context learning, there might exist special heads that are accountable for information retrieval and implement a conditional copy-paste algorithm.

## Detection Approach and Main Findings [p. 2]

The authors design algorithms to detect retrieval heads within the transformer architecture (Sec. 2), and conduct large-scale experiments to demonstrate their properties (Sec. 3). The key properties of retrieval heads are universal and sparse: for any model family (LLaMA [21], Yi [25], QWen [2] and Mistral [12]), at any scale (6B, 14B, and 34B and 8×7B), either base or chat, either dense or MoE, as long as the model can precisely recite the input after seeing it once, they have a small number of retrieval heads (Fig. 1).

The retrieval heads are (2) intrinsic: the base model (e.g., LLaMA2 base) already contains retrieval heads (as a consequence of large-scale pretraining). Subsequent derivations, such as the long-context continue pretraining (LLaMA2 7B 80K), chat fine-tuning (Qwen Chat), or even sparse upcycling [16, 13] uses the same retrieval heads as the base model (Fig. 5). (3) They are dynamically activated according to the context: the strongest retrieval heads (e.g., 13 for LLaMA 2 7B) are always activated no matter what the required information is, while weaker retrieval heads are activated on different parts of the required information; consequently these heads compensate each other's functionality: removing a subset of the heads, the model at least partially retrieves some of the required information. (4) The retrieval heads are causal: say we put a needle "the best thing to do in San Francisco is to eat a sandwich in Dolores Park on a sunny day", completely masking out the retrieval heads, the model hallucinates (by saying the best thing is to visit Golden Gate bridge); partially masking out the heads, the model retrieves part of the needle (e.g., it gets the sandwich but forget the Dolores Park); masking out random non-retrieval heads, the model still find full needle; masking out the heads, the model still find full needle; even in some cases, the retrieval heads are not activated. The authors further note that chain-of-thought reasoning also heavily relies on retrieval heads to relate the model needs to the input information, indicating a complex relationship between the model's retrieval and reasoning capability.

## Implications [p. 2]

The discovery of retrieval head has profound implications on long-context modeling: (1) it marks a significant step forward in the field of mechanistic interpretability [3, 19] because for the first time the authors pin point a particular subnet implementing the conditional retrieval algorithm; (2) it explains why certain context-compression methods fail to keep factuality (because they removes the retrieval head, e.g., in Xiao et al. [24]), and suggests future research on KV cache compression [7, 15], a key problem for deploying long-context models, should consider the influence of retrieval heads.
