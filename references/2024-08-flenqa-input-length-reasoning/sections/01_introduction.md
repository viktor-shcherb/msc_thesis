# 1 Introduction [p. 1-2]

[p. 1] Recent advancements in LLMs show impressive performance across a range of tasks (OpenAI, 2023; Anil et al., 2023; Jiang et al., 2024), including answering complex questions requiring multiple reasoning steps (Kojima et al., 2022; Wei et al., 2022). These models also claim to support increasingly longer inputs. This development underscores the need to examine their performance on the longer inputs they are now technically supporting.

A reasonable assumption is that support for long inputs would transfer across tasks and enable a model adept at solving a task when presented in a short input prompt, to perform the same task when it is embedded within a longer prompt. Recent studies that benchmark models over tasks involving longer inputs, including reasoning tasks, indicate that models often struggle with reasoning over long inputs (Shaham et al., 2023; Li et al., 2023; Bai et al., 2023). However, these studies do not properly control their variables, and vary both the input length and the associated tasks to be performed. This makes it hard to say if the degraded performance is due to the requirement to work with longer input, or due to the task being generally harder.

[p. 2] The authors study the effect of increasing the input length on model performance, while keeping other factors as constant as possible. They employ a methodology to measure model performance trends as a function of input length, by isolating it as a variable, while keeping the underlying task intact.

They introduce **FLenQA** (**Fl**exible **Len**gth **Q**uestion **A**nswering) dataset (footnote 1: https://github.com/alonj/Same-Task-More-Tokens), a QA dataset for text-based reasoning. For each sample, composed of a True/False question over two pieces of information required to answer it (the context), they create multiple versions of different lengths by embedding the context parts within longer, irrelevant texts. The dataset is composed of tasks for which both pieces of information must be reasoned over together to correctly answer the question. The tasks are kept simple enough that models answer most of them correctly when presented on their own, with no additional padding.

**Key findings stated as contributions:**
- LLMs quickly degrade in their reasoning capabilities, even on input length of 3000 tokens, which is much shorter than their technical maximum (on average over all tested models, a drop in accuracy from 0.92 to 0.68). [p. 2]
- The effect of embedding the information pieces in various locations within the context, as well as with two kinds of contexts (similar to the information pieces, or dissimilar to them) is explored. Regardless of the experimental setting, there are similar trends of degradation.
- Next-word prediction performance of models on long inputs is uncorrelated with their performance on downstream tasks of reasoning on long inputs.
- While Chain-of-Thought (CoT) prompting (Kojima et al., 2022; Wei et al., 2022) increases performance in short inputs, in most models it does not mitigate the degradation of performance when inputs are longer: while CoT prompting increases the accuracy over non-CoT prompting, the amount of increase is roughly consistent across context lengths, and is far from closing the performance drop due to long context. The only exception to that is GPT4 (footnote 2: the authors refer to the models gpt-4-1106-preview, gpt-3.5-turbo-1106 as GPT4 and GPT3.5 accordingly), in which the gap between CoT and normal prompting increases as the input is longer.
- The authors analyse results and identify several failure modes in model responses. With longer inputs models tend not to follow specific instructions in the input, either providing no answer, or in the case of CoT prompting, presenting the final answer before outlining the reasoning steps. They also observe a bias towards answering "false", as well as a decline in the models' ability to incorporate relevant information in their responses, as input length increases.

**Figure 1** (p. 1): "Reasoning performance drops as input grows, across a variety of tasks. Inputs are composed of text containing information relevant to the task (in red), and irrelevant text (grey) which is drawn from various sources and extended incrementally. Two separate text spans are required to answer correctly, and are located randomly in the input. Each point reflects the performance across 600 samples."
- Title: "Reasoning over input text"
- X-axis: Input length (# tokens), values: 250, 500, 1000, 1500, 2000, 2500, 3000
- Y-axis: Accuracy, range approximately 0.6 to 1.0
- Lines for GPT3.5 (blue), GPT4 (green), Gemini Pro (yellow/gold), Mistral Medium (purple), Mixtral 8x7B (pink/magenta)
- Dashed lines for CoT prompting, solid lines for Normal prompting
- All models show declining accuracy as input length increases. GPT4 with CoT maintains highest performance (staying above ~0.9). GPT3.5 drops most steeply. The degradation is consistent across all models, though at different rates. CoT prompting generally provides a consistent uplift but does not close the gap caused by longer inputs, except for GPT4 where CoT advantage grows with length.
