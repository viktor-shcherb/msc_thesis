# References

## An et al., 2023a
Chen An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023a. L-eval: Instituting standardized evaluation for long context language models. *ArXiv*, abs/2307.11088.
- Cited in 08_related-work.md as a benchmark for long-input evaluation.

## An et al., 2023b
Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023b. L-eval: Instituting standardized evaluation for long context language models.
- Cited in 08_related-work.md as a benchmark for long-input evaluation.

## Anil et al., 2023
Rohan Anil, Sebastian Borgeaud, Yonghui Wu, and Gemini Team Google. 2023. Gemini: A family of highly capable multimodal models. *ArXiv*, abs/2312.11805.
- Cited in 01_introduction.md as example of recent LLM advances; in 05_correlation-next-word-prediction.md as benchmark using perplexity; in 08_related-work.md for next word prediction evaluation.

## Bai et al., 2023
Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023. Longbench: A bilingual, multitask benchmark for long context understanding. *ArXiv*, abs/2308.14508.
- Cited in 01_introduction.md and 08_related-work.md as a long-input benchmark.

## Chen and Durrett, 2019
Jifan Chen and Greg Durrett. 2019. Understanding dataset design choices for multi-hop reasoning. In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*.
- Cited in 02_desired-data-properties.md regarding models answering existing reasoning datasets with partial information.

## Clark et al., 2021
Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2021. Transformers as soft reasoners over language. In *Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence*, pages 3882-3890.
- Cited in 03_flenqa.md as the source of the Ruletaker benchmark; in 10_appendix-a-datasets.md as the original data source.

## Cobbe et al., 2021
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. *arXiv preprint arXiv:2110.14168*.
- Cited in 04_main-experiments.md as the GSM-8K dataset used by Shi et al.

## Dasgupta et al., 2022
Ishita Dasgupta, Andrew K Lampinen, Stephanie CY Chan, Antonia Creswell, Dharshan Kumaran, James L McClelland, and Felix Hill. 2022. Language models show human-like content effects on reasoning. *arXiv preprint arXiv:2207.07051*.
- Cited in 08_related-work.md as prior work on input intervention studying semantic content.

## Ding et al., 2024
Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. 2024. Longrope: Extending llm context window beyond 2 million tokens.
- Cited in 05_correlation-next-word-prediction.md as work using perplexity as benchmark for long inputs.

## Faraglia and Contributors, 2012
Daniele Faraglia and Other Contributors. 2012. Faker.
- Cited in 03_flenqa.md as the Python library used for random name generation.

## Gidiotis and Tsoumakas, 2020
Alexios Gidiotis and Grigorios Tsoumakas. 2020. A divide-and-conquer approach to the summarization of long documents. *IEEE/ACM Transactions on Audio, Speech, and Language Processing*, 28:3029-3040.
- Cited in 02_desired-data-properties.md as example of divide-and-conquer approaches to long documents.

## Holtzman et al., 2023
Ari Holtzman, Peter West, and Luke Zettlemoyer. 2023. Generative models as a complex systems science: How can we make sense of large language model behavior? *arXiv preprint arXiv:2308.00189*.
- Cited in 02_desired-data-properties.md regarding the behavioral approach relying on input intervention.

## Huang and Chang, 2022
Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards reasoning in large language models: A survey. *arXiv preprint arXiv:2212.10403*.
- Cited in 02_desired-data-properties.md regarding tasks requiring drawing conclusions from evidence.

## Jacovi et al., 2023
Alon Jacovi, Avi Caciularu, Omer Goldman, and Yoav Goldberg. 2023. Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks. In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages 5075-5084, Singapore. Association for Computational Linguistics.
- Cited in 02_desired-data-properties.md regarding data contamination concerns.

## Jiang et al., 2024
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lelio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Theophile Gervet, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. 2024. Mixtral of experts.
- Cited in 01_introduction.md as example of recent LLM advances; in 05_correlation-next-word-prediction.md regarding perplexity benchmarks; in 08_related-work.md for next word prediction evaluation.

## Jin et al., 2024
Mingyu Jin, Qinkai Yu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du, et al. 2024. The impact of reasoning step length on large language models. *arXiv preprint arXiv:2401.04925*.
- Cited in 08_related-work.md as prior work on prompting strategies through input intervention.

## Kojima et al., 2022
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. *Advances in neural information processing systems*, 35:22199-22213.
- Cited in 01_introduction.md for multi-step reasoning; in 06_chain-of-thought.md as introducing CoT prompting; in 07_failure-modes.md regarding reasoning steps after the answer; in 08_related-work.md as prior work on prompting strategies.

## Levy et al., 2023
Mosh Levy, Shauli Ravfogel, and Yoav Goldberg. 2023. Guiding llm to fool itself: Automatically manipulating machine reading comprehension shortcut triggers. In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pages 8495-8505.
- Cited in 08_related-work.md as prior work on QA task properties through input intervention.

## Li et al., 2023
Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023. Loogle: Can long-context language models understand long contexts? *ArXiv*, abs/2311.04939.
- Cited in 01_introduction.md as a study showing models struggle with reasoning over long inputs.

## Liu et al., 2022
Yang Liu, Chenguang Zhu, and Michael Zeng. 2022. End-to-end segmentation-based news summarization. In *Findings of the Association for Computational Linguistics: ACL 2022*, pages 544-554.
- Cited in 02_desired-data-properties.md as example of divide-and-conquer approaches.

## Liu et al., 2023a
Hong Liu, Sang Michael Xie, Zhiyuan Li, and Tengyu Ma. 2023a. Same pre-training loss, better downstream: Implicit bias matters for language models. In *International Conference on Machine Learning*, pages 22188-22214. PMLR.
- Cited in 05_correlation-next-word-prediction.md and 08_related-work.md regarding perplexity not correlating with downstream performance.

## Liu et al., 2023b
Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023b. Lost in the middle: How language models use long contexts. *arXiv preprint arXiv:2307.03172*.
- Cited in 04_main-experiments.md regarding position of answer affecting model ability in extractive QA (recency bias finding).

## Mao et al., 2020
Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. 2020. Generation-augmented retrieval for open-domain question answering. In *Annual Meeting of the Association for Computational Linguistics*.
- Cited in 03_flenqa.md regarding the RAG setup resemblance of their similar-padding approach.

## Min et al., 2019
Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019. Compositional questions do not necessitate multi-hop reasoning. In *Annual Meeting of the Association for Computational Linguistics*.
- Cited in 02_desired-data-properties.md regarding models answering datasets with partial information.

## OpenAI, 2023
OpenAI. 2023. Gpt-4 technical report.
- Cited in 01_introduction.md as example of recent LLM advances.

## Sainz et al., 2023
Oscar Sainz, Jon Campos, Iker Garcia-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre. 2023. Nlp evaluation in trouble: On the need to measure llm data contamination for each benchmark. In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pages 10776-10787.
- Cited in 02_desired-data-properties.md regarding data contamination.

## Shaham et al., 2022
Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, et al. 2022. Scrolls: Standardized comparison over long language sequences. In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*, 12007-12021.
- Cited in 08_related-work.md as a benchmark for long-input evaluation.

## Shaham et al., 2023
Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. 2023. Zeroscrolls: A zero-shot benchmark for long text understanding. *arXiv preprint arXiv:2305.14196*.
- Cited in 01_introduction.md and 08_related-work.md as a study showing models struggle with reasoning over long inputs / as a long-input benchmark.

## Shi et al., 2023
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Scharli, and Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context. In *International Conference on Machine Learning*, pages 31210-31227. PMLR.
- Cited in 04_main-experiments.md regarding demonstration that appending irrelevant texts reduces model performance on GSM-8K.

## Sinha et al., 2018
Koustuv Sinha, Shagun Sodhani, William L. Hamilton, and Joelle Pineau. 2018. Compositional language understanding with text-based relational reasoning. *ArXiv*, abs/1811.02959.
- Cited in 03_flenqa.md as inspiration for monotonic relations describing kinship.

## Tay et al., 2022
Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. 2022. Scaling laws vs model architectures: How does inductive bias influence scaling? *arXiv preprint arXiv:2207.10551*.
- Cited in 05_correlation-next-word-prediction.md and 08_related-work.md regarding perplexity not correlating with downstream performance.

## Wei et al., 2022
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. *Advances in Neural Information Processing Systems*, 35:24824-24837.
- Cited in 01_introduction.md for multi-step reasoning; in 06_chain-of-thought.md as introducing CoT prompting.

## Weston et al., 2016
Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merrienboer, Armand Joulin, and Tomas Mikolov. 2016. Towards ai-complete question answering: A set of prerequisite toy tasks. In *4th International Conference on Learning Representations, ICLR 2016*.
- Cited in 03_flenqa.md as the source of the bAbI tasks inspiring PIR.

## Wolhandler et al., 2022
Ruben Wolhandler, Arie Cattan, Ori Ernst, and Ido Dagan. 2022. How "multi" is multi-document summarization? In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*, pages 5761-5769.
- Cited in 02_desired-data-properties.md as example of divide-and-conquer approaches.

## Xia et al., 2022
Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, and Ves Stoyanov. 2022. Training trajectories of language models across scales. *arXiv preprint arXiv:2212.09803*.
- Cited in 05_correlation-next-word-prediction.md and 08_related-work.md regarding perplexity not correlating with downstream performance.

## Yao et al., 2023
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. *arXiv preprint arXiv:2305.10601*.
- Cited in 08_related-work.md as prior work on prompting strategies through input intervention.

## Zhou et al., 2022
Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2022. Large language models are human-level prompt engineers. *arXiv preprint arXiv:2211.01910*.
- Cited in 06_chain-of-thought.md as source of the optimised CoT elicitation string.

## Zhu et al., 2015
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Santi Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In *The IEEE International Conference on Computer Vision (ICCV)*.
- Cited in 03_flenqa.md as the source of the Books Corpus used for "different" padding.
