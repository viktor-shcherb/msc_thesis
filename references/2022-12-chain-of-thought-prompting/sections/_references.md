# References

References cited in the section notes, with full bibliographic information from the paper's bibliography.

---

**Ahn et al. (2022)**
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. 2022. Do as I can, not as I say: Grounding language in robotic affordances. *arXiv preprint arXiv:2204.01691*.
- Cited in 04_commonsense-reasoning.md as the source of the SayCan dataset and training set exemplars; 14_appendix-e.md for dataset details and license.

**Amini et al. (2019)**
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics*, Minneapolis, Minnesota. Association for Computational Linguistics.
- Cited in 01_introduction.md and 07_related-work.md as part of the literature using formal languages to reason about math word problems.

**BIG-bench collaboration (2021)**
BIG-bench collaboration. 2021. Beyond the imitation game: Measuring and extrapolating the capabilities of language models. *In preparation*.
- Cited in 04_commonsense-reasoning.md as the source of the Date Understanding and Sports Understanding evaluation sets; 14_appendix-e.md for dataset details and licenses.

**Brown et al. (2020)**
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. *NeurIPS*.
- Cited in 01_introduction.md for scaling benefits and few-shot prompting; 03_arithmetic-reasoning.md as the standard prompting baseline; 07_related-work.md as popularizing few-shot prompting; 10_appendix-a.md regarding small models' weak arithmetic abilities.

**Chen et al. (2019)**
Xinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, and Quoc V. Le. 2019. Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension. *ICLR*.
- Cited in 01_introduction.md and 07_related-work.md as part of the literature using formal languages to reason.

**Chen et al. (2021)**
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. *arXiv preprint arXiv:2107.03374*.
- Cited in 03_arithmetic-reasoning.md as the source of Codex (code-davinci-002); 12_appendix-c.md in the program synthesis discussion.

**Chiang and Chen (2019)**
Ting-Rui Chiang and Yun-Nung Chen. 2019. Semantically-aligned equation generation for solving and reasoning math word problems. In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics*, pages 2656--2668, Minneapolis, Minnesota. Association for Computational Linguistics.
- Cited in 01_introduction.md and 07_related-work.md as part of the literature using formal languages to reason.

**Cobbe et al. (2021)**
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. *arXiv preprint arXiv:2110.14168*.
- Cited in 01_introduction.md for rationale-augmented finetuning; 03_arithmetic-reasoning.md as the source of GSM8K and as the concise chain-of-thought style baseline; 07_related-work.md as extending Ling et al. (2017); 10_appendix-a.md regarding the annotation process for chain-of-thought exemplars and GSM8K crowd-worker reasoning chains; 11_appendix-b.md as source of prior best on GSM8K and for the observation about arithmetic errors in chains of thought.

**Devlin et al. (2019)**
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. *NAACL*.
- Cited in 01_introduction.md as part of the language model revolution.

**Geva et al. (2021)**
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies. *TACL*.
- Cited in 04_commonsense-reasoning.md as the source of StrategyQA and as a prior best leaderboard result.

**Hendrycks et al. (2021)**
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. *arXiv preprint arXiv:2103.03874*.
- Cited in 03_arithmetic-reasoning.md as evidence that arithmetic reasoning is challenging for language models.

**Jie et al. (2022)**
Zhanming Jie, Jierui Li, and Wei Lu. 2022. Learning to reason deductively: Math word problem solving as complex relation extraction. *arXiv preprint arXiv:2203.10316*.
- Cited in 03_arithmetic-reasoning.md (Figure 4 caption) as the prior best on SVAMP.

**Kaplan et al. (2020)**
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. *arXiv preprint arXiv:2001.08361*.
- Cited in 01_introduction.md for scaling benefits of language models; 10_appendix-a.md regarding scaling conferring benefits such as improved performance and sample efficiency.

**Koncel-Kedziorski et al. (2016)**
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS: A math word problem repository. *NAACL*.
- Cited in 03_arithmetic-reasoning.md as the source of the AQuA and MAWPS benchmarks.

**Lampinen et al. (2022)**
Andrew K. Lampinen, Ishita Dasgupta, Stephanie C.Y. Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L. McClelland, Jane X. Wang, and Felix Hill. 2022. Can language models learn from explanations in context? *arXiv preprint arXiv:2204.02329*.
- Cited in 02_chain-of-thought-prompting.md noting that solutions/explanations typically come after the final answer.

**Lan et al. (2021)**
Yihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan, Bing Tian Dai, Yan Wang, Dongxiang Zhang, and Ee-Peng Lim. 2021. MWPToolkit: An open-source framework for deep learning-based math word problem solvers. *arXiv preprint arXiv:2109.00799*.
- Cited in 03_arithmetic-reasoning.md (Figure 4 caption) as the prior best on MAWPS; 11_appendix-b.md (Table 1 caption) as prior best on ASDiv.

**Le Scao and Rush (2021)**
Teven Le Scao and Alexander Rush. 2021. How many data points is a prompt worth? *NAACL*.
- Cited in 03_arithmetic-reasoning.md regarding expected variance in exemplar-based prompting.

**Lester et al. (2021)**
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. *EMNLP*.
- Cited in 07_related-work.md as an approach for automatically learning prompts; 12_appendix-c.md in the prompting discussion.

**Ling et al. (2017)**
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. *ACL*.
- Cited in 01_introduction.md and 07_related-work.md as pioneering the idea of using natural language rationales to solve math word problems.

**Miao et al. (2020)**
Shen Yun Miao, Chao Chun Liang, and Keh Yih Su. 2020. A diverse corpus for evaluating and developing English math word problem solvers. *ACL*.
- Cited in 03_arithmetic-reasoning.md as the source of the ASDiv dataset.

**Narang et al. (2020)**
Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan. 2020. WT5?! Training text-to-text models to explain their predictions. *arXiv preprint arXiv:2004.14546*.
- Cited in 02_chain-of-thought-prompting.md noting that solutions/explanations typically come after the final answer; 12_appendix-c.md in the natural language explanations discussion.

**Nye et al. (2021)**
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. *arXiv preprint arXiv:2112.00114*.
- Cited in 07_related-work.md for predicting intermediate computational results in program synthesis; 12_appendix-c.md as the closest program execution work.

**Ouyang et al. (2022)**
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. *arXiv preprint arXiv:2203.02155*.
- Cited in 03_arithmetic-reasoning.md for InstructGPT parameter correspondence; 07_related-work.md as an instruction-based prompting approach; 12_appendix-c.md in the prompting discussion.

**Patel et al. (2021)**
Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? *NAACL*.
- Cited in 03_arithmetic-reasoning.md as evidence that arithmetic reasoning is challenging for language models and as the source of the SVAMP dataset.

**Peters et al. (2018)**
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. *NAACL*.
- Cited in 01_introduction.md as part of the language model revolution.

**Rae et al. (2021)**
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis & insights from training Gopher. *arXiv preprint arXiv:2112.11446*.
- Cited in 01_introduction.md noting that scaling model size alone has not proved sufficient for reasoning tasks.

**Rashkin et al. (2021)**
Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2021. Measuring attribution in natural language generation models. *arXiv preprint arXiv:2112.12870*.
- Cited in 06_discussion.md regarding improving factual generations as future work; 13_appendix-d.md regarding factuality of language model generations.

**Reynolds and McDonell (2021)**
Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. *Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems*.
- Cited in 03_arithmetic-reasoning.md regarding expected variance in exemplar-based prompting.

**Roy and Roth (2015)**
Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. *EMNLP*.
- Cited in 01_introduction.md as a neuro-symbolic method using formal languages.

**Roy et al. (2015)**
Subhro Roy, Tim Vieira, and Dan Roth. 2015. Reasoning about Quantities in Natural Language. *TACL*.
- Cited in 07_related-work.md as part of the literature using formal languages to reason.

**Sanh et al. (2022)**
Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2022. Multitask prompted training enables zero-shot task generalization. *ICLR*.
- Cited in 07_related-work.md as an instruction-based prompting approach; 12_appendix-c.md in the prompting discussion.

**Talmor et al. (2019)**
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. *NAACL*.
- Cited in 04_commonsense-reasoning.md as the source of the CSQA dataset and as a prior best leaderboard result.

**Talmor et al. (2021)**
Alon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bhagavatula, Yoav Goldberg, Yejin Choi, and Jonathan Berant. 2021. CommonsenseQA 2.0: Exposing the limits of ai through gamification. *NeurIPS Track on Datasets and Benchmarks*.
- Cited in 04_commonsense-reasoning.md noting that commonsense reasoning is still beyond the reach of current NLU systems.

**Tay et al. (2022)**
Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, and Donald Metzler. 2022. Unifying language learning paradigms. *arXiv preprint arXiv:2205.05131*.
- Cited in 03_arithmetic-reasoning.md as the source of UL2 20B.

**Thoppilan et al. (2022)**
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. LaMDA: Language models for dialog applications. *arXiv preprint arXiv:2201.08239*.
- Cited in 03_arithmetic-reasoning.md as the source of LaMDA models.

**Wang et al. (2022a)**
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022a. Self-consistency improves chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*.
- Cited in 03_arithmetic-reasoning.md as follow-up work showing chain-of-thought prompting can be improved by majority voting over sampled generations.

**Wei et al. (2022a)**
Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022a. Finetuned language models are zero-shot learners. *ICLR*.
- Cited in 07_related-work.md as an instruction-based prompting approach.

**Wei et al. (2022b)**
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022b. Emergent abilities of large language models. *Transactions on Machine Learning Research*.
- Cited in 03_arithmetic-reasoning.md and 06_discussion.md regarding chain-of-thought prompting as an emergent ability of model scale.

**Wiegreffe et al. (2022)**
Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi. 2022. Reframing human-AI collaboration for generating free-text explanations. *NAACL*.
- Cited in 02_chain-of-thought-prompting.md and 06_discussion.md regarding explanations and improving factual generations; 12_appendix-c.md in the natural language explanations discussion; 13_appendix-d.md regarding factuality of language model generations.

**Ye and Durrett (2022)**
Xi Ye and Greg Durrett. 2022. The unreliability of explanations in few-shot in-context learning. *arXiv preprint arXiv:2205.03401*.
- Cited in 06_discussion.md regarding improving factual generations as future work; 13_appendix-d.md regarding factuality of language model generations.

**Zhao et al. (2021)**
Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. *ICML*.
- Cited in 03_arithmetic-reasoning.md for exemplar sensitivity (GPT-3 on SST-2 ranging from 54.3% to 93.4%) and expected variance in exemplar-based prompting; 10_appendix-a.md regarding prompt order sensitivity and classification bias from exemplar ordering.

**Min et al. (2022)**
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? *arXiv preprint arXiv:2202.12837*.
- Cited in 10_appendix-a.md regarding work showing that prompts affect language models in unexpected ways.

**Pi et al. (2022)**
Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and Weizhu Chen. 2022. Reasoning like program executors. *arXiv preprint arXiv:2201.11473*.
- Cited in 11_appendix-b.md (Table 1 caption) as the source of prior best numbers on SVAMP and MAWPS.

**Piekos et al. (2021)**
Piotr Piekos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving BERT's mathematical abilities by predicting the order of reasoning. *ACL*.
- Cited in 11_appendix-b.md (Table 1 caption) as the source of prior best number on AQuA; 12_appendix-c.md in the numeric and logical reasoning discussion.

**Hosseini et al. (2014)**
Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. *EMNLP*.
- Cited in 14_appendix-e.md as part of the Math Word Problem Repository (AddSub dataset source).

**Marasovic et al. (2022)**
Ana Marasovic, Iz Beltagy, Doug Downey, and Matthew E Peters. 2022. Few-shot self-rationalization with natural language prompts. *NAACL Findings*.
- Cited in 12_appendix-c.md in the natural language explanations discussion; 13_appendix-d.md regarding recent work evaluating the factuality of language model generations and explanations.

**Maynez et al. (2020)**
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In *ACL*.
- Cited in 13_appendix-d.md regarding recent work evaluating the factuality of language model generations.

**Shen et al. (2021)**
Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. 2021. Generate & rank: A multi-task framework for math word problems. In *Findings of the Association for Computational Linguistics: EMNLP 2021*.
- Cited in 13_appendix-d.md as a potential verifier-based approach for improving reasoning quality.

**Andor et al. (2019)**
Daniel Andor, Luheng He, Kenton Lee, and Emily Pitler. 2019. Giving BERT a calculator: Finding operations and arguments with reading comprehension. *EMNLP*.
- Cited in 12_appendix-c.md in the numeric and logical reasoning discussion for augmenting BERT with executable operations.

**Andreas et al. (2018)**
Jacob Andreas, Dan Klein, and Sergey Levine. 2018. Learning with latent language. *NAACL*.
- Cited in 12_appendix-c.md in the intermediate language steps discussion for image and reinforcement learning settings.

**Austin et al. (2021)**
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. *arXiv preprint arXiv:2108.07732*.
- Cited in 12_appendix-c.md in the program synthesis and execution discussion.

**Bostrom et al. (2021)**
Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, and Greg Durrett. 2021. Flexible generation of natural language deductions. *EMNLP*.
- Cited in 12_appendix-c.md in the natural language explanations discussion.

**Cai et al. (2017)**
Jonathon Cai, Richard Shin, and Dawn Song. 2017. Making neural programming architectures generalize via recursion. *ICLR*.
- Cited in 12_appendix-c.md in the program synthesis and execution discussion.

**Camburu et al. (2018)**
Oana-Maria Camburu, Tim Rocktaschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI: Natural language inference with natural language explanations. *NeurIPS*.
- Cited in 12_appendix-c.md in the natural language explanations and intermediate language steps discussions.

**Chen et al. (2022)**
Howard Chen, Jacqueline He, Karthik Narasimhan, and Danqi Chen. 2022. Can rationalization improve robustness? *NAACL*.
- Cited in 12_appendix-c.md in the intermediate language steps discussion for improving robustness.

**Clark et al. (2020)**
Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over language. *IJCAI*.
- Cited in 12_appendix-c.md in the numeric and logical reasoning discussion for verbalizing rules in natural language.

**Dong et al. (2019)**
Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. 2019. Neural logic machines. *ICLR*.
- Cited in 12_appendix-c.md in the program synthesis and execution discussion.

**Dua et al. (2020)**
Dheeru Dua, Sameer Singh, and Matt Gardner. 2020. Benefits of intermediate annotations in reading comprehension. *ACL*.
- Cited in 12_appendix-c.md in the intermediate language steps discussion for mitigating bias.

**Gu et al. (2022)**
Yuling Gu, Bhavana Dalvi Mishra, and Peter Clark. 2022. DREAM: Uncovering mental models behind language models. *NAACL*.
- Cited in 12_appendix-c.md in the intermediate language steps discussion for improving performance.

**Hancock et al. (2018)**
Braden Hancock, Paroma Varma, Stephanie Wang, Martin Bringmann, Percy Liang, and Christopher Re. 2018. Training classifiers with natural language explanations. *ACL*.
- Cited in 12_appendix-c.md in the intermediate language steps discussion for speeding up training.

**Hase and Bansal (2022)**
Peter Hase and Mohit Bansal. 2022. When can models learn from explanations? a formal framework for understanding the roles of explanation data. *ACL*.
- Cited in 12_appendix-c.md in the intermediate language steps discussion for improving performance.

**Lev et al. (2004)**
Iddo Lev, Bill MacCartney, Christopher Manning, and Roger Levy. 2004. Solving logic puzzles: From robust processing to precise semantics. *Proceedings of the 2nd Workshop on Text Meaning and Interpretation*.
- Cited in 12_appendix-c.md in the numeric and logical reasoning discussion.

**Li and Liang (2021)**
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. *ACL*.
- Cited in 12_appendix-c.md in the prompting discussion as a general prompting approach.

**Liang et al. (2021)**
Zhengzhong Liang, Steven Bethard, and Mihai Surdeanu. 2021. Explainable multi-hop verbal reasoning through internal monologue. *NAACL*.
- Cited in 12_appendix-c.md in the numeric and logical reasoning discussion for verbalizing rules.

**Liu et al. (2021)**
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. *arXiv preprint arXiv:2107.13586*.
- Cited in 12_appendix-c.md as a survey of prompting methods.

**Majumder et al. (2021)**
Bodhisattwa Prasad Majumder, Oana-Maria Camburu, Thomas Lukasiewicz, and Julian McAuley. 2021. Rationale-inspired natural language explanations with commonsense. *arXiv preprint arXiv:2106.13876*.
- Cited in 12_appendix-c.md in the natural language explanations discussion.

**Raffel et al. (2020)**
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. *Journal of Machine Learning Research*, 21:1--67.
- Cited in 12_appendix-c.md in the prompting discussion for instruction-based approaches.

**Rajagopal et al. (2021)**
Dheeraj Rajagopal, Vidhisha Balachandran, Eduard H. Hovy, and Yulia Tsvetkov. 2021. SelfExplain: A self-explaining architecture for neural text classifiers. *EMNLP*.
- Cited in 12_appendix-c.md in the natural language explanations discussion for improving model interpretability.

**Rajani et al. (2019)**
Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain yourself! Leveraging language models for commonsense reasoning. *ACL*.
- Cited in 12_appendix-c.md in the intermediate language steps discussion for finetuning on manually annotated training datasets.

**Ran et al. (2019)**
Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. 2019. NumNet: Machine reading comprehension with numerical reasoning. *EMNLP*.
- Cited in 12_appendix-c.md in the numeric and logical reasoning discussion for including a graph neural network.

**Recchia (2021)**
Gabriel Recchia. 2021. Teaching autoregressive language models complex tasks by demonstration. *arXiv preprint arXiv:2109.02102*.
- Cited in 12_appendix-c.md as the most-related work in the numeric and logical reasoning discussion for finetuning to enable longhand module operations.

**Reif et al. (2022)**
Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. 2022. A recipe for arbitrary text style transfer with large language models. *ACL*.
- Cited in 12_appendix-c.md in the prompting discussion as a general prompting approach.

**Saeed et al. (2021)**
Mohammed Saeed, Naser Ahmadi, Preslav Nakov, and Paolo Papotti. 2021. RuleBERT: Teaching soft rules to pre-trained language models. *EMNLP*.
- Cited in 12_appendix-c.md in the numeric and logical reasoning discussion for verbalizing rules.

**Talmor et al. (2020)**
Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, and Jonathan Berant. 2020. Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge. *NeurIPS*.
- Cited in 12_appendix-c.md in the intermediate language steps discussion for generating synthetic datasets.

**Wang et al. (2022b)**
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. 2022b. Benchmarking generalization via in-context instructions on 1,600+ language tasks. *arXiv preprint arXiv:2204.07705*.
- Cited in 12_appendix-c.md in the prompting discussion for instruction-based approaches.

**Wiegreffe and Marasovic (2021)**
Sarah Wiegreffe and Ana Marasovic. 2021. Teach me to explain: A review of datasets for explainable NLP. *NeurIPS*.
- Cited in 12_appendix-c.md in the natural language explanations discussion.

**Wiegreffe et al. (2021)**
Sarah Wiegreffe, Ana Marasovic, and Noah A. Smith. 2021. Measuring association between labels and free-text rationales. *EMNLP*.
- Cited in 12_appendix-c.md in the natural language explanations discussion.

**Wu et al. (2022a)**
Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina, Michael Terry, and Carrie J Cai. 2022a. PromptChainer: Chaining large language model prompts through visual programming. *CHI Extended Abstracts*.
- Cited in 12_appendix-c.md for combining sequential generations of language models.

**Wu et al. (2022b)**
Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022b. AI chains: Transparent and controllable human-AI interaction by chaining large language model prompts. *CHI*.
- Cited in 12_appendix-c.md for combining sequential generations of language models.

**Yan et al. (2020)**
Yujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan, and Milad Hashemi. 2020. Neural execution engines: Learning to execute subroutines. *NeurIPS*.
- Cited in 12_appendix-c.md in the program synthesis and execution discussion.

**Yao et al. (2021)**
Huihan Yao, Ying Chen, Qinyuan Ye, Xisen Jin, and Xiang Ren. 2021. Refining language models with compositional explanations. *NeurIPS*.
- Cited in 12_appendix-c.md in the intermediate language steps discussion for improving performance.

**Yordanov et al. (2021)**
Yordan Yordanov, Vid Kocijan, Thomas Lukasiewicz, and Oana-Maria Camburu. 2021. Few-shot out-of-domain transfer learning of natural language explanations. *arXiv preprint arXiv:2112.06204*.
- Cited in 12_appendix-c.md in the natural language explanations discussion.

**Zaidan et al. (2007)**
Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using "annotator rationales" to improve machine learning for text categorization. *NAACL*.
- Cited in 12_appendix-c.md in the intermediate language steps discussion for improving performance.

**Zaremba and Sutskever (2014)**
Wojciech Zaremba and Ilya Sutskever. 2014. Learning to execute. *arXiv preprint arXiv:1410.4615*.
- Cited in 12_appendix-c.md in the program synthesis and execution discussion.

**Zelikman et al. (2022)**
Eric Zelikman, Yuhuai Wu, and Noah D. Goodman. 2022. STaR: Bootstrapping reasoning with reasoning. *arXiv preprint arXiv:2203.14465*.
- Cited in 12_appendix-c.md in the intermediate language steps discussion for generating synthetic datasets.

**Zhou et al. (2020)**
Wangchunshu Zhou, Jinyi Hu, Hanlin Zhang, Xiaodan Liang, Maosong Sun, Chenyan Xiong, and Jian Tang. 2020. Towards interpretable natural language understanding with explanations as latent variables. *NeurIPS*.
- Cited in 12_appendix-c.md in the natural language explanations discussion.
