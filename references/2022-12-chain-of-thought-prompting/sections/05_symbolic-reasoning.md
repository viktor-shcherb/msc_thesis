# 5 Symbolic Reasoning [p. 8]

[p. 8] The final experimental evaluation considers symbolic reasoning, which is simple for humans but potentially challenging for language models. The authors show that chain-of-thought prompting not only enables language models to perform symbolic reasoning tasks that are challenging in the standard prompting setting, but also facilitates length generalization to inference-time inputs longer than those seen in the few-shot exemplars.

## Tasks [p. 8]

Two toy tasks are used:

- **Last letter concatenation.** This task asks the model to concatenate the last letters of words in a name (e.g., "Amy Brown" -> "yn"). It is a more challenging version of first letter concatenation, which language models can already perform without chain of thought. Full names are generated by randomly concatenating names from the top one-thousand first and last names from name census data (https://namecensus.com/). Footnote 3 notes: 10 common names were tested using GPT-3 `davinci` and it got all but one correct.

- **Coin flip.** This task asks the model to answer whether a coin is still heads up after people either flip or don't flip the coin (e.g., "A coin is heads up. Phoebe flips the coin. Osvaldo does not flip the coin. Is the coin still heads up?" -> "no").

## Experimental Setup [p. 8]

As the construction of these symbolic reasoning tasks is well-defined, for each task an **in-domain** test set is considered (for which examples had the same number of steps as the training/few-shot exemplars), as well as an **out-of-domain (OOD)** test set (for which evaluation examples had more steps than those in the exemplars). For last letter concatenation, the model only sees exemplars of names with two words, and then performs last letter concatenation on names with 3 and 4 words. Footnote 4 notes: for names of length longer than 2 words, multiple first and last names are concatenated together. The same is done for the number of potential flips in the coin flip task. The experimental setup uses the same methods and models as in the prior two sections. Chains of thought for the few-shot exemplars are again manually composed for each task, which are given in Figure 3.

## Results [p. 8]

The results of these in-domain and OOD evaluations are shown in Figure 8 for PaLM, with results for LaMDA shown in Appendix Table 5.

**In-domain results:** With PaLM 540B, chain-of-thought prompting leads to almost 100% solve rates (note that standard prompting already solves coin flip with PaLM 540, though not for LaMDA 137B). These in-domain evaluations are "toy tasks" in the sense that perfect solution structures are already provided by the chains of thought in the few-shot exemplars; all the model has to do is repeat the same steps with the new symbols in the test-time example. And yet, small models still fail -- the ability to perform abstract manipulations on unseen symbols for these three tasks only arises at the scale of 100B model parameters.

**OOD results:** Standard prompting fails for both tasks. With chain-of-thought prompting, language models achieve upward scaling curves (though performance is lower than in the in-domain setting). Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of thought for language models of sufficient scale.

## Figure 8 [p. 8]

**Figure 8** (p. 8): "Using chain-of-thought prompting facilitates generalization to longer sequences in two symbolic reasoning tasks."

The figure contains four subplots arranged in a 2x2 grid:
- **Top left:** Letter Concat: 2 (in domain) -- Solve rate (%) vs. model scale (# parameters in billions: 8, 62, 540). Standard prompting stays near 0%, chain-of-thought prompting rises sharply to ~100% at 540B.
- **Top right:** Letter Concat: 4 (OOD) -- Standard prompting stays near 0%. Chain-of-thought prompting rises to ~60% at 540B.
- **Bottom left:** Coin Flip: 2 (in domain) -- Standard prompting rises to ~100% at 540B. Chain-of-thought prompting also reaches ~100% at 540B.
- **Bottom right:** Coin Flip: 4 (OOD) -- Standard prompting stays near ~50%. Chain-of-thought prompting rises to ~100% at 540B.

Two lines per plot: standard prompting (solid, filled circles) and chain-of-thought prompting (dashed, open circles).
