# References [p. 11-15]

This file contains only the references cited in the section notes.

## A

**Agarwal et al., 2024**
Agarwal, R., Singh, A., Zhang, L. M., Bohnet, B., Chan, S., Anand, A., Abbas, Z., Nova, A., Co-Reyes, J. D., Chu, E., et al. Many-shot jailbreaking. arXiv preprint arXiv:2404.11018, 2024.
- Cited in 01_introduction.md on in-context learning capabilities with longer contexts

**AI@Meta, 2024**
AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md
- Cited in 03c_fine-tuning-models.md for Llama 3 model

**An et al., 2023**
An, C., Gong, S., Zhong, M., Li, M., Zhang, J., Kong, L., and Qiu, X. L-eval: Instituting standardized evaluation for long context language models. arXiv preprint arXiv:2307.11088, 2023.
- Cited in 01_introduction.md as an example of existing benchmarks that scale only up to 40,000 tokens

**Abdin et al., 2024**
Abdin, M., Jacobs, S. A., Awan, A. A., Aneja, J., Awadalla, A., Awadalla, H., Bach, N., Bahree, A., Bakhtiari, A., Behl, H., et al. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024.
- Cited in 03_benchmarking-results.md for Phi-3 and Phi-3.5 models

**Anthropic, 2024**
Anthropic. Introducing the next generation of Claude, 2024. URL https://www.anthropic.com/news/claude-3-family
- Cited in 01_introduction.md and 03_benchmarking-results.md for context length capabilities

## B

**Beltagy et al., 2020**
Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.
- Cited in 10_appendix-b-related-work-long-context.md on sparse attention mechanisms

**Bai et al., 2023**
Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., et al. Longbench: A bilingual multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023.
- Cited in 01_introduction.md as an example of existing benchmarks that scale only up to 40,000 tokens

**Bai et al., 2024**
Bai, Y., Lv, X., Zhang, J., He, Y., Qi, J., Hou, L., Tang, J., Dong, Y., and Li, J. Longalign: A recipe for long context alignment of large language models. arXiv preprint arXiv:2401.10858, 2024.
- Cited in 04_related-work.md for LongAlign and LongBench-chat long-context benchmarks

**Borgeaud et al., 2022**
Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Van Den Driessche, G. B., Lespiau, J.-B., Damoc, B., Clark, A., et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pp. 2206–2240. PMLR, 2022.
- Cited in 03b_retrieval-augmented-generation.md and 10_appendix-b-related-work-long-context.md on RAG implementations

**Bulatov et al., 2022**
Bulatov, A., Kuratov, Y., and Burtsev, M. Recurrent memory transformer. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 11079–11091. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/47e288629a6996a17ce50b90a0S6a0e1-Paper-Conference.pdf
- Cited in 01_introduction.md, 02_babilong-benchmark.md, 03_benchmarking-results.md, 03c_fine-tuning-models.md, 10_appendix-b-related-work-long-context.md for RMT architecture and results

**Bulatov et al., 2024**
Bulatov, A., Kuratov, Y., Kapushev, Y., and Burtsev, M. Beyond attention: Breaking the limits of transformer context length with recurrent memory. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):17700–17708, Jun. 2024. doi: 10.1609/aaai.v38i16.29722. URL https://ojs.aaai.org/index.php/AAAI/article/view/29722
- Cited in 01_introduction.md, 03c_fine-tuning-models.md, and 10_appendix-b-related-work-long-context.md for ARMT model and context extension capabilities

## C

**Chase, 2022**
Chase, H. LangChain, October 2022. URL https://github.com/langchain-ai/langchain
- Cited in 15_appendix-g-rag-pipeline-details.md for RAG pipeline implementation

**Chen et al., 2023**
Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J. Longlora: Efficient fine-tuning of long-context large language models. In The Twelfth International Conference on Learning Representations, 2023.
- Cited in 03_benchmarking-results.md for LongAlpaca long-context fine-tuning

**Chevalier et al., 2023**
Chevalier, A., Wettig, A., Ajith, A., and Chen, D. Adapting language models to compress contexts. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 5829–5846, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.232. URL https://aclanthology.org/2023.emnlp-main.232
- Cited in 01_introduction.md and 10_appendix-b-related-work-long-context.md on in-context learning with longer contexts

**Clark et al., 2020**
Clark, P., Tafjord, O., and Richardson, K. Transformers as soft reasoners over language. In Bessiere, C. (ed.), Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pp. 3882–3890. ijcai.org, 2020. doi: 10.24963/ijcai.2020/537. URL https://doi.org/10.24963/ijcai.2020/537
- Cited in 04_related-work.md on reasoning over language

**Cohere, 2024**
Cohere. Command r: Retrieval-augmented generation at production scale, March 2024. URL https://cohere.com/blog/command-r
- Cited in 01_introduction.md and 03_benchmarking-results.md for Command-R model

## D

**Didolkar et al., 2022**
Didolkar, A., Gupta, K., Goyal, A., Gundavarapu, N. B., Lamb, A. M., Ke, N. R., and Bengio, Y. Temporal latent bottleneck: Synthesis of fast and slow processing mechanisms in sequence learning. Advances in Neural Information Processing Systems, 35:10505–10520, 2022.
- Cited in 10_appendix-b-related-work-long-context.md on recurrent architectures

**Ding et al., 2023**
Ding, J., Ma, S., Dong, L., Zhang, X., Huang, S., Wang, W., and Wei, F. Longnet: Scaling transformers to 1,000,000,000 tokens. arXiv preprint arXiv:2307.02486, 2023.
- Cited in 10_appendix-b-related-work-long-context.md on sparse attention mechanisms

**Dong et al., 2023**
Dong, Z., Tang, T., Li, J., Zhao, W. X., and Wen, J.-R. Bamboo: A comprehensive benchmark for evaluating long text modeling capacities of large language models. arXiv preprint arXiv:2309.13345, 2023.
- Cited in 04_related-work.md on long context benchmarks

**Douze et al., 2024**
Douze, M., Guzhva, A., Deng, C., Johnson, J., Szilvasy, G., Mazare, P.-E., Lomeli, M., Hosseini, L., and Jegou, H. The faiss library. 2024.
- Cited in 15_appendix-g-rag-pipeline-details.md for FAISS vector database in RAG pipeline

**Du et al., 2022**
Du, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z., and Tang, J. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 320–335, 2022.
- Cited in 03_benchmarking-results.md for GLM model

**Dubey et al., 2024**
Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.
- Cited in 03_benchmarking-results.md and 03c_fine-tuning-models.md for Llama 3 model family

## F

**Fan et al., 2020**
Fan, A., Lavril, T., Grave, E., Joulin, A., and Sukhbaatar, S. Addressing some limitations of transformers with feedback memory. arXiv preprint arXiv:2002.09402, 2020.
- Cited in 10_appendix-b-related-work-long-context.md on recurrent transformer architectures

## G

**Gebru et al., 2021**
Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallach, H., Daumé III, H., and Crawford, K. Datasheets for datasets. Communications of the ACM, 64(12):86–92, 2021.
- Cited in 22_appendix-n-babilong-datasheet.md as the recommended form for dataset documentation

**Graves et al., 2014**
Graves, A., Wayne, G., and Danihelka, I. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.
- Cited in 10_appendix-b-related-work-long-context.md on recurrent mechanisms for long context

**Gu & Dao, 2023**
Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023.
- Cited in 01_introduction.md, 03_benchmarking-results.md, and 10_appendix-b-related-work-long-context.md for Mamba architecture

**Gu et al., 2021**
Gu, A., Goel, K., and Re, C. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021.
- Cited in 10_appendix-b-related-work-long-context.md on S4 architecture

**Guu et al., 2020**
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. Retrieval augmented language model pre-training. In International conference on machine learning, pp. 3929–3938. PMLR, 2020.
- Cited in 03b_retrieval-augmented-generation.md and 10_appendix-b-related-work-long-context.md on RAG implementations

## H

**Hochreiter & Schmidhuber, 1997**
Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Comput., 9(8):1735–1780, November 1997. doi: 10.1162/neco.1997.9.8.1735.
- Cited in 10_appendix-b-related-work-long-context.md on LSTM-style gates in Block-Recurrent Transformer

**Hutchins et al., 2022**
Hutchins, D., Schlag, I., Wu, Y., Dyer, E., and Neyshabur, B. Block-recurrent transformers. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=uloenYmLCAo
- Cited in 10_appendix-b-related-work-long-context.md on Block-Recurrent Transformer architecture

**Han et al., 2022**
Han, S., Schoelkopf, H., Zhao, Y., Qi, Z., Riddell, M., Benson, L., Sun, L., Zubova, E., Qiao, Y., Burtell, M., Peng, D., Fan, J., Liu, Y., Wong, B., Sailor, M., Ni, A., Nan, L., Kasai, J., Yu, T., Zhang, R., Joty, S. R., Fabbri, A. R., Kryscinski, W., Lin, X. V., Xiong, C., and Radev, D. FOLIO: natural language reasoning with first-order logic. arXiv preprint arXiv:2209.00840, 2022. doi: 10.48550/ARXIV.2209.00840. URL https://doi.org/10.48550/arXiv.2209.00840
- Cited in 02_babilong-benchmark.md on logical reasoning benchmarks

**Hendrycks et al., 2020**
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2020.
- Cited in 03_benchmarking-results.md for MMLU benchmark

**Hsieh et al., 2024**
Hsieh, C.-P., Sun, S., Kriman, S., Achurya, S., Rekesh, D., Jia, F., and Ginsburg, B. Ruler: What's the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024.
- Cited in 01_introduction.md and 04_related-work.md on needle-in-haystack tasks and context size evaluation

## J

**Jiang et al., 2023**
Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral of experts. arXiv preprint arXiv:2310.06825, 2023.
- Cited in 03_benchmarking-results.md for Mistral model

**Jiang et al., 2024**
Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.
- Cited in 03_benchmarking-results.md for Mixtral model

## K

**Khandelwal et al., 2019**
Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2019.
- Cited in 03b_retrieval-augmented-generation.md and 10_appendix-b-related-work-long-context.md on retrieval mechanisms

## L

**Lee et al., 2024**
Lee, J., Xie, A., Pacchiano, A., Chandak, Y., Finn, C., Nachum, O., and Brunskill, E. Supervised pretraining can learn in-context reinforcement learning. arXiv preprint, 36, 2024.
- Cited in 01_introduction.md on in-context learning

**Lei et al., 2020**
Lei, J., Wang, L., Shen, Y., Yu, D., Berg, T. L., and Bansal, M. Mart: Memory-augmented recurrent transformer for coherent video paragraph captioning, 2020.
- Cited in 10_appendix-b-related-work-long-context.md on recurrent transformer architectures

**Lei et al., 2023**
Lei, F., Liu, Q., Huang, Y., He, S., Zhao, J., and Liu, K. S3eval: A synthetic, scalable, systematic evaluation suite for large language models. arXiv preprint arXiv:2310.15147, 2023.
- Cited in 04_related-work.md on synthetic evaluation benchmarks

**Li et al., 2023a**
Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez, J., Stoica, I., Ma, X., and Zhang, H. How long can context length of open-source llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023a.
- Cited in 04_related-work.md on effective context length

**Li et al., 2023b**
Li, J., Wang, M., Zheng, Z., and Zhang, M. Loogle: Can long-context language models understand long contexts? arXiv preprint arXiv:2311.04939, 2023b.
- Cited in 04_related-work.md on long context understanding

**Li et al., 2024**
Li, T., Zhang, G., Do, Q. D., Yue, X., and Chen, W. Long-context llms struggle with long in-context learning. arXiv preprint arXiv:2404.02060, 2024.
- Cited in 04_related-work.md on in-context learning limitations

**Lieber et al., 2024**
Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., Dalmedigos, I., Safahi, E., Meirom, S., Belinkov, Y., Shalev-Shwartz, S., et al. Jamba: A hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887, 2024.
- Cited in 03_benchmarking-results.md for Jamba model

**Liu et al., 2024a**
Liu, H., Yan, W., Zaharia, M., and Abbeel, P. World model on million-length video and language with blockwise ringattention. arXiv preprint arXiv:2402.08268, 2024a.
- Cited in 01_introduction.md on models achieving perfect performance on needle-in-haystack

**Liu et al., 2024b**
Liu, Z., Ping, W., Roy, R., Xu, P., Shoeybi, M., and Catanzaro, B. Chatqa: Building gpt-4 level conversational qa models. arXiv preprint arXiv:2401.10225, 2024b.
- Cited in 15_appendix-g-rag-pipeline-details.md for Llama3-ChatQA-1.5-8B language model in RAG pipeline

**Loshchilov & Hutter, 2019**
Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7
- Cited in 11_appendix-c-rmt-armt-mamba-finetuning.md for AdamW optimizer

## M

**Mistral AI, 2024**
Mistral AI. Mistral NeMo, 2024.
- Cited in 03_benchmarking-results.md for Mistral-NeMo model

## N

**Ni et al., 2024**
Ni, X., Cai, H., Wei, X., Wang, S., Yin, D., and Li, P. XL^2 Bench: A benchmark for extremely long context understanding with long-range dependencies. arXiv preprint arXiv:2404.05446, 2024.
- Cited in 04_related-work.md on long context benchmarks

## O

**OpenAI, 2023a**
OpenAI. Gpt-4 technical report, 2023a.
- Cited in 01_introduction.md and 03_benchmarking-results.md for GPT-4 capabilities

**OpenAI, 2023b**
OpenAI. New models and developer products announced at devday, 2023b. URL https://openai.com/index/new-models-and-developer-products-announced-at-devday/
- Cited in 01_introduction.md on context length evolution

## P

**Peng et al., 2023a**
Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K., et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023a.
- Cited in 10_appendix-b-related-work-long-context.md on RWKV architecture

**Peng et al., 2023b**
Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn: Efficient context window extension of large language models. In The Twelfth International Conference on Learning Representations, 2023b.
- Cited in 03a_evaluation-effective-context-size.md for context extension methods

**Pouflis et al., 2024**
Pouflis, A., Tsalapati, E., and Koubarakis, M. Transformer-based language models for reasoning in the description logic alcq. arXiv preprint arXiv:2410.09613, 2024.
- Cited in 04_related-work.md on reasoning benchmarks

## Q

**Qiu et al., 2024**
Qiu, Z., Li, J., Huang, S., Zhong, W., and King, I. Clongeval: A chinese benchmark for evaluating long-context large language models. arXiv preprint arXiv:2403.03514, 2024.
- Cited in 04_related-work.md on long context benchmarks

## R

**Radford et al., 2019**
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019.
- Cited in 03c_fine-tuning-models.md and 11_appendix-c-rmt-armt-mamba-finetuning.md for GPT-2 model

**Rae et al., 2020**
Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., and Lillicrap, T. P. Compressive transformers for long-range sequence modelling. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SylKikSYDH
- Cited in 01_introduction.md, 02_babilong-benchmark.md, 10_appendix-b-related-work-long-context.md, 14_appendix-f-babilong-dataset-statistics.md, 22_appendix-n-babilong-datasheet.md for PG-19 corpus and Compressive Transformer

**Rajpurkar et al., 2016**
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383–2392, 2016.
- Cited in 02_babilong-benchmark.md for SQuAD benchmark

**Rubin & Berant, 2023**
Rubin, O. and Berant, J. Long-range language modeling with self-retrieval. arXiv preprint arXiv:2306.13421, 2023.
- Cited in 10_appendix-b-related-work-long-context.md on Retrieval-Pretrained Transformer

**Reddy et al., 2024**
Reddy, V., Koncel-Kedziorski, R., Lai, V. D., and Tanner, C. Docfinga: A long-context financial reasoning dataset. arXiv preprint arXiv:2401.06915, 2024.
- Cited in 04_related-work.md on long context datasets

**Reid et al., 2024**
Reid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lillicrap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat, O., Schrittwieser, J., et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.
- Cited in 01_introduction.md and 03_benchmarking-results.md for Gemini 1.5 capabilities

**Rodkin et al., 2024**
Rodkin, I., Kuratov, Y., Bulatov, A., and Burtsev, M. Associative recurrent memory transformer. In ICML Workshop Next Generation of Sequence Modeling Architectures, 2024. URL https://arxiv.org/abs/2407.04841
- Cited in 01_introduction.md, 02_babilong-benchmark.md, 03_benchmarking-results.md, 03c_fine-tuning-models.md, 11_appendix-c-rmt-armt-mamba-finetuning.md for ARMT architecture

## S

**Saparov & He, 2023**
Saparov, A. and He, H. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. In The Eleventh International Conference on Learning Representations, ICLR 2023, 2023. URL https://openreview.net/pdf?id=qFVVBzXxR2V
- Cited in 06_limitations.md on PrOntoQA reasoning dataset

**Sainz et al., 2023**
Sainz, O., Campos, J., García-Ferrero, I., Etxaniz, J., de Lacalle, O. L., and Agirre, E. NLP evaluation in trouble: On the need to measure LLM data contamination for each benchmark. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: EMNLP 2023, pp. 10776–10787, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.722. URL https://aclanthology.org/2023.findings-emnlp.722
- Cited in 03_benchmarking-results.md on data contamination concerns

**Schlag et al., 2021**
Schlag, I., Irie, K., and Schmidhuber, J. Linear transformers are secretly fast weight programmers. In International Conference on Machine Learning, pp. 9355–9366. PMLR, 2021.
- Cited in 11_appendix-c-rmt-armt-mamba-finetuning.md for DPFP-3 non-linearity in ARMT

**Shaham et al., 2022**
Shaham, U., Segal, E., Ivgi, M., Efrat, A., Yoran, O., Haviv, A., Gupta, A., Xiong, W., Geva, M., Berant, J., et al. Scrolls: Standardized comparison over long language sequences. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 12007–12021, 2022.
- Cited in 04_related-work.md on long sequence benchmarks

**Shaham et al., 2023**
Shaham, U., Ivgi, M., Efrat, A., Berant, J., and Levy, O. Zeroscrolls: A zero-shot benchmark for long text understanding. arXiv preprint arXiv:2305.14196, 2023.
- Cited in 04_related-work.md on ZeroSCROLLS benchmark

**Shi et al., 2023**
Shi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., and Yih, W.-t. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023.
- Cited in 03b_retrieval-augmented-generation.md and 10_appendix-b-related-work-long-context.md on RAG implementations

**Sorokin et al., 2022**
Sorokin, A., Buzun, N., Pugachev, L., and Burtsev, M. Explain my surprise: Learning efficient long-term memory by predicting uncertain outcomes. Advances in Neural Information Processing Systems, 35:36875–36888, 2022.
- Cited in 10_appendix-b-related-work-long-context.md on recurrent mechanisms

**Song et al., 2024a**
Song, D., Chen, S., Chen, G. H., Yu, F., Wan, X., and Wang, B. Milebench: Benchmarking mllms in long context. arXiv preprint arXiv:2404.18532, 2024a.
- Cited in 04_related-work.md on long context benchmarks

**Song et al., 2024b**
Song, M., Zheng, M., and Luo, X. Counting-stars: A multi-evidence, position-aware, and scalable benchmark for evaluating long-context large language models. arXiv preprint arXiv:2403.11802, 2024b.
- Cited in 01_introduction.md and 04_related-work.md on needle-in-haystack variations

**Sun et al., 2022**
Sun, S., Thai, K., and Iyyer, M. Chapterbreak: A challenge dataset for long-range language models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 3704–3714, 2022.
- Cited in 04_related-work.md on long-range language model benchmarks

## T

**Tafjord et al., 2021**
Tafjord, O., Dalvi, B., and Clark, P. ProofWriter: Generating implications, proofs, and abductive statements over natural language. In Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pp. 3621–3634. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.findings-acl.317. URL https://doi.org/10.18653/v1/2021.findings-acl.317
- Cited in 02_babilong-benchmark.md on reasoning benchmarks

**Tay et al., 2021**
Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=qVyeW-grC2k
- Cited in 04_related-work.md on efficiency benchmarks

**Team, 2024**
Team, Q. Qwen2.5: A party of foundation models, September 2024. URL https://qwenlm.github.io/blog/qwen2.5/
- Cited in 03_benchmarking-results.md for Qwen2.5 model

**Tian et al., 2021**
Tian, J., Li, Y., Chen, W., Xiao, L., He, H., and Jin, Y. Diagnosing the first-order logical reasoning ability through LogicNLI. In Moens, M., Huang, X., Specia, L., and Yih, S. W. (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 3738–3747. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.emnlp-main.303. URL https://doi.org/10.18653/v1/2021.emnlp-main.303
- Cited in 02_babilong-benchmark.md on logical reasoning benchmarks

## V

**Voelker et al., 2019**
Voelker, A., Kajic, I., and Eliasmith, C. Legendre memory units: Continuous-time representation in recurrent neural networks. Advances in neural information processing systems, 32, 2019.
- Cited in 10_appendix-b-related-work-long-context.md on recurrent mechanisms

## W

**Wu et al., 2022a**
Wu, Q., Lan, Z., Qian, K., Gu, J., Geramifard, A., and Yu, Z. Memformer: A memory-augmented transformer for sequence modeling. In Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022, pp. 308–318. Association for Computational Linguistics, 2022a. URL https://aclanthology.org/2022.findings-aacl.29
- Cited in 10_appendix-b-related-work-long-context.md on Memorizing Transformer

**Wu et al., 2022b**
Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Memorizing transformers. In International Conference on Learning Representations, 2022b. URL https://openreview.net/forum?id=TrjbxzRcnf-
- Cited in 10_appendix-b-related-work-long-context.md on Memorizing Transformer cached key-value pairs

**Wang et al., 2024a**
Wang, C., Duan, H., Zhang, S., Lin, D., and Chen, K. Ada-leval: Evaluating long-context llms with length-adaptable benchmarks. arXiv preprint arXiv:2404.06480, 2024a.
- Cited in 04_related-work.md on long context benchmarks

**Wang et al., 2024b**
Wang, C., Ning, R., Pan, B., Wu, T., Guo, Q., Deng, C., Bao, G., Wang, Q., and Zhang, Y. Novelqa: A benchmark for long-range novel question answering. arXiv preprint arXiv:2403.12766, 2024b.
- Cited in 04_related-work.md on long context QA benchmarks

**Wang et al., 2024c**
Wang, S., Bai, Y., Zhang, L., Zhou, P., Zhao, S., Zhang, G., Wang, S., Chen, R., Xu, H., and Sun, H. Xf3m: A training-free framework for llm length extension based on segment-wise inference. arXiv preprint arXiv:2405.17755, 2024c.
- Cited in 01_introduction.md on models achieving perfect performance on needle-in-haystack

**Weston et al., 2016**
Weston, J., Bordes, A., Chopra, S., and Mikolov, T. Towards ai-complete question answering: A set of prerequisite toy tasks. In Bengio, Y. and LeCun, Y. (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1502.05698
- Cited in 01_introduction.md, 02_babilong-benchmark.md, 04_related-work.md, 06_limitations.md, 09_appendix-a-code-data-availability.md, 22_appendix-n-babilong-datasheet.md for original bAbI tasks

## X

**Xiong et al., 2023**
Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023.
- Cited in 03a_evaluation-effective-context-size.md on context scaling

## Y

**Yang et al., 2018**
Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., and Manning, C. D. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2369–2380, 2018.
- Cited in 02_babilong-benchmark.md for multi-hop QA datasets

**Young et al., 2024**
Young, A., Chen, B., Li, C., Huang, C., Zhang, G., Zhang, G., Li, H., Zhu, J., Chen, J., Chang, J., et al. Yi: Open foundation models by 01. arXiv preprint arXiv:2403.04652, 2024.
- Cited in 03_benchmarking-results.md for Yi model

**Yuan et al., 2024**
Yuan, T., Ning, X., Zhou, D., Yang, Z., Li, S., Zhuang, M., Tan, Z., Yao, Z., Lin, D., Li, B., et al. L∞-eval: A balanced long-context benchmark with length levels up to 256k. arXiv preprint arXiv:2402.05136, 2024.
- Cited in 04_related-work.md on long context benchmarks

## Z

**Zaheer et al., 2020**
Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., and Ahmed, A. Big bird: Transformers for longer sequences. In Advances in Neural Information Processing Systems, volume 33, pp. 17283–17297, 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf
- Cited in 10_appendix-b-related-work-long-context.md on sparse attention mechanisms

**Zhang et al., 2023**
Zhang, L., Li, Y., Liu, Z., Liu, J., Yang, M., et al. Marathon: A race through the realm of long context with large language models. arXiv preprint arXiv:2312.09542, 2023.
- Cited in 04_related-work.md on long context benchmarks

**Zhang et al., 2024a**
Zhang, P., Liu, Z., Xiao, S., Shao, N., Ye, Q., and Dou, Z. Soaring from 4k to 400k: Extending llm's context with activation beacon. arXiv preprint arXiv:2401.03462, 2024a.
- Cited in 03a_evaluation-effective-context-size.md and 10_appendix-b-related-work-long-context.md for Activation Beacon context extension method

**Zhang et al., 2024b**
Zhang, X., Chen, Y., Hu, S., Xu, Z., Chen, J., Hao, M. K., Han, X., Thai, Z. L., Wang, S., Liu, Z., et al. ∞ bench: Extending long context evaluation beyond 100k tokens. arXiv preprint arXiv:2402.13718, 2024b.
- Cited in 01_introduction.md and 04_related-work.md on needle-in-haystack tasks and long context benchmarks
