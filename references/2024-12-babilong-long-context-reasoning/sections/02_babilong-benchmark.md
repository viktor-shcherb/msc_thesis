# The BABILong Benchmark for Long Context Processing [p. 3]

## Fundamental concept

The fundamental concept behind the Benchmark for Artificial Intelligence for Long-context evaluation is extending the length of existing tasks to test the ability of generative models in handling long contexts. Solving tasks with a long context size requires the model to distinguish important information from large amounts of irrelevant details. To simulate this behavior we "hide" the sentences of the original task between the sentences of irrelevant text that is drawn from another closely related distribution (see Figure 1a). Examples are constructed by gradually adding new sentences from the background dataset in their natural order until the augmented sample reaches the desired length. This way, we are not bound by the length of the original task itself, making it possible to assess even the longest available models with context windows of token. As background text we use books from the PG19 dataset (Rae et al., 2020) due to the substantial book lengths and naturally occurring long contexts. The model is required first to distinguish sentences related to the original task, then memorize and subsequently utilize them to generate the correct solution. [p. 3]

## Extension of bAbI benchmark

In this work we extend the bAbI benchmark (Weston et al., 2016), which consists of 20 tasks designed to evaluate basic aspects of reasoning. These tasks are generated by simulating interactions among characters and objects across various locations, each represented as a fact, such as "Mary traveled to the office." The challenge is to answer questions based on the facts generated in the current simulation, such as "Where is Mary?" The tasks in bAbI vary in the number of facts, question complexity, and the reasoning skills they assess, including spatial and temporal reasoning, deduction, and coreference resolution. In our paper, we label these tasks from 'QA1' to 'QA20'. The first ten tasks, as shown in Table 1²demonstrate that current LLMs exhibit varied performance even without distractor texts, indicating that the BABILong tasks span a broad spectrum of difficulty and allow for testing models across various performance dimensions. Details on performance metrics for the bAbI tasks, along with examples of BABILong samples generated using our pipeline, can be found in Appendix E. [p. 3]

²code: https://github.com/booydar/babilong; evaluation dataset: https://huggingface.co/datasets/RMT-team/babilong; leaderboard: https://huggingface.co/spaces/RMT-team/babilong

## Challenges for language models

As evident in the following sections, these seemingly simple tasks pose significant challenges to language models. Although filtering facts from background text might seem straightforward, models encounter next challenges of finding supporting facts among distractors and performing types of reasoning such as counting that are especially difficult for LLMs. Additionally, most NLP benchmarks [p. 3]

**Table 1** (p. 3): "The first ten tasks of BABILong with the number of supporting and distracting facts. The last column displays the performance of LLMs on each task in the absence of background text. Each dot represents one of the selected models, while the blue bars indicate the median accuracy across tested models."

| TASK | NAME | FACTS PER TASK | RELEVANT FACTS PER TASK | LLMS ANSWER ACCURACY WITHOUT BACKGROUND TEXT (0K) |
|------|------|----------------|-------------------------|--------------------------------------------------|
| QA1 | SINGLE SUPPORTING FACT | 2-10 | 1 | [chart shows ~100% accuracy for all models] |
| QA2 | TWO SUPPORTING FACTS | 2-68 | 2 | [chart shows ~90-100% accuracy] |
| QA3 | THREE SUPPORTING FACTS | 4-320 | 3 | [chart shows ~80-100% accuracy with more variation] |
| QA4 | TWO ARG RELATIONS | 2-6 | 1 | [chart shows ~80-100% accuracy] |
| QA5 | THREE ARG RELATIONS | 2-126 | 1 | [chart shows ~80-100% accuracy] |
| QA6 | YES-NO QUESTIONS | 2-26 | 1 | [chart shows ~70-100% accuracy] |
| QA7 | COUNTING | 2-52 | 1-10 | [chart shows wider spread ~60-100%] |
| QA8 | LISTS-SETS | 2-50 | 1-8 | [chart shows wide variation ~40-100%] |
| QA9 | SIMPLE NEGATION | 2-10 | 1 | [chart shows ~80-100% accuracy] |
| QA10 | INDEFINITE KNOWLEDGE | 2-10 | 1 | [chart shows ~80-100% accuracy] |

## Key findings from initial results

3. We find that popular LLMs effectively utilize only 10-20% of the context, with performance degrading sharply as reasoning complexity increases. Retrieval augmented generation fails to demonstrate good scores but fine-tuning on specific task helps. [p. 3]

4. We demonstrate successful in domain single fact question answering with the recurrent memory transformer on input texts up to 50 million tokens, which is a record for the sequence size processed by a single model. [p. 3]

The BABILong benchmark data and code for evaluation are available². [p. 3]
