# Appendix B: Related Work on Long Context Models [p. 16-17]

## Approaches to long context processing [p. 16]

In retrieval augmented generation (RAG), a language model is combined with a separate model, called a retriever. Given a specific request, the retriever finds a set of relevant parts from a dedicated data storage. Then parts selected by the retriever along with the input are incorporated by the language model to make predictions. Many different implementations of the retrieval mechanism have been proposed (Guu et al., 2020; Borgeaud et al., 2022; Shi et al., 2023). Some works focus on directly retrieving predictions (Khandelwal et al., 2019). [p. 16]

Other works retrieve individual input tokens or text segments and add them to the LM input (Guu et al., 2020; Borgeaud et al., 2022). For example, in REALM (Guu et al., 2020) whole text segments are retrieved and appended to the input to improve masked language modeling. In Memorizing Transformer (Wu et al., 2022a) pairs saved from previous training steps of the language model. In Retrieval-Pretrained Transformer (Rubin & Berant, 2023), an LM component processes input and output finds relevant chunks. Representations of retrieved chunks are fused with current chunk in the LM component, and both the LM and retrieval parts are trained end-to-end. Compressive Memory (Chevalier et al., 2023) combines RMT-like (Bulatov et al., 2022) approach with retrieval from external corpora. AutoCompressor is first used to produce memory tokens for summary vectors for top chunks. Next, off-the-shelf retriever is used and corresponding chunk's memory tokens are added to the context of the model. [p. 16-17]

In this work, we augment the Recurrent Memory Transformer (Bulatov et al., 2024) with the ability to retrieve its own past memory tokens. As far as we know, this is the first combination of a recurrent transformer with a trainable retrieval mechanism. [p. 17]

---
[p. 17 continued]

Recurrence is another mechanism to deal with long context (Graves et al., 2014; Moelker et al., 2019; Sorokin et al., 2022). Instead of processing the entire context, a recurrent model breaks it down into smaller segments. The recurrent hidden state acts as an aggregator of information from past segments of the sequence. Attending to past memory state is a much easier task for self-attention. Many different architectures adding recurrence to transformers have been proposed (Wu et al., 2022a; Lei et al., 2020; Fan et al., 2020). For example, Compressive Transformer (Rae et al., 2020) updates recurrent memory by compressing hidden activation's from the previous segment to a smaller set of representations. Recurrent Memory Transformer (Bulatov et al., 2022) recurrently passes states of special memory tokens added to the input of Transformer. [p. 17]

Activation Beacon (Zhang et al., 2024a) compresses activations from prior segments using separate parameters and integrates a sliding window for handling up to 400k tokens. Temporal Latent Bottleneck (Didolkar et al., 2022) Transformer splits computation into two streams: recurrent slow stream and fast stream with self-attention mechanism. Block-Recurrent Transformer (Hutchins et al., 2022) employs LSTM-style (Hochreiter & Schmidhuber, 1997) gates to update its recurrent state. [p. 17]

We use RMT in our experiments because of its simplicity, plug-and-play compatibility with pre-trained transformer-based language models, and promising scaling capabilities (Bulatov et al., 2024). [p. 17]

Big Bird (Zaheer et al., 2020), Longformer (Beltagy et al., 2020), LongNet (Ding et al., 2023) help extend context length for Transformers by replacing full self-attention to sparse self-attention mechanisms with linear complexity. Works like RWKV (Peng et al., 2023a), S4 (Gu et al., 2021), Mamba (Gu & Dao, 2023), take another route by focusing on achieving recurrent networks to reach high parallelism levels available to Transformers while retaining the linear complexity of RNN. These works show promising results on long sequences but are still lagging behind the best transformer models in natural language processing tasks. Mamba, however, seeks to bridge this gap. [p. 17]
