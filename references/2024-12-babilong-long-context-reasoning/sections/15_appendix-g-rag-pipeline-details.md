# Appendix G: Details of the RAG Pipeline [p. 21-22]

For the GPT4-RAG pipelines, we employed the FAISS (Douze et al., 2024) vector database, using Langchain (Chase, 2022), for our experimental RAG setup. We utilized the 'text-embedding-ada-002' model for generating text embeddings. Our methodology encompassed two distinct approaches for text chunking: firstly, segmentation by sentences utilizing the NLTK library, and secondly, division into segments of 512 tokens each. We employed a binary metric for retrieval accuracy, where the criterion was the presence or absence of relevant facts (singular or multiple, based on the specific task) within the retrieved segments. Retrieval accuracy was quantified for the top 5 chunks. Additionally, we assessed the performance of GPT-4-turbo in conjunction with the retrieved facts, specifically focusing on the 'QA1' task. Our experimental scope spanned various context lengths, including 8k, 64k, and 128k tokens for tasks 'QA1' through 'QA5' of the BABILong dataset, with added 4k, 16k, 32k, 500k, 1M and 10M token length for an in-depth analysis of the 'QA1' task. Additionally, we assessed the performance of RAG on the 'QA1' task, utilizing precomputed Wikipedia embeddings instead of pg-19 with an average embedding size of 250 tokens. This evaluation aimed to determine the influence of embedding size and noise characteristics on model performance. For each task, we maintained a consistent sample size of 50 across different context lengths. For the Llama3 + RAG we used the Llama3-ChatQA-1.5-8B as the language model (Liu et al., 2024b) and the 'nvidia/dragon-multiturn-query-encoder' for context embedding. Another difference is that we did not use any caching and Wikipedia embeddings unlike with GPT-4. [p. 21-22]
