# Contributions [p. 42]

[p. 42] The paper lists individual contributions of each author:

- **Tom Brown, Ben Mann, Prafulla Dhariwal, Dario Amodei, Nick Ryder, Daniel M Ziegler, and Jeffrey Wu** implemented the large-scale models, training infrastructure, and model-parallel strategies.
- **Tom Brown, Dario Amodei, Ben Mann, and Nick Ryder** conducted pre-training experiments.
- **Ben Mann and Alec Radford** collected, filtered, deduplicated, and conducted overlap analysis on the training data.
- **Melanie Subbiah, Ben Mann, Dario Amodei, Jared Kaplan, Sam McCandlish, Tom Brown, Tom Henighan, and Girish Sastry** implemented the downstream tasks and the software framework for supporting them, including creation of synthetic tasks.
- **Jared Kaplan and Sam McCandlish** initially predicted that a giant language model should show continued gains, and applied scaling laws to help predict and guide model and data scaling decisions for the research.
- **Ben Mann** implemented sampling without replacement during training.
- **Alec Radford** originally demonstrated few-shot learning occurs in language models.
- **Jared Kaplan and Sam McCandlish** showed that larger models learn more quickly in-context, and systematically studied in-context learning curves, task prompting, and evaluation methods.
- **Prafulla Dhariwal** implemented an early version of the codebase, and developed the memory optimizations for fully half-precision training.
- **Rewon Child and Mark Chen** developed an early version of the model-parallel strategy.
- **Rewon Child and Scott Gray** contributed the sparse transformer.
- **Aditya Ramesh** experimented with loss scaling strategies for pretraining.
- **Melanie Subbiah and Arvind Neelakantan** implemented, experimented with, and tested beam search.
- **Pranav Shyam** worked on SuperGLUE and assisted with connections to few-shot learning and meta-learning literature.
- **Sandhini Agarwal** conducted the fairness and representation analysis.
- **Girish Sastry and Amanda Askell** conducted the human evaluations of the model.
- **Ariel Herbert-Voss** conducted the threat analysis of malicious use.
- **Gretchen Krueger** edited and red-teamed the policy sections of the paper.
- **Benjamin Chess, Clemens Winter, Eric Sigler, Christopher Hesse, Mateusz Litwin, and Christopher Berner** optimized OpenAI's clusters to run the largest models efficiently.
- **Scott Gray** developed fast GPU kernels used during training.
- **Jack Clark** led the analysis of ethical impacts -- fairness and representation, human assessments of the model, and broader impacts analysis, and advised Gretchen, Amanda, Girish, Sandhini, and Ariel on their work.
- **Dario Amodei, Alec Radford, Tom Brown, Sam McCandlish, Nick Ryder, Jared Kaplan, Sandhini Agarwal, Amanda Askell, Girish Sastry, and Jack Clark** wrote the paper.
- **Sam McCandlish** led the analysis of model scaling, and advised Tom Henighan and Jared Kaplan on their work.
- **Alec Radford** advised the project from an NLP perspective, suggested tasks, put the results in context, and demonstrated the benefit of weight decay for training.
- **Ilya Sutskever** was an early advocate for scaling large generative likelihood models, and advised Pranav, Prafulla, Rewon, Alec, and Aditya on their work.
- **Dario Amodei** designed and led the research.
