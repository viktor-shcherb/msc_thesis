# Language Models are Few-Shot Learners

**Authors:** Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei
**Affiliation:** OpenAI

## Publication Status

- **arXiv preprint:** May 2020, arXiv:2005.14165
- **Peer-reviewed:** Yes
- **Conference/Journal:** Advances in Neural Information Processing Systems 33 (NeurIPS 2020), pages 1877-1901
- **Status:** Published conference paper

GPT-3 is one of the most cited papers in the LLM literature. It introduced the in-context learning paradigm and demonstrated that large language models can perform tasks without gradient updates simply by conditioning on a few examples in the prompt. The paper established few-shot learning as a standard evaluation protocol for language models.

## Preferred Citation

Cite the NeurIPS 2020 version:

> Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., & Amodei, D. (2020). Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems 33 (NeurIPS 2020), pages 1877-1901.

## Links

- arXiv: https://arxiv.org/abs/2005.14165
- Proceedings: https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html
