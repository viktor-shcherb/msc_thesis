# References

This file contains only the bibliographic entries for works cited in the section notes.

---

**Amar et al., 2023**
Shmuel Amar, Liat Schiff, Ori Ernst, Asi Shefer, Ori Shapira, and Ido Dagan. 2023. OpenAsp: A benchmark for multi-document open aspect-based summarization. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 1967–1991, Singapore. Association for Computational Linguistics.
- Cited in: 09_appendix-a.md (Table 1 classification)

**An et al., 2023**
Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: Instituting standardized evaluation for long context language models. _Preprint_, arXiv:2307.11088.
- Cited in: 04_challenging-long-context-is-under-explored.md for obfuscated answers in QA
- Cited in: 09_appendix-a.md (Table 1 classification)

**Angelidis et al., 2021**
Stefanos Angelidis, Reinald Kim Amplayo, Yoshihiko Suhara, Xiaolan Wang, and Mirella Lapata. 2021. Extractive opinion summarization in quantized transformer spaces. _Transactions of the Association for Computational Linguistics_, 9:277–293.
- Cited in: 02_task-design-in-long-context.md for multi-document summarization
- Cited in: 09_appendix-a.md (Table 1 classification)

**Arora et al., 2023**
Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher Re. 2023. Zoology: Measuring and improving recall in efficient language models. _arXiv preprint arXiv:2312.04927_.
- Cited in: 02_task-design-in-long-context.md for multiple needles tasks
- Cited in: 04_challenging-long-context-is-under-explored.md for several needles tasks

**Aumiller and Gertz, 2022**
Dennis Aumiller and Michael Gertz. 2022. Klexikon: A German dataset for joint summarization and simplification. In _Proceedings of the Thirteenth Language Resources and Evaluation Conference_, pages 2693–2701, Marseille, France. European Language Resources Association.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Bai et al., 2023**
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023. Longbench: A bilingual, multitask benchmark for long context understanding. _Preprint_, arXiv:2308.14508.
- Cited in: 01_introduction.md for long-context benchmarks grouping tasks by length
- Cited in: 02_task-design-in-long-context.md for natural vs synthetic construction
- Cited in: 09_appendix-a.md (Table 1 classification)

**Bertsch et al., 2024**
Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew R. Gormley, and Graham Neubig. 2024. In-context learning with long-context models: An in-depth exploration. _Preprint_, arXiv:2405.00200.
- Cited in: 05_discussion-towards-genuinely-difficult-long-context-task-design.md for in-context examples

**Bishop et al., 2024**
Jennifer A Bishop, Qianqian Xie, and Sophia Ananiadou. 2024. Longdocfactscore: Evaluating the factuality of long document abstractive summarisation. _Preprint_, arXiv:2309.12455.
- Cited in: 02_task-design-in-long-context.md for long-context evaluation
- Cited in: 09_appendix-a.md (Table 1 classification)

**Boni et al., 2021**
Odellia Boni, Guy Feigenblat, Guy Lev, Michal Shmueli-Scheuer, Benjamin Sznajder, Dafna Konopnicki. 2021. Howsumm: A multi-document summarization dataset derived from wikihow articles. _Preprint_, arXiv:2110.03179.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Bruno and Roth, 2022**
William Bruno and Dan Roth. 2022. Lawngli: A long-premise benchmark for in-domain generalization from short to long contexts and for implication-based retrieval. _Preprint_, arXiv:2212.03222.
- Cited in: 02_task-design-in-long-context.md for specialized legal domain
- Cited in: 05_discussion-towards-genuinely-difficult-long-context-task-design.md for domain expertise tasks
- Cited in: 09_appendix-a.md (Table 1 classification)

**Caciularu et al., 2024**
Avi Caciularu, Alon Jacovi, Eyal Ben-David, Sasha Goldshtein, Tal Schuster, Jonathan Herzig, Gal Elidan, and Amir Globerson. 2024. Tact: Advancing complex aggregative reasoning with information extraction tools. _Preprint_, arXiv:2406.03618.
- Cited in: 02_task-design-in-long-context.md for structured data aggregation
- Cited in: 05_discussion-towards-genuinely-difficult-long-context-task-design.md for table manipulation

**Chen et al., 2022a**
Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2022a. SummScreen: A dataset for abstractive screenplay summarization. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 8602–8615, Dublin, Ireland. Association for Computational Linguistics.
- Cited in: 04_challenging-long-context-is-under-explored.md for book summarization scope

**Chen et al., 2022b**
Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2022b. Summscreen: A dataset for abstractive screenplay summarization. _Preprint_, arXiv:2104.07091.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Chen et al., 2023**
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023. Longlora: Efficient fine-tuning of long-context large language models. _ArXiv_, abs/2309.12307.
- Cited in: 02_task-design-in-long-context.md for training data

**Cohan et al., 2018**
Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents. _Preprint_, arXiv:1804.05685.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Dasigi et al., 2021**
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021. A dataset of information-seeking questions and answers anchored in research papers. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 4599–4610, Online. Association for Computational Linguistics.
- Cited in: 04_challenging-long-context-is-under-explored.md for factual single-hop QA
- Cited in: 09_appendix-a.md (Table 1 classification)

**Devlin et al., 2019**
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.
- Cited in: 01_introduction.md for first attention-based LLMs with limited context

**Dong et al., 2024**
Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. 2024. BAMBOO: A comprehensive benchmark for evaluating long text modeling capacities of large language models. In _Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)_, pages 2086–2099, Torino, Italia. ELRA and ICCL.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Dunietz et al., 2020**
Jesse Dunietz, Greg Burnham, Akash Bharadwaj, Owen Rambow, Jennifer Chu-Carroll, and Dave Ferrucci. 2020. To test machine comprehension, start by defining comprehension. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 7839–7859, Online. Association for Computational Linguistics.
- Cited in: 05_discussion-towards-genuinely-difficult-long-context-task-design.md for machine comprehension

**Dunn et al., 2017**
Matthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik, and Kyunghyun Cho. 2017. Searchqa: A new q&a dataset augmented with context from a search engine. _arXiv preprint arXiv:1704.05179_.
- Cited in: 02_task-design-in-long-context.md for QA tasks

**Fabbri et al., 2019**
Alexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir R. Radev. 2019. Multi-news: a large-scale multi-document dataset and abstractive hierarchical model. _Preprint_, arXiv:1906.01749.
- Cited in: 02_task-design-in-long-context.md for multi-document summarization
- Cited in: 09_appendix-a.md (Table 1 classification)

**Feng et al., 2021**
Song Feng, Siva Sankalp Patel, Hui Wan, and Sachindra Joshi. 2021. MultiDoc2Dial: Modeling dialogues grounded in multiple documents. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 6162–6176, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Gao et al., 2023**
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Y. Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. 2023. Rarr: Researching and revising what language models say, using language models. _Preprint_, arXiv:2210.08726.
- Cited in: 05_discussion-towards-genuinely-difficult-long-context-task-design.md for retrieval-based pipelines

**Gao et al., 2024**
Fan Gao, Hang Jiang, Rui Yang, Qingcheng Zeng, Jinghui Lu, Moritz Blum, Dairui Liu, Tianwei She, Yuang Jiang, and Irene Li. 2024. Large language models on wikipedia-style survey generation: an evaluation in nlp concepts. _Preprint_, arXiv:2308.10410.
- Cited in: 02_task-design-in-long-context.md for survey generation

**Gemini Team Google, 2024**
Gemini Team Google. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. _Preprint_, arXiv:2403.05530.
- Cited in: 01_introduction.md for 1M token context capability

**GLM Team, 2024**
GLM Team. 2024. GLM-4-9b-chat technical report.
- Cited in: 02_task-design-in-long-context.md for evaluating new long-context models

**Guo et al., 2023**
Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. 2023. Longcoder: A long-range pre-trained language model for code completion. _Preprint_, arXiv:2306.14893.
- Cited in: 09_appendix-a.md (Table 1 classification)

**He et al., 2023**
Junqing He, Kunhao Pan, Xiaoqun Dong, Zhuoyang Song, Yibo Liu, Yuxin Liang, Hao Wang, Qianguo Sun, Songxin Zhang, Zejian Xie, and Min Zhang. 2023. Never lost in the middle: Improving large language models via attention strengthening question answering. _Preprint_, arXiv:2311.09198.
- Cited in: 02_task-design-in-long-context.md for training data
- Cited in: 04_challenging-long-context-is-under-explored.md for obfuscated answers in QA

**Hendrycks et al., 2021**
Dan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. 2021. Cuad: An expert-annotated nlp dataset for legal contract review. _Preprint_, arXiv:2103.06268.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Ho et al., 2020**
Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. In _Proceedings of the 28th International Conference on Computational Linguistics_, pages 6609–6625, Barcelona, Spain (Online). International Committee on Computational Linguistics.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Hsieh et al., 2024**
Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. 2024. Ruler: What's the real context size of your long-context language models? _Preprint_, arXiv:2404.06654.
- Cited in: 01_introduction.md for NIAH tasks and multi-needle variants
- Cited in: 02_task-design-in-long-context.md for natural vs synthetic construction and multiple needles
- Cited in: 04_challenging-long-context-is-under-explored.md for several needles, common words extraction, and multi-hop questions
- Cited in: 09_appendix-a.md (Table 1 classification)

**Hu et al., 2023**
Yebowen Hu, Timothy Ganter, Hanieh Deilamsalehy, Franck Dernoncourt, Hassan Foroosh, and Fei Liu. 2023. MeetingBank: A benchmark dataset for meeting summarization. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 16409–16423, Toronto, Canada. Association for Computational Linguistics.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Huang et al., 2021a**
Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021a. Efficient attentions for long document summarization. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 1419–1436, Online. Association for Computational Linguistics.
- Cited in: 02_task-design-in-long-context.md for summarization tasks
- Cited in: 04_challenging-long-context-is-under-explored.md for book summarization scope and high dispersion summarization

**Huang et al., 2021b**
Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021b. Efficient attentions for long document summarization. _Preprint_, arXiv:2104.02112.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Ivgi et al., 2023**
Maor Ivgi, Uri Shaham, and Jonathan Berant. 2023. Efficient long-text understanding with short-text models. _Transactions of the Association for Computational Linguistics_, 11:284–299.
- Cited in: 01_introduction.md for natural NIAH tasks
- Cited in: 02_task-design-in-long-context.md for NIAH tasks

**Ivgi et al., 2024**
Maor Ivgi, Uri Shaham, and Jonathan Berant. 2023. Efficient long-text understanding with short-text models. _Transactions of the Association for Computational Linguistics_, 11:284–299. [Note: the paper's text cites "Ivgi et al., 2024" but the bibliography lists only the 2023 entry; appears to be the same work.]
- Cited in: 01_introduction.md for NIAH tasks

**Jiang et al., 2023**
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lelio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Theophile Gervet, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. 2024. Mixtral of experts. _Preprint_, arXiv:2401.04088.
- Cited in: 05_discussion-towards-genuinely-difficult-long-context-task-design.md for other modalities and structures

**Jiang et al., 2024**
Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Xin Zhao, and Ji-Rong Wen. 2024. StructGPT: A general framework for large language model to reason over structured data. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 9237–9251, Singapore. Association for Computational Linguistics.
- Cited in: 02_task-design-in-long-context.md for evaluating new long-context models

**Kamradt, 2023**
Gregory Kamradt. 2023. Needle in a haystack - pressure testing LLMs. GitHub.
- Cited in: 02_task-design-in-long-context.md for NIAH tasks
- Cited in: 04_challenging-long-context-is-under-explored.md for NIAH tasks

**Kočiský et al., 2017**
Tomas Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. 2017. The narrativeqa reading comprehension challenge. _Preprint_, arXiv:1712.07040.
- Cited in: 04_challenging-long-context-is-under-explored.md for factual single-hop QA

**Kočiský et al., 2018**
Tomas Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis, and Edward Grefenstette. 2018. The NarrativeQA Reading Comprehension Challenge. _Transactions of the Association for Computational Linguistics_, 6:317–328.
- Cited in: 02_task-design-in-long-context.md for QA tasks
- Cited in: 09_appendix-a.md (Table 1 classification)

**Koreeda and Manning, 2021a**
Yuta Koreeda and Christopher Manning. 2021a. Contractnli: A dataset for document-level natural language inference for contracts. In _Findings of the Association for Computational Linguistics: EMNLP 2021_, pages 1907–1919, Punta Cana, Dominican Republic. Association for Computational Linguistics.
- Cited in: 02_task-design-in-long-context.md for NLI tasks

**Koreeda and Manning, 2021b**
Yuta Koreeda and Christopher D. Manning. 2021b. Contractnli: A dataset for document-level natural language inference for contracts. _Preprint_, arXiv:2110.01799.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Kornilova and Eidelman, 2019**
Anastassia Kornilova and Vladimir Eidelman. 2019. BillSum: A corpus for automatic summarization of US legislation. In _Proceedings of the 2nd Workshop on New Frontiers in Summarization_, pages 48–56, Hong Kong, China. Association for Computational Linguistics.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Kryściński et al., 2022**
Wojciech Kryscinski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir Radev. 2022. Booksum: A collection of datasets for long-form narrative summarization. In _Findings of the Association for Computational Linguistics: EMNLP 2022_, pages 6536–6558, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
- Cited in: 02_task-design-in-long-context.md for specialized literature domain
- Cited in: 09_appendix-a.md (Table 1 classification)

**Kulkarni et al., 2020**
Sayali Kulkarni, Sheide Chammas, Wan Zhu, Fei Sha, and Eugene Ie. 2020. Aquamuse: Automatically generating datasets for query-based multi-document summarization. _Preprint_, arXiv:2010.12694.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Kuratov et al., 2024**
Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev. 2024. In search of needles in a 11m haystack: Recurrent memory finds what llms miss. _Preprint_, arXiv:2402.10790.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Kwiatkowski et al., 2019**
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. _Transactions of the Association for Computational Linguistics_, 7:452–466.
- Cited in: 04_challenging-long-context-is-under-explored.md for factual single-hop QA
- Cited in: 09_appendix-a.md (Table 1 classification)

**Levy et al., 2024**
Mosh Levy, Alon Jacoby, and Yoav Goldberg. 2024. Same task, more tokens: the impact of input length on the reasoning performance of large language models. _Preprint_, arXiv:2402.14848.
- Cited in: 01_introduction.md for needle positioning variants
- Cited in: 02_task-design-in-long-context.md for distractors between needles
- Cited in: 04_challenging-long-context-is-under-explored.md for finding multiple snippets
- Cited in: 09_appendix-a.md (Table 1 classification)

**Li et al., 2023a**
Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. 2023a. How long can open-source LLMs truly promise on context length?
- Cited in: 09_appendix-a.md (Table 1 classification)

**Li et al., 2023b**
Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023b. Loogle: Can long-context language models understand long contexts? _Preprint_, arXiv:2311.04939.
- Cited in: 04_challenging-long-context-is-under-explored.md for QA datasets with small scope
- Cited in: 09_appendix-a.md (Table 1 classification)

**Liu et al., 2023a**
Shuaiqi Liu, Jiannong Cao, Ruosong Yang, and Zhiyuan Wen. 2023a. Long text and multi-table summarization: Dataset and method. _Preprint_, arXiv:2302.03815.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Liu et al., 2023b**
Tianyang Liu, Canwen Xu, and Julian McAuley. 2023b. Repobench: Benchmarking repository-level code auto-completion systems. _Preprint_, arXiv:2306.03091.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Liu et al., 2024a**
Hao Liu, Wilson Yan, Matej Zaharia, and Pieter Abbeel. 2024a. World model on million-length video and language with ringattention. _arXiv preprint arXiv:2402.08268_.
- Cited in: 02_task-design-in-long-context.md for increasing number of critical details

**Liu et al., 2024b**
Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024b. Lost in the middle: How language models use long contexts. _Transactions of the Association for Computational Linguistics_, 12:157–173.
- Cited in: 02_task-design-in-long-context.md for changing position within input
- Cited in: 09_appendix-a.md (Table 1 classification)

**Malaviya et al., 2024**
Chaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth Sieber, Mark Yatskar, and Dan Roth. 2024. ExpertQA: Expert-curated questions and attributed answers. In _Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)_, pages 3025–3045, Mexico City, Mexico. Association for Computational Linguistics.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Mohtashami and Jaggi, 2023**
Amirkeivan Mohtashami and Martin Jaggi. 2023. Landmark attention: Random-access infinite context length for transformers. In _Workshop on Efficient Systems for Foundation Models, ICML2023_.
- Cited in: 01_introduction.md for NIAH tasks
- Cited in: 02_task-design-in-long-context.md for PassKey retrieval
- Cited in: 04_challenging-long-context-is-under-explored.md for NIAH tasks

**Narayan et al., 2018**
Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, pages 1797–1807, Brussels, Belgium. Association for Computational Linguistics.
- Cited in: 02_task-design-in-long-context.md for summarization tasks

**Nguyen et al., 2024**
Chau Nguyen, Phuong Nguyen, Thanh Tran, Dat Nguyen, An Trieu, Tin Pham, Anh Dang, and Le-Minh Nguyen. 2024. Captain at coliee 2023: Efficient methods for legal information retrieval and entailment tasks. _Preprint_, arXiv:2401.03551.
- Cited in: 02_task-design-in-long-context.md for specialized legal domain
- Cited in: 09_appendix-a.md (Table 1 classification)

**OpenAI, 2024**
OpenAI. 2024. GPT-4 technical report. _Preprint_, arXiv:2303.08774.
- Cited in: 01_introduction.md for 128k token context capability

**Pal et al., 2023**
Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, and Siddartha Naidu. 2023. Giraffe: Adventures in expanding context lengths in llms. _Preprint_, arXiv:2308.10882.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Pang et al., 2022**
Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. 2022. QuALITY: Question answering with long input texts, yes! In _Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 5336–5358, Seattle, United States. Association for Computational Linguistics.
- Cited in: 04_challenging-long-context-is-under-explored.md for increased dispersion
- Cited in: 09_appendix-a.md (Table 1 classification)

**Prasad et al., 2023**
Archiki Prasad, Trung Bui, Seunghyun Yoon, Hanieh Deilamsalehy, Franck Dernoncourt, and Mohit Bansal. 2023. MeetingQA: Extractive question-answering on meeting transcripts. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 15000–15025, Toronto, Canada. Association for Computational Linguistics.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Qian et al., 2023**
Hongjing Qian, Yutao Zhu, Zhicheng Dou, Haoqi Gu, Xinyu Zhang, Zheng Liu, Ruofei Lai, Zhao Cao, Jian-Yun Nie, and Ji-Rong Wen. 2023. Webbrain: Learning to generate factually correct articles for queries by grounding on large web corpus. _Preprint_, arXiv:2304.04358.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Rae et al., 2019**
Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, and Timothy P. Lillicrap. 2019. Compressive transformers for long-range sequence modelling. _Preprint_, arXiv:1911.05507.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Raffel et al., 2020**
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of machine learning research_, 21(140):1–67.
- Cited in: 01_introduction.md for first attention-based LLMs with limited context

**Reddy et al., 2024**
Varshini Reddy, Rik Koncel-Kedziorski, Viet Dac Lai, Michael Krumdick, Charles Lovering, and Chris Tanner. 2024. Docfinqa: A long-context financial reasoning dataset. _Preprint_, arXiv:2401.06915.
- Cited in: 04_challenging-long-context-is-under-explored.md for QA datasets with small scope
- Cited in: 05_discussion-towards-genuinely-difficult-long-context-task-design.md for financial reports
- Cited in: 09_appendix-a.md (Table 1 classification)

**Saunders et al., 2022**
William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. Self-critiquing models for assisting human evaluators. _Preprint_, arXiv:2206.05802.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Shaham et al., 2022**
Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. 2022. SCROLLS: Standardized CompaRison over long language sequences. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 12007–12021, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
- Cited in: 01_introduction.md for long-context benchmarks grouping tasks by length

**Shaham et al., 2023**
Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. 2023. ZeroSCROLLS: A zero-shot benchmark for long text understanding. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 9977–9989, Singapore. Association for Computational Linguistics.
- Cited in: 04_challenging-long-context-is-under-explored.md for aggregation and book summarization
- Cited in: 09_appendix-a.md (Table 1 classification)

**Sharma et al., 2019**
Eva Sharma, Chen Li, and Lu Wang. 2019. BIGPATENT: A large-scale dataset for abstractive and coherent summarization. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 2204–2213, Florence, Italy. Association for Computational Linguistics.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Stylianou et al., 2021**
Nikolaos Stylianou, Panagiotis Kosmoliaptsis, and Ioannis Vlahavas. 2021. Improved biomedical entity recognition via longer context modeling. In _IFIP WG 12.5 International Conference, AIAI 2021, Hersonissos, Crete, Greece, June 25-27, 2021, Proceedings 17_, pages 45–56. Springer.
- Cited in: 05_discussion-towards-genuinely-difficult-long-context-task-design.md for biomedical domain and retrieval-based use-cases

**Su et al., 2024**
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. _Neurocomputing_, 568:127063.
- Cited in: 02_task-design-in-long-context.md for extrapolation architectures

**Takeshita et al., 2024**
Sotaro Takeshita, Tommaso Green, Ines Reinig, Kai Eckert, and Simone Ponzetto. 2024. ACLSum: A new dataset for aspect-based summarization of scientific publications. In _Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)_, pages 6660–6675, Mexico City, Mexico. Association for Computational Linguistics.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Tay et al., 2020**
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020. Long range arena: A benchmark for efficient transformers. _Preprint_, arXiv:2011.04006.
- Cited in: 02_task-design-in-long-context.md for natural vs synthetic construction
- Cited in: 09_appendix-a.md (Table 1 classification)

**Trivedi et al., 2022**
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. Musique: Multihop questions via single-hop question composition. _Preprint_, arXiv:2108.00573.
- Cited in: 04_challenging-long-context-is-under-explored.md for multi-hop questions
- Cited in: 09_appendix-a.md (Table 1 classification)

**Tseng et al., 2016**
Bo-Hsiang Tseng, Sheng-Syun Shen, Hung-Yi Lee, and Lin-Shan Lee. 2016. Towards machine comprehension of spoken content: Initial toefl listening comprehension test by machine.
- Cited in: 04_challenging-long-context-is-under-explored.md for factual single-hop QA

**Vaswani et al., 2017**
Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In _Neural Information Processing Systems_.
- Cited in: 02_task-design-in-long-context.md for extrapolation architectures

**Wang et al., 2022**
Alex Wang, Richard Yuanzhe Pang, Angelica Chen, Jason Phang, and Samuel R. Bowman. 2022. SQuALITY: Building a long-document summarization dataset the hard way. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 1139–1156, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
- Cited in: 02_task-design-in-long-context.md for specialized literature domain
- Cited in: 04_challenging-long-context-is-under-explored.md for query-based summarization and high dispersion summarization
- Cited in: 09_appendix-a.md (Table 1 classification)

**Williams et al., 2018**
Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_, pages 1112–1122, New Orleans, Louisiana. Association for Computational Linguistics.
- Cited in: 02_task-design-in-long-context.md for NLI tasks

**Wu et al., 2024**
Yunshu Wu, Hayate Iso, Pouya Pezeshkpour, Nikita Bhutani, and Estevam Hruschka. 2024. Less is more for long document summary evaluation by llms. _Preprint_, arXiv:2309.07382.
- Cited in: 02_task-design-in-long-context.md for long-context evaluation

**Yang et al., 2018**
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. _Preprint_, arXiv:1809.09600.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Zhang et al., 2024a**
Jiebin Zhang, Eugene J. Yu, Qinyu Chen, Chenhao Xiong, Dawei Zhu, Han Qian, Mingbo Song, Xiaoguang Li, Qun Liu, and Sujian Li. 2024a. Retrieval-based full-length wikipedia generation for emergent events. _Preprint_, arXiv:2402.18264.
- Cited in: 09_appendix-a.md (Table 1 classification)

**Zhang et al., 2024b**
Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. 2024b. Extending long context evaluation beyond 100k tokens. _Preprint_, arXiv:2402.13718.
- Cited in: 01_introduction.md for long-context benchmarks grouping tasks by length

**Zhao et al., 2022**
Yilun Zhao, Yunxiang Li, Chenying Li, and Rui Zhang. 2022. MultiHiertt: Numerical reasoning over multi hierarchical tabular and textual data. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 6588–6600, Dublin, Ireland. Association for Computational Linguistics.
- Cited in: 04_challenging-long-context-is-under-explored.md for multi-hop questions
- Cited in: 05_discussion-towards-genuinely-difficult-long-context-task-design.md for specialized domains with higher dispersion

**Zhao et al., 2023**
Yilun Zhao, Yitao Long, Hongjun Liu, Linyong Nan, Lyuhao Chen, Ryo Kamoi, Yixin Liu, Xiangru Tang, Rui Zhang, and Arman Cohan. 2023. Docmath-eval: Evaluating numerical reasoning capabilities of llms in understanding long documents with tabular data. _ArXiv_, abs/2311.09805.
- Cited in: 04_challenging-long-context-is-under-explored.md for QA datasets with small scope
- Cited in: 09_appendix-a.md (Table 1 classification)

**Zhong et al., 2021**
Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. 2021. QMSum: A new benchmark for query-based multi-domain meeting summarization. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 5905–5921, Online. Association for Computational Linguistics.
- Cited in: 04_challenging-long-context-is-under-explored.md for query-based summarization
- Cited in: 09_appendix-a.md (Table 1 classification)

**Zhou et al., 2023**
Yijie Zhou, Kejian Shi, Wencai Zhang, Yixin Liu, Yilun Zhao, and Arman Cohan. 2023. Odsum: New benchmarks for open domain multi-document summarization. _Preprint_, arXiv:2309.08960.
- Cited in: 09_appendix-a.md (Table 1 classification)
