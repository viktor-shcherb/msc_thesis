# Task Design in Long Context [p. 2–3]

Evaluating the performance of NLP models over very long contexts is a fast-changing area of research (Bishop et al., 2024; Wu et al., 2024). Measurements are regularly updated to account for new capabilities which improve with extrapolation architectures (Vaswani et al., 2017; Su et al., 2024) and training data (He et al., 2023; Chen et al., 2023). Evaluators were tasked with designing measurements of long-context capabilities cheaply, efficiently, and quickly, while matching realistic use cases as much as possible. The most common way of differentiating long-context tasks, besides the context's length, is whether they are naturally-constructed or synthetically-constructed (Tay et al., 2020; Bai et al., 2023; Hsieh et al., 2024).

## Natural construction

A simple yet effective way of "moving the goalpost" for context length is by modeling long-context tasks based on short-context tasks with longer text. For example, with QA (Kočiský et al., 2018, cf. Dunn et al., 2017), summarization (Huang et al., 2021a, cf. Narayan et al., 2018), and NLI (Koreeda and Manning, 2021a, cf. Williams et al., 2018). Specialized domains like legal (Bruno and Roth, 2022; Nguyen et al., 2024) and literature (Wang et al., 2022; Kryscinski et al., 2022) often involve longer texts, turning typically short-context tasks such as QA and NLI into long-context scenarios. Another more native methodology is to create new tasks which inherently require a long context, such as multi-document summarization (Fabbri et al., 2019; Angelidis et al., 2021), survey generation (Gao et al., 2024), and structured data aggregation (Caciularu et al., 2024). Both methodologies share the constraint that, due to their natural construction (i.e., using natural text), once created, they are difficult to modify for longer contexts as models' long-context capabilities improve.

## Synthetic construction

A more flexible approach, sacrificing natural construction for length control, is to use distractors to synthetically increase the context length. This method allows for cheap and efficient (in terms of task construction cost) evaluation of models' full context length capabilities, with difficulty adjusted by controlling the distractors. Tasks like Needle-in-a-Haystack (NIAH; Ivgi et al., 2023; Kamradt, 2023) and PassKey retrieval (Mohtashami and Jaggi, 2023) were created to evaluate a model's ability to pinpoint specific information amid lengthy distractors. Flexible and effective against existing models, they became standard benchmarks for evaluating new long-context models (GLM Team, 2024; Jiang et al., 2024). Followup studies have complicated these tasks by increasing the number of critical details to locate (Arora et al., 2023; Liu et al., 2024a) and changing their position within the input (Liu et al., 2024b; Levy et al., 2024).

## Limitations of the status quo

NIAH-like tasks aim to assess information retrieval capabilities, yet many "artificially constructed" QA and reading-comprehension tasks with trivial questions about a long context accomplish the same goal. At the same time, "multiple needles" NIAH tasks increase difficulty not by increasing the quantity of needles or length of input, but by adding distractors between needles (Levy et al., 2024). What can we thematically explain the different variables at play, in order to inform better task design in the future?

Clearly, there are underlying qualitative differences that discriminate between those different tasks besides their natural and synthetic constructions, and besides their actual context length. Therefore, we require a more informative vocabulary to discuss the goals of each task design, what it accomplishes, and what it does not, in terms of measuring long-context capabilities.
