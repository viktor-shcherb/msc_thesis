# References

This file contains only the references that were cited in the section notes.

## References cited in notes

**Arora et al. (2023a)**
- Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher Ré. Zoology: Measuring and improving recall in efficient language models. ArXiv preprint, abs/2312.04927, 2023a.
- Cited in: 01_introduction.md (challenges in managing information over long sequences)

**Arora et al. (2023b)**
- Simran Arora, Brandon Yang, Sabri Eyuboglu, Avanika Narayan, Andrew Hojel, Immanuel Trummer, and Christopher Re. Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes, 2023b.
- Cited in: 09_appendix-b-experiment-contunued.md (FDA for PDF key-value retrieval)

**Arora et al. (2024a)**
- Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, and Christopher Ré. Simple linear attention language models balance the recall-throughput tradeoff. ArXiv preprint, abs/2402.18668, 2024a.
- Cited in: 01_introduction.md (challenges in managing information over long sequences); 03_gated-delta-networks.md (limitations in modeling local shifts)

**Akyurek et al. (2024)**
- Ekin Akyurek, Bailin Wang, Yoon Kim, and Jacob Andreas. In-Context Language Learning: Architectures and Algorithms, 2024.
- Cited in: 01_introduction.md (challenges in managing information over long sequences)

**Andriushchenko et al. (2023)**
- Maksym Andriushchenko, Francesco D'Angelo, Aditya Vardhan Varre, and Nicolas Flammarion. Why do we need weight decay in modern deep learning? ArXiv, abs/2310.04415, 2023.
- Cited in: 03_gated-delta-networks.md (weight decay perspective on gated delta rule)

**Arora et al. (2024b)**
- Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, and Christopher Ré. Just read twice: closing the recall gap for recurrent language models, 2024b.
- Cited in: 04_experiments.md (real-world recall-intensive tasks, repetition errors); 09_appendix-b-experiment-contunued.md (real-world task evaluation)

**Bai et al. (2023)**
- Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context understanding. ArXiv preprint, abs/2308.14508, 2023.
- Cited in: 04_experiments.md (LongBench benchmark for long context understanding evaluation); 09_appendix-b-experiment-contunued.md (14 tasks from LongBench)

**Bisk et al. (2020)**
- [PIQA benchmark]
- Cited in: 09_appendix-b-experiment-contunued.md (commonsense reasoning evaluation)

**Beck et al. (2024)**
- Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlstm: Extended long short-term memory. ArXiv preprint, abs/2405.04517, 2024.
- Cited in: 02_preliminary.md (example of recurrence structure with decay)

**Behrouz et al. (2024)**
- Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time, 2024.
- Cited in: 03_gated-delta-networks.md (weight decay in RNN test-time SGD updates); 05_related-work.md (nonlinear regression online learning formulation)

**Bischof & Loan (1985)**
- Christian H. Bischof and Charles Van Loan. The WY representation for products of householder matrices. In SIAM Conference on Parallel Processing for Scientific Computing, 1985.
- Cited in: 03_gated-delta-networks.md (WY representation for optimizing cumulative products)

**Bischof & Loan (1987)**
- Christian H. Bischof and Charles Van Loan. [cited in 01_introduction.md for WY representation]
- Cited in: 01_introduction.md (WY representation for efficient algorithm)

**Gers et al. (2000)**
- Felix A. Gers, Jurgen Schmidhuber, and Fred A. Cummins. Learning to forget: Continual prediction with LSTM. Neural Comput., 12(10):2451-2471, 2000.
- Cited in: 05_related-work.md (classical concept of gating/forgetting mechanisms originating in gated RNN literature)

**Clark et al. (2018)**
- [ARC-easy and ARC-challenge benchmarks]
- Cited in: 09_appendix-b-experiment-contunued.md (commonsense reasoning evaluation)

**Clark et al. (2019)**
- [BoolQ benchmark]
- Cited in: 09_appendix-b-experiment-contunued.md (commonsense reasoning evaluation)

**Chou et al. (2024)**
- Yuhong Chou, Man Yao, Kexin Wang, Yuqi Jie, Rui-Jie Zhu, Jibin Wu, Yiran Zhong, Yu Qiao, Bo XU, and Guoqi Li. MetaLA: Unified optimal linear approximation to softmax attention map.
- Cited in: 02_preliminary.md (matrix-valued gamma formulation)

**Dao (2023)**
- [Flash-Attention-2 kernel optimization]
- Cited in: 04_experiments.md (highly optimized Flash-Attention-2 kernel for 2K context window)

**Dasigi et al. (2021)**
- [QasperQA for scientific understanding]
- Cited in: 09_appendix-b-experiment-contunued.md (long context understanding evaluation)

**Dao & Gu (2024a)**
- [Mamba2 paper and State Space Duality]
- Cited in: 01_introduction.md (Mamba2 as recent enhancement with data-dependent gating); 02_preliminary.md (Mamba2 formulation, state space duality)

**Dua et al. (2019)**
- [Drop question-answering benchmark]
- Cited in: 09_appendix-b-experiment-contunued.md (real-world in-context retrieval evaluation)

**Dao & Gu (2024b)**
- [Mamba2 baseline]
- Cited in: 04_experiments.md (baseline model for experiments)

**De et al. (2024)**
- [Griffin hybrid architecture]
- Cited in: 03_gated-delta-networks.md (recent hybrid architecture example)

**Dong et al. (2025)**
- [hybrid linear/softmax attention within single layer]
- Cited in: 05_related-work.md (hybrid attention approaches)

**Fan et al. (2024)**
- Ting-Han Fan, Ta-Chung Chi, and Alexander Rudnicky. Advancing regular language reasoning in linear recurrent neural networks. In NAACL 2024, 2024.
- Cited in: 05_related-work.md (regular language recognition with DeltaNet's structural advantage)

**Fabbri et al. (2019)**
- [MultiNews for document summarization]
- Cited in: 09_appendix-b-experiment-contunued.md (long context understanding evaluation)

**Gao et al. (2021)**
- [lm-evaluation-harness]
- Cited in: 09_appendix-b-experiment-contunued.md (evaluation framework for ablation studies)

**Gliwa et al. (2019)**
- [SamSum dataset]
- Cited in: 09_appendix-b-experiment-contunued.md (long context understanding evaluation)

**Gardner (1988)**
- [Delta learning rule memory capacity compared to Hebbian learning]
- Cited in: 05_related-work.md (superior memory capacity of delta rule)

**González et al. (2024)**
- [approximating nonlinear recurrence methods]
- Cited in: 05_related-work.md (nonlinear recurrence approximation)

**Grazzi et al. (2024)**
- Riccardo Grazzi, Julien N. Siems, et al. Unlocking state-tracking in linear rnns through negative eigenvalues, 2024.
- Cited in: 02_preliminary.md (footnote about extending beta range for state tracking); 05_related-work.md (regular language recognition with DeltaNet's structural advantage, state tracking enhancements)

**Greff et al. (2015)**
- [Significance of gating/forgetting mechanisms reaffirmed]
- Cited in: 05_related-work.md (reaffirmation of gating mechanisms)

**Gu et al. (2022)**
- [S4 model with data-independent decay]
- Cited in: 05_related-work.md (evolution from data-independent decay mechanisms)

**Gu & Dao (2023)**
- [Mamba with data-dependent decay, gating mechanisms]
- Cited in: 04_experiments.md (baseline model); 05_related-work.md (data-dependent decay mechanisms, significance of gating); 09_appendix-b-experiment-contunued.md (evaluation methodology for commonsense reasoning)

**Guo et al. (2023)**
- [LCC dataset]
- Cited in: 09_appendix-b-experiment-contunued.md (long context understanding evaluation)

**He et al. (2025)**
- [matrix-valued gamma formulation]
- Cited in: 02_preliminary.md (matrix-valued gamma extension)

**Ho et al. (2020)**
- [2WikiMulti QA for multi-document QA]
- Cited in: 09_appendix-b-experiment-contunued.md (long context understanding evaluation)

**Hsieh et al. (2024)**
- [RULER benchmark suite with S-NIAH]
- Cited in: 03_gated-delta-networks.md (RULER benchmark suite source); 09_appendix-b-experiment-contunued.md (Needle-In-A-Haystack benchmark suite)

**Huang et al. (2021)**
- [GovReport for document summarization]
- Cited in: 09_appendix-b-experiment-contunued.md (long context understanding evaluation)

**Hua et al. (2022a)**
- [hybrid linear/softmax attention within single layer]
- Cited in: 05_related-work.md (hybrid attention approaches)

**Hua et al. (2022b)**
- [chunkwise parallel form for efficient training]
- Cited in: 01_introduction.md (benefits of chunkwise parallelism); 02_preliminary.md (chunkwise parallel form motivation)

**Irie et al. (2021)**
- [Delta rule language modeling, enhancing expressiveness through nonlinear recurrence]
- Cited in: 05_related-work.md (delta rule extends to language modeling, nonlinear recurrence attempts)

**Irie et al. (2022a)**
- Kazuki Irie, Robert Csordas, and Jurgen Schmidhuber. The dual form of neural networks revisited: Connecting test time predictions to training patterns via spotlights of attention. In ICML 2022, 2022a.
- Cited in: 03_gated-delta-networks.md (fast weight programming perspective)

**Irie et al. (2022b)**
- [Delta rule reinforcement learning, nonlinear recurrence]
- Cited in: 05_related-work.md (delta rule in RL, nonlinear recurrence)

**Irie et al. (2023)**
- [Delta rule theoretical limitations]
- Cited in: 05_related-work.md (theoretical limitations of delta rule)

**Irie & Schmidhuber (2023)**
- [Delta rule image generation]
- Cited in: 05_related-work.md (delta rule in image generation)

**Jelassi et al. (2024)**
- [in-context retrieval challenges for linear transformers]
- Cited in: 01_introduction.md (challenges persist in in-context retrieval)

**Joshi et al. (2017a)**
- [TriviaQA question-answering benchmark]
- Cited in: 09_appendix-b-experiment-contunued.md (real-world in-context retrieval evaluation)

**Joshi et al. (2017b)**
- [Trivia QA specialized task]
- Cited in: 09_appendix-b-experiment-contunued.md (long context understanding evaluation)

**Kočiský et al. (2018)**
- [Narrative QA for narrative comprehension]
- Cited in: 09_appendix-b-experiment-contunued.md (long context understanding evaluation)

**Kwiatkowski et al. (2019)**
- [NQ (Natural Questions) benchmark]
- Cited in: 09_appendix-b-experiment-contunued.md (real-world in-context retrieval evaluation)

**Li & Roth (2002)**
- [TRec specialized task]
- Cited in: 09_appendix-b-experiment-contunued.md (long context understanding evaluation)

**Liu et al. (2023)**
- [RepoBench-P]
- Cited in: 09_appendix-b-experiment-contunued.md (long context understanding evaluation)

**Joffrain et al. (2006)**
- [UT transform]
- Cited in: 03_gated-delta-networks.md (UT transform for matrix form)

**Krogh & Hertz (1991)**
- Anders Krogh and John A. Hertz. A simple weight decay can improve generalization. In Neural Information Processing Systems, 1991.
- Cited in: 03_gated-delta-networks.md (weight decay perspective on gated delta rule)

**Kulis & Bartlett (2010)**
- Brian Kulis and Peter L. Bartlett. Implicit online learning. In ICML 2010, pp. 575-582, 2010.
- Cited in: 03_gated-delta-networks.md (footnote 3: Longhorn's implicit online learning approach)

**Katharopoulos et al. (2020a)**
- [linear Transformers alternative]
- Cited in: 01_introduction.md (linear Transformers as alternative to softmax attention)

**Katharopoulos et al. (2020b)**
- [linear transformer formulation]
- Cited in: 02_preliminary.md (linear transformer formulation as linear recurrence)

**Lim et al. (2024)**
- [approximating nonlinear recurrence methods]
- Cited in: 05_related-work.md (nonlinear recurrence approximation)

**Liu et al. (2024)**
- [online learning framework for recurrent state updates]
- Cited in: 03_gated-delta-networks.md (online learning framework for formal analysis)

**Lockard et al. (2019)**
- [SWDE for structured HTML relation extraction]
- Cited in: 09_appendix-b-experiment-contunued.md (real-world in-context retrieval evaluation)

**Lu et al. (2025)**
- [matrix-valued gamma formulation]
- Cited in: 02_preliminary.md (matrix-valued gamma extension)

**Martin & Cundy (2018)**
- [efficient parallelism in modern forget gates]
- Cited in: 05_related-work.md (efficient parallelism enabled by removing hidden state dependency)

**Merity et al. (2017)**
- [Wikitext benchmark]
- Cited in: 09_appendix-b-experiment-contunued.md (commonsense reasoning evaluation)

**Merrill et al. (2024)**
- [regular language recognition, state-tracking beyond TC0]
- Cited in: 05_related-work.md (structural advantage for complex reasoning)


**MiniMax et al. (2025)**
- [MiniMax-α hybrid attention]
- Cited in: 05_related-work.md (interleaving hybrid attention layers example)

**Munkhdalai et al. (2024)**
- [hybrid linear/softmax attention within single layer]
- Cited in: 05_related-work.md (hybrid attention approaches)

**Nunez et al. (2024)**
- [hybrid linear/softmax attention within single layer]
- Cited in: 05_related-work.md (hybrid attention approaches)

**Orvieto et al. (2023)**
- [LRU with data-independent decay]
- Cited in: 05_related-work.md (evolution from data-independent decay mechanisms)

**Paperno et al. (2016)**
- [LAMBADA dataset]
- Cited in: 09_appendix-b-experiment-contunued.md (commonsense reasoning evaluation)

**Penedo et al. (2024)**
- [FineWeb-Edu dataset]
- Cited in: 04_experiments.md (training dataset source)

**Peng et al. (2021)**
- [Gated RFA recurrence structure]
- Cited in: 02_preliminary.md (recurrence structure example with decay)

**Peng et al. (2023)**
- Bo Peng et al. RWKV: Reinventing RNNs for the transformer era. In EMNLP 2023, 2023.
- Cited in: 05_related-work.md (RWKV4/5 as data-independent decay mechanism example)

**Peng et al. (2024)**
- [RWKV6 with data-dependent decay]
- Cited in: 05_related-work.md (recent architectures with data-dependent decay)

**Peng et al. (2024b)**
- [matrix-valued gamma formulation]
- Cited in: 02_preliminary.md (matrix-valued gamma extension)

**Prados & Kak (1989)**
- [Delta learning rule memory capacity]
- Cited in: 05_related-work.md (memory capacity advantage)

**Qin et al. (2023a)**
- [SiLU activation function performance]
- Cited in: 09_appendix-b-experiment-contunued.md (activation function selection)

**Qin et al. (2023b)**
- [HGRN1 with data-dependent decay, gating mechanisms, efficient parallelism]
- Cited in: 02_preliminary.md (matrix-valued gamma extension); 05_related-work.md (data-dependent decay, gating mechanisms, efficient parallelism)

**Rajpurkar et al. (2018)**
- [SQuAD question-answering benchmark]
- Cited in: 09_appendix-b-experiment-contunued.md (real-world in-context retrieval evaluation)

**Qin et al. (2024a)**
- [Lightning-Attention when gamma is data-dependent]
- Cited in: 02_preliminary.md (reduces to Lightning-Attention)

**Qin et al. (2024b)**
- [HGRN2 baseline, data-dependent decay, gating mechanisms, state tracking enhancements, matrix-valued gamma]
- Cited in: 02_preliminary.md (matrix-valued gamma extension); 04_experiments.md (baseline model); 05_related-work.md (data-dependent decay, gating mechanisms, state tracking enhancements)

**Ren et al. (2024)**
- [Samba hybrid architecture and baseline]
- Cited in: 03_gated-delta-networks.md (recent hybrid architecture); 04_experiments.md (baseline model)

**Sap et al. (2019)**
- [SIQA (Social IQa) benchmark]
- Cited in: 09_appendix-b-experiment-contunued.md (commonsense reasoning evaluation)

**Sakaguchi et al. (2020)**
- [WinoGrande benchmark]
- Cited in: 09_appendix-b-experiment-contunued.md (commonsense reasoning evaluation)

**Schlag et al. (2021a)**
- [DeltaNet associative recall, memory collisions, language modeling performance]
- Cited in: 01_introduction.md (memory collisions, DeltaNet); 03_gated-delta-networks.md (associative recall and language modeling)

**Schlag et al. (2021b)**
- [delta update rule]
- Cited in: 02_preliminary.md (delta update rule definition)

**Schöne et al. (2025)**
- [approximating nonlinear recurrence methods]
- Cited in: 05_related-work.md (nonlinear recurrence approximation)

**Siems et al. (2025)**
- [multiple products of householder transition matrices for high-rank transformations, state tracking abilities]
- Cited in: 02_preliminary.md (footnote about beta range for state tracking); 05_related-work.md (enhancements for state tracking)

**Smith et al. (2023)**
- [S5 with data-independent decay]
- Cited in: 05_related-work.md (evolution from data-independent decay mechanisms)

**Smolensky (1990)**
- [tensor product representation]
- Cited in: 01_introduction.md (outer-product-based key-value association memory)

**Sun et al. (2023a)**
- [RetNet formulation, WY representation, chunkwise parallel form]
- Cited in: 01_introduction.md (WY representation); 02_preliminary.md (RetNet when gamma is data-dependent, chunkwise parallel form); 04_experiments.md (baseline model)

**Sun et al. (2023b)**
- [chunkwise parallel form notation]
- Cited in: 02_preliminary.md (chunkwise parallel form notation)

**Sun et al. (2024a)**
- [TTT nonlinear regression online learning formulation]
- Cited in: 05_related-work.md (nonlinear regression online learning)

**Sun et al. (2024b)**
- [Gated RetNet recurrence structure]
- Cited in: 02_preliminary.md (recurrence structure example with decay)


**Trivedi et al. (2022)**
- [Musique for multi-hop reasoning]
- Cited in: 09_appendix-b-experiment-contunued.md (long context understanding evaluation)

**van der Westhuizen & Lasenby (2018)**
- [Significance of gating/forgetting mechanisms reaffirmed]
- Cited in: 05_related-work.md (reaffirmation of gating mechanisms)

**von Oswald et al. (2024)**
- [Mesa layer considering entire history in online learning]
- Cited in: 05_related-work.md (regression considering entire history)

**Waleffe et al. (2024)**
- [Hybrid Mamba2-Attention]
- Cited in: 05_related-work.md (interleaving hybrid attention layers example)

**Wang et al. (2025)**
- Ke Alexander Wang, Jiaxin Shi, and Emily B. Fox. Test-time regression: a unifying framework for designing sequence models with associative memory, 2025.
- Cited in: 03_gated-delta-networks.md (fast weight programming / test-time regression perspective)

**Wen et al. (2024)**
- Kaiyue Wen, Xingyu Dang, and Kaifeng Lyu. RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval. ArXiv preprint, abs/2402.18510, 2024.
- Cited in: 01_introduction.md (challenges in managing information over long sequences)

**Widrow et al. (1960)**
- [delta update rule original]
- Cited in: 01_introduction.md (delta rule origin); 02_preliminary.md (delta update rule definition)

**Yang & Zhang (2024)**
- [Flash Linear Attention implementation]
- Cited in: 05_related-work.md (chunkwise algorithm adaptation implementation)

**Yang et al. (2024a)**
- [GLA with data-dependent gating, WY representation, chunkwise parallelism, extended chunkwise form, matrix-valued gamma]
- Cited in: 01_introduction.md (GLA as recent enhancement, WY representation, chunkwise parallelism); 02_preliminary.md (extended chunkwise form, matrix-valued gamma extension)

**Yang et al. (2018)**
- [HotpotQA for multi-document QA]
- Cited in: 09_appendix-b-experiment-contunued.md (long context understanding evaluation)

**Yang et al. (2024b)**
- [DeltaNet baseline, hardware-efficient chunkwise algorithm, WY representation, chunkwise parallelism, synthetic benchmarks performance, moderate real-world performance, data-dependent structure, chunkwise notation, parallelized delta rule]
- Cited in: 01_introduction.md (DeltaNet, moderate real-world performance, WY representation, chunkwise parallelism); 03_gated-delta-networks.md (hardware-efficient chunkwise algorithm); 04_experiments.md (baseline model, synthetic in-context retrieval, real-world performance); 05_related-work.md (DeltaNet's data-dependent structure, moderate real-world benchmarks, parallelized delta rule); 06_conclusion.md (extended parallel algorithm); 09_appendix-b-experiment-contunued.md (L2 normalization and feature map choice)

**Zancato et al. (2024)**
- [hybrid linear/softmax attention within single layer]
- Cited in: 05_related-work.md (hybrid attention approaches)

**Zhang et al. (2024)**
- [GSA with data-dependent decay, matrix-valued gamma]
- Cited in: 02_preliminary.md (matrix-valued gamma extension); 05_related-work.md (data-dependent decay)

**Zhang et al. (2025)**
- [hybrid linear/softmax attention within single layer]
- Cited in: 05_related-work.md (hybrid attention approaches)

**Zellers et al. (2019)**
- [HellaSwag benchmark]
- Cited in: 09_appendix-b-experiment-contunued.md (commonsense reasoning evaluation)

**Zhong et al. (2021)**
- [QMSum for document summarization]
- Cited in: 09_appendix-b-experiment-contunued.md (long context understanding evaluation)

---

**Footnote references:**

⁵ https://github.com/BlinkDL/RWKV-LM/tree/main/RWKV-v7

⁶ https://github.com/fla-org/flash-linear-attention/tree/main/fla/ops/generalized_delta_rule

⁷ We use Mamba2's parameterization for α but omit it for brevity [p. 6, footnote]
