# Conclusion [p. 10]

In this work, we introduced Gated DeltaNet, which enables better key-value association learning compared to Mamba2 and more adaptive memory clearance than DeltaNet, leading to consistently better empirical results across various tasks [p. 10]. We extended the parallel algorithm from Yang et al. (2024b) to enable hardware-efficient training [p. 10]. Hybrid Gated DeltaNet model achieves even higher training throughput and overall performance, making it well-suited for practical deployment [p. 10].
