# Related Work [p. 9-10]

## Gated linear RNN [p. 9]

Large linear recurrent language models have attracted significant attention due to their training and inference efficiency [p. 9]. The field of linear RNNs has rapidly evolved from using data-independent decay mechanisms, as exemplified by models like S4 (Gu et al., 2022), S5 (Smith et al., 2023), LRU (Orvieto et al., 2023), RWKV4/5 (Peng et al., 2023), and RetNet (Sun et al., 2023a), to incorporating data-dependent decay mechanisms in more recent architectures such as HGRN1/2 (Qin et al., 2024b; 2023b), Mamba1/2 (Gu & Dao, 2023; Dao & Gu, 2024a), RWKV6 (Peng et al., 2024), GSA (Zhang et al., 2024) [p. 9]. This transition stems from the proven advantages of gating/forgetting mechanisms (termed selective mechanisms in Mamba)—a classical concept originating in the gated RNN literature (Gers et al., 2000) whose significance has been consistently reaffirmed (Greff et al., 2015; van der Westhuizen & Lasenby, 2018; Qin et al., 2024b; 2023b; Gu & Dao, 2023) [p. 9].

Modern forget gates differ from traditional designs like those in LSTM by removing the dependency on the previous hidden state, relying solely on input data [p. 9]. This modification enables efficient parallelism across sequence lengths (Martin & Cundy, 2018; Qin et al., 2023b) [p. 9]. The absence of a forget gate has been a notable limitation in DeltaNet, and our gated extension addresses this gap in a natural, effective, and hardware-efficient way [p. 9-10]. We also note a recent concurrent work RWKV-7⁵ using a similar idea, but with a more relaxable formalism using diagonal-plus-low-rank transitions: $\mathbf{S}_{t-1}(\text{diag}(\boldsymbol{d}_t) + \boldsymbol{a}_t \boldsymbol{b}_t^\top) + \boldsymbol{v}_t \boldsymbol{k}_t^\top \in \mathbb{R}^{d_k}$ [p. 10]. The chunkwise algorithm could be similarly adapted to this case, as implemented in Flash Linear Attention (Yang & Zhang, 2024)⁶ [p. 10].

## Delta rule [p. 10]

The delta learning rule demonstrates superior memory capacity compared to Hebbian learning (Gardner, 1988; Prados & Kak, 1989), an advantage DeltaNet leverages while linear transformers rely on Hebbian-like rules [p. 10]. This memory capacity advantage is evident in synthetic in-context learning tasks and extends to language modeling (Irie et al., 2021; Yang et al., 2024b), reinforcement learning (Irie et al., 2022b), and image generation (Irie & Schmidhuber, 2023) [p. 10]. Yang et al. (2024b) parallelized delta rule computation and demonstrated how DeltaNet's data-dependent identity-plus-low-rank structure $(\mathbf{I} - \beta_t \boldsymbol{k}_t \boldsymbol{k}_t^\top)$ offers greater flexibility than Mamba2's data-dependent diagonal matrices ($\alpha_t \mathbf{I}$) [p. 10]. This structural advantage could enable complex reasoning, including regular language recognition (Fan et al., 2024; Grazzi et al., 2024) and state-tracking beyond TC⁰ complexity (Merrill et al., 2024)—crucial for coding and reasoning applications [p. 10].

Despite these significant advantages, the delta rule faces theoretical limitations (Irie et al., 2023) and shows only moderate performance on real-world benchmarks (Yang et al., 2024b), suggesting room for improvement [p. 10]. Previous attempts to enhance expressiveness through nonlinear recurrence (Irie et al., 2021; 2022b) addressed some limitations but incurred a performance-efficiency tradeoff [p. 10]. Recent work proposes some enhancements without compromising parallelism for better state tracking performance, such as HGRN2 (Qin et al., 2024) and multiple products of householder transition matrices (Siems et al., 2025) which enable high-rank transformations [p. 10]. These methods could be combined with Gated DeltaNet seamlessly [p. 10].

From a (online) learning objective perspective, alternative formulations could further extend expressiveness: nonlinear regression $(\mathcal{L}(\mathbf{S}_t) = \frac{1}{2}\|f_{\mathbf{S}_t}(\boldsymbol{k}_t) - \boldsymbol{v}_t\|^2)$ as in TTT (Sun et al., 2024a) and Titans (Behrouz et al., 2024), where $f_{\mathbf{S}}$ is a nonlinear function parameterized by $\mathbf{S}$; or regression considering the entire history $(\mathcal{L}(\mathbf{S}_t) = \frac{1}{2}\sum_{i=1}^t \|\mathbf{S}_t \boldsymbol{k}_i - \boldsymbol{v}_i\|^2)$ as in Mesa layer (von Oswald et al., 2024)—analogous to the difference between Least Mean Square and Recursive Least Square algorithms [p. 10]. However, these more expressive variants introduce nonlinear recurrence and require workarounds, such as performing nonlinear updates only after processing entire chunks (as in TTT and Titans); or approximating nonlinear recurrence methods like Lim et al. (2024); González et al. (2024); Schöne et al. (2025) [p. 10].

## Hybrid models [p. 10]

In this work, we explore interleaving hybrid attention layers across layers, which is commonly used such as in MiniMax-01 (MiniMax et al., 2025) and Hybrid Mamba2-Attention (Waleffe et al., 2024) [p. 10]. It is also interesting to investigate hybrid linear/softmax attention within a single layer (Hua et al., 2022a; Zancato et al., 2024; Munkhdalai et al., 2024; Nunez et al., 2024; Dong et al., 2025; Zhang et al., 2025) [p. 10].
