# Round and Round We Go! What makes Rotary Positional Encodings useful?

**Authors:** Federico Barbero*, Alex Vitvitskyi, Christos Perivolaropoulos, Razvan Pascanu, Petar Veličković
**Affiliations:** Federico Barbero (University of Oxford; *work performed while at Google DeepMind); Alex Vitvitskyi, Christos Perivolaropoulos, Razvan Pascanu, Petar Veličković (Google DeepMind)

## Publication Status

- **arXiv preprint:** October 2024, arXiv:2410.06205 (v1 October 2024, v2 February 2025, v3 May 2025)
- **Peer-reviewed:** Yes
- **Conference:** ICLR 2025 (The Thirteenth International Conference on Learning Representations), Singapore, April 24--28, 2025, poster presentation
- **Status:** Published conference paper

## Preferred Citation

Cite the ICLR 2025 version:

> Barbero, F., Vitvitskyi, A., Perivolaropoulos, C., Pascanu, R., & Veličković, P. (2025). Round and Round We Go! What makes Rotary Positional Encodings useful? In International Conference on Learning Representations.

## Links

- arXiv: https://arxiv.org/abs/2410.06205
- OpenReview: https://openreview.net/forum?id=GtvuNrk58a
- ICLR 2025 poster: https://iclr.cc/virtual/2025/poster/30251
