# References [p. 19-21]

This section contains selected references from the paper's bibliography that are relevant to the content extracted in the section notes.

## References cited in paper

**Amazon Web Services.** *Amazon Simple Storage Service (Amazon S3)*. Web. Available at: https://aws.amazon.com/s3/ (2023). URL: https://aws.amazon.com/s3/ (visited on 12/15/2023).

**Bai, Shuai et al.** *Qwen2.5-VL Technical Report*. 2025. arXiv: 2502.13923 [cs.CV]. URL: https://arxiv.org/abs/2502.13923

**Bonatti, Rogerio et al.** *Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale*. 2024. arXiv: 2409.08264 [cs.AI]. URL: https://arxiv.org/abs/2409.08264

**Chen, Lin et al.** "Are We on the Right Way for Evaluating Large Vision-Language Models?" In: *arXiv preprint arXiv:2403.20330* (2024).

**Chen, Tianqi et al.** *Training Deep Nets with Sublinear Memory Cost*. 2016. arXiv: 1604.06174 [cs.LG]. URL: https://arxiv.org/abs/1604.06174

**Dao, Tri et al.** *FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness*. 2022. arXiv: 2205.14135 [cs.LG]. URL: https://arxiv.org/abs/2205.14135

**DeepSeek-AI, Daya Guo, et al.** *DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning*. 2025. arXiv: 2501.12948 [cs.CL]. URL: https://arxiv.org/abs/2501.12948

**DeepSeek-AI, Aixin Liu, et al.** *DeepSeek-V3 Technical Report*. 2025. arXiv: 2412.19437 [cs.CL]. URL: https://arxiv.org/abs/2412.19437

**Dehghani, Mostafa et al.** *Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution*. 2023. arXiv: 2307.06304 [cs.CV]. URL: https://arxiv.org/abs/2307.06304

**Fedus, William, Barret Zoph, and Noam Shazeer.** *Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity*. 2022. arXiv: 2101.03961 [cs.LG]. URL: https://arxiv.org/abs/2101.03961

**Fu, Chaoyou et al.** "Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis". In: *arXiv:2405.21075* (2024).

**Fu, Xingyu et al.** "Blink: Multimodal large language models can see but not perceive". In: *European Conference on Computer Vision*. Springer. 2024, pp. 148–166.

**Gadre, Samir Yitzhak et al.** "Datacomp: In search of the next generation of multimodal datasets". In: *Advances in Neural Information Processing Systems* 36 (2024).

**Guo, Travis et al.** *MAmmioTH-VL: Eliciting Multimodal Instruction Tuning at Scale*. 2024. arXiv: 2412.05237 [cs.CL]. URL: https://arxiv.org/abs/2412.05237

**Hu, Jieni et al.** "Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos". In: *arXiv preprint arXiv:2501.13826* (2025).

**Huang, Yanping et al.** *GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism*. 2019. arXiv: 1811.06965 [cs.CV]. URL: https://arxiv.org/abs/1811.06965

**Jacobs, Sam Ade et al.** *DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models*. 2023. arXiv: 2309.14509 [cs.LG]. URL: https://arxiv.org/abs/2309.14509

**Jordan, Keller et al.** *Muon: An optimizer for hidden layers in neural networks*. 2024. URL: https://kellerjordan.github.io/posts/muon/

**Kembhavi, Aniruddha et al.** "A diagram is worth a dozen images". In: *European conference on computer vision*. Springer. 2016. pp. 235–251.

**Korthikanti, Vijay et al.** *Reducing Activation Recomputation in Large Transformer Models*. 2022. arXiv: 2205.05198 [cs.LG]. URL: https://arxiv.org/abs/2205.05198

**Laurençon, Hugo et al.** "Obelics: An open web-scale filtered dataset of interleaved image-text documents". In: *Advances in Neural Information Processing Systems* 36 (2024).

**Li, Bo et al.** *LLaVA-OneVision: Easy Visual Task Transfer*. 2024. arXiv: 2408.03326 [cs.CV]. URL: https://arxiv.org/abs/2408.03326

**Li, Dongxu et al.** *Aria: An Open Multimodal Native Mixture-of-Experts Model*. 2024. arXiv: 2410.05993 [cs.CV]. URL: https://arxiv.org/abs/2410.05993

**Li, Kaivin et al.** "ScreenSpot-Pro: GUI Prompt-based Professional High-Resolution Computer Use". In: *Workshop on Reasoning and Planning for Large Language Models*. 2025.

**Li, Shen et al.** *PyTorch Distributed: Experiences on Accelerating Data Parallel Training*. 2020. arXiv: 2006.15704 [cs.DC]. URL: https://arxiv.org/abs/2006.15704

**Liu, Hao, Matei Zaharia, and Pieter Abbeel.** *Ring Attention with Blockwise Transformers for Near-Infinite Context*. 2023. arXiv: 2310.01889 [cs.CL]. URL: https://arxiv.org/abs/2310.01889

**Liu, Jingyuan et al.** "Muon is Scalable for LLM Training". In: *arXiv preprint arXiv:2502.16982* (2025). Cited as J. Liu et al. 2025a (Moonlight model) and J. Liu et al. 2025b (Muon optimizer) in the paper.

**Lu, Pan et al.** "Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts". In: *arXiv preprint arXiv:2310.02255* (2023).

**Mangalam, Karttikeya, Raiymbek Akshulakov, and Jitendra Malik.** "Egoschema: A diagnostic benchmark for very long-form video language understanding". In: *Advances in Neural Information Processing Systems* 36 (2023), pp. 46212–46244.

**Mathew, Minesh et al.** "Infographicvqa". In: *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision*. 2022, pp. 1697–1706.

**Narayanan, Deepak et al.** *Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM*. 2021. arXiv: 2104.04473 [cs.CL]. URL: https://arxiv.org/abs/2104.04473

**OpenAI.** "Learning to reason with LLMs". In: (2024). URL: https://openai.com/index/learning-to-reason-with-llms/

**OpenAI et al.** *GPT-4o System Card*. 2024. arXiv: 2410.21276 [cs.CL]. URL: https://arxiv.org/abs/2410.21276

**Rajbhandari, Samyam et al.** "Zero: Memory optimizations toward training trillion parameter models". In: *SC20: International Conference for High Performance Computing, Networking, Storage and Analysis*. IEEE. 2020, pp. 1–16.

**Schuhmann, Christoph et al.** "Laion-5b: An open large-scale dataset for training next generation image-text models". In: *Advances in Neural Information Processing Systems* 35 (2022), pp. 25278–25294.

**Shangguan, Ziyao et al.** "TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models". In: *International Conference on Learning Representations*. 2025. URL: https://openreview.net/forum?id=C14c83Mfe

**Su, Dan et al.** "Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset". In: *arXiv preprint arXiv:2412.02595* (2024).

**Su, Jianlin et al.** *RoFormer: Enhanced Transformer with Rotary Position Embedding*. 2023. arXiv: 2104.09864 [cs.CL]. URL: https://arxiv.org/abs/2104.09864

**Team, Gemini et al.** *Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context*. 2024. arXiv: 2403.05530 [cs.CL]. URL: https://arxiv.org/abs/2403.05530

**Team, Gemma et al.** *Gemma 3 Technical Report*. 2025. arXiv: 2503.19786 [cs.CL]. URL: https://arxiv.org/abs/2503.19786

**Team, Kimi et al.** "Kimi k1. 5: Scaling reinforcement learning with llms". In: *arXiv preprint arXiv:2501.12599* (2025).

**Tong, Shengbang et al.** *Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs*. 2024. arXiv: 2406.16860 [cs.CV]. URL: https://arxiv.org/abs/2406.16860

**Wang, Ke et al.** "Measuring multimodal mathematical reasoning with math vision dataset". In: *arXiv preprint arXiv:2402.14804* (2024).

**Wei, Haoran et al.** "General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model". In: *arXiv preprint arXiv:2409.01704* (2024).

**Wu, Haoming et al.** "Longvideobench: A benchmark for long-context interleaved video-language understanding". In: *Advances in Neural Information Processing Systems* 37 (2024), pp. 28828–28857.

**Wu, Zhiyu et al.** *DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding*. 2024. arXiv: 2412.10302 [cs.CV]. URL: https://arxiv.org/abs/2412.10302

**Xie, Tianbao et al.** "Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments". In: *Advances in Neural Information Processing Systems* 37 (2024).

**Xu, Yiheng et al.** *Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction*. 2024. arXiv: 2412.04454 [cs.CL]

**Yang, Jihan et al.** "Thinking in space: How multimodal large language models see, remember, and recall spaces". In: *arXiv preprint arXiv:2412.14171* (2024).

**Yu, Jiahui et al.** *CoCa: Contrastive Captioners are Image-Text Foundation Models*. 2022. arXiv: 2205.01917 [cs.CV]. URL: https://arxiv.org/abs/2205.01917

**Yu, Wenhao et al.** "Mm-vet: Evaluating large multimodal models for integrated capabilities". In: *International conference on machine learning*. PMLR. 2024.

**Yue, Xiang, Yuansheng Ni, et al.** "Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi". In: *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2024, pp. 9556–9567.

**Yue, Xiang, Xingwei Qu, et al.** "Mammoth: Building math generalist models through hybrid instruction tuning". In: *arXiv preprint arXiv:2309.05653* (2023).

**Zhai, Xiaohua et al.** *Sigmoid Loss for Language Image Pre-Training*. 2023. arXiv: 2303.15343 [cs.CV]. URL: https://arxiv.org/abs/2303.15343

**Zhao, Yilun et al.** "MMVU: Measuring Expert-Level Multi-Discipline Video Understanding". In: *arXiv preprint arXiv:2501.12380* (2025).

**Zhou, Junjie et al.** "Mlvu: A comprehensive benchmark for multi-task long video understanding". In: *arXiv preprint arXiv:2406.04264* (2024).

**Zhu, Wanrong et al.** "Multimodal c4: An open, billion-scale corpus of images interleaved with text". In: *Advances in Neural Information Processing Systems* 36 (2024).

## Notes

- The paper uses author-year citation format in the reference list
- References span topics including vision-language models, multimodal benchmarks, training optimization, long-context processing, and agent-based evaluation
- Key cited works include GPT-4o, Qwen2.5-VL, Gemma-3, DeepSeek-V3, and various benchmark datasets (MMMU, MathVista, VideoMME, etc.)
- Many references are arXiv preprints from 2023-2025, indicating recent research developments
