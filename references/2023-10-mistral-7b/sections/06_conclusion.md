# 6 Conclusion [p. 6]

[p. 6] The work on Mistral 7B demonstrates that language models may compress knowledge more than what was previously thought. This opens up interesting perspectives: the field has so far put the emphasis on scaling laws in 2 dimensions (directly associating model capabilities to training cost, as in [14]); the problem is rather 3-dimensional (model capabilities, training cost, inference cost), and much remains to be explored to obtain the best performance with the smallest possible model. [p. 6]
