# B Additional Related Works [p. 15-16]

## Sparse Transformers [p. 15]

The literature on efficient Transformer models primarily focuses on reducing the computational and memory complexity of the self-attention mechanism. A relevant line of work involves sparsifying the attention matrix by restricting the field of view to fixed, predefined patterns, such as local windows or block patterns with fixed strides (Tay et al., 2022). Sparse Transformer (Child et al., 2019) introduces sparse factorizations of the attention matrix, reducing the computational complexity of attention to $O(n\sqrt{n})$. LongFormer (Beltagy et al., 2020) combines dilated local windowed attention with task-motivated global attention. Extended Transformer Construction (ETC) (Ainslie et al., 2020) presents a novel global-local attention mechanism, incorporating four types of attention patterns: global-to-global, local-to-local, local-to-global, and global-to-local. Building on ETC, BigBird (Zaheer et al., 2020a) proposes another linear complexity attention alternative, utilizing global tokens, local sliding window attentions, and random attention. [p. 15]

However, these methods have several limitations:
1. Sparse Transformer and ETC require custom GPU kernels for a specific block-sparse variant of matrix-matrix multiplication.
2. LongFormer, ETC, and BigBird all rely on a global attention pattern, which is unsuitable for autoregressive language models.
3. These methods are incompatible with pre-trained models, necessitating retraining from scratch. [p. 15]

In contrast, StreamingLLM offers ease of implementation using standard GPU kernels and is compatible with pre-trained autoregressive language models using dense attention, which are prevalent in the NLP community. This compatibility provides a significant advantage, allowing for the leveraging of existing pre-trained models without any fine-tuning. [p. 15]

## Concurrent Works [p. 15-16]

The authors' research coincides with the work of Han et al., who conducted a theoretical study on the length generalization failure of language models, identifying three out-of-distribution factors. Their approach, inspired by this analysis, involves employing a "Lambda"-shaped attention pattern and reconfiguring position encoding distances to enhance length generalization in LLMs. This approach bears a resemblance to StreamingLLM's methodology. However, the authors' work uncovers the "attention sink" phenomenon, wherein Transformer models tend to assign high attention scores to initial tokens with small semantics. This phenomenon extends beyond the scope of length generalization failure, indicating a more pervasive issue in Transformer models. The authors observe this "attention sink" behavior not only in auto-regressive language models but also in encoder Transformers such as BERT (see Section H), and Vision Transformers (ViTs) (Darcet et al., 2023), suggesting its broader prevalence in Transformer architectures. To mitigate the "attention sink" phenomenon, the authors propose the introduction of a learnable sink token during pre-training, and support their findings with extensive ablation studies. [p. 15-16]

In parallel, Darcet et al. observed similar attention concentration on random background patch tokens in Vision Transformers, termed as "registers." These registers act as repositories for global image information. Their solution, adding dedicated "register" tokens, aims to balance attention distribution. The authors' finding of "attention sinks" parallels this concept. In the authors' paper, the "attention sinks" are initial tokens that disproportionately attract attention from subsequent tokens. Introducing a dedicated sink token during pre-training prevents the model from inappropriately using content tokens as attention sinks, leading to more effective attention distribution. [p. 16]

However, a key difference exists: "registers" in Vision Transformers function as global information holders within intermediate layers, whereas the authors' "attention sinks" are positioned as initial tokens in autoregressive models. This positional variance suggests that the softmax function in attention computation might play a more fundamental role in the emergence of attention sinks. [p. 16]
