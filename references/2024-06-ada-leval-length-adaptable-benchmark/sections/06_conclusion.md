# Conclusion [p. 9]

[p. 9] The paper introduces Ada-LEval, a length-adaptable dataset to assess long-context capability of LLMs. Comprehensive experiments are conducted on multiple LLMs with the following findings:

- All open-source models still lag significantly behind state-of-the-art proprietary models in terms of long context capability.
- When the input length scales to 4,000 tokens, most open-source models rapidly deteriorate to random guess level.
- The capability of proprietary models is also severely limited in the meantime.
- When it comes to the ultra-long setting (32,000+ tokens), no proprietary model notably outperforms the random baseline.

Ada-LEval is the first benchmark that evaluates LLMs under the ultra-long setting, and the authors hope that the limitations pointed out by this benchmark can serve as valuable references for future developments of long-context LLMs. [p. 9]

## Acknowledgement [p. 9]

This project is supported by the National Key R&D Program of China No.2022ZD0161600 and the Shanghai Postdoctoral Excellence Program (No.2023023). [p. 9]
