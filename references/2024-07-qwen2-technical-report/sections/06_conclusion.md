# Conclusion [p. 20]

[p. 20] This technical report has presented the Qwen2 series, a versatile suite of foundational and instruction-tuned language models, ranging from 0.5 to 72 billion parameters, including models of dense and Mixture-of-Experts architecture. Qwen2 outperforms previous open-weight models, notably its predecessor Qwen1.5, and displays competitive performance against proprietary models across a broad spectrum of benchmarks in language understanding, generation, multilingual capabilities, coding, mathematics, and reasoning. In this update, extra focus is on long-context, multilingual, coding, mathematics capabilities and safety and responsibility. In a commitment to fostering innovation and accessibility within the community, the authors have made the Qwen2 model weights openly accessible, which enables researchers and developers to harness the full potential of Qwen2 in a variety of applications and research projects. Through these efforts, they aim to contribute to the advancement of AI technologies and their positive impact on society.
