# Post-training [p. 6â€“8]

[p. 6] Following extensive large-scale pre-training, Qwen2 engages in a post-training phase pivotal in enhancing proficiency across a broad spectrum of domains, including coding, mathematics, logical reasoning, instruction following, and multilingual comprehension. Moreover, it ensures that the generation from the models is in harmony with human values, making it helpful, honest, and harmless. Unlike traditional methods that heavily rely on extensive human supervision, the approach focuses on scalable alignment with minimal human annotation (Cao et al., 2024). Specifically, the authors investigate methods to acquire high-quality demonstration and preference data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), aiming to minimize the need for human labeling while maximizing the quality and reliability of the data.

## Post-training Data [p. 6]

[p. 6] The post-training data primarily consists of two components:
- **Demonstration data** D = {(x_i, y_i)}, where x_i represents the instruction and y_i represents a satisfactory response
- **Preference data** P = {(x_i, y_i^+, y_i^-)}, where y_i^+ and y_i^- are two responses to x_i, with y_i^+ being the preferred choice over y_i^-

The set D is utilized in SFT, whereas P is employed in RLHF.

The construction of training data entails a two-step process: **collaborative data annotation** and **automated data synthesis**. First, the data ontology is extracted from large-scale instruction corpora, leading to a broad and diverse set of high-quality instructions. These instructions are systematically enhanced to incorporate greater complexity. Through human annotation, the target response y_i and their positive and negative counterparts (y_i^+, y_i^-) are obtained.

[p. 7] Subsequently, a variety of automated alignment strategies are employed to synthesize a substantial volume of artificially annotated data across the domains of code, mathematics, instruction-following, creation, role-playing, and safety.

### Collaborative Data Annotation [p. 7]

**Automatic Ontology Extraction**: The process initiates with the application of InsTag (Lu et al., 2024c), an open-set fine-grained tagger, to extract the underlying ontology from a large-scale instruction dataset. Subsequent manual refinement ensures the accuracy of the extracted ontology.

**Instruction Selection**: Each instruction, with tags annotated, is evaluated for tag diversity, semantic richness, complexity, and intent completeness. Based on these criteria, a set of representative instructions is selected (Dong et al., 2023).

**Instruction Evolution**: To enrich the instruction dataset, a self-evolution strategy (Zhao et al., 2024) is employed, prompting the Qwen models to add constraints or requirements to existing instructions, thereby increasing their complexity and ensuring a diverse range of difficulty levels within the dataset.

**Human Annotation**: Multiple responses to an instruction are obtained using diverse generation strategies and Qwen models of different scales. Annotators rank these responses based on their preferences, ensuring the best response meets established criteria, yielding both demonstration and preference data.

### Automated Data Synthesis [p. 7]

[p. 7] Maintaining the quality of annotations for responses to instructions presents significant challenges on a large scale, particularly those that require expertise, experience, carefulness, or patience. To address these challenges, various automated alignment strategies are devised to synthesize data at scale.

**Rejection Sampling**: For mathematical or similar tasks with definitive final answers, rejection sampling (Yuan et al., 2023) is applied to improve the quality of solutions. LLMs are tasked to generate multiple responses, namely the reasoning paths, for each instruction. Paths that result in accurate conclusions and are considered reasonable by the model are preserved, serving as demonstration data. Preference data is generated by contrasting correct and incorrect paths.

**Execution Feedback**: For coding tasks, LLMs are employed to generate solutions and associated test cases. The efficacy of these solutions is evaluated by compiling and executing them against the test cases, thereby creating demonstration and preference data. This methodology is also applicable to assessing instruction following (Dong et al., 2024). For each instruction with constraints, e.g., length limit, the LLM is tasked to generate a Python verification function to ensure the response aligns with the instruction requirements.

**Data Repurposing**: Creating skilled responses in literary writing tasks is challenging for annotators without specialized training. To tackle this problem, high-quality literary works from the public domain are aggregated and LLMs are employed to develop instructions with varying levels of detail. These instructions, paired with the original works, serve as demonstration data. For example, to compile roleplay data with vivid and engaging responses, detailed character profiles from knowledge repositories such as Wikipedia are sourced and LLMs are instructed to generate corresponding instructions and responses (Lu et al., 2024b). This process, similar to a reading comprehension task, ensures that the integrity of the character's profile is maintained.

**Constitutional Feedback**: Constitutional AI refers to the process of guiding LLMs to generate responses based on predefined sets of principles (Bai et al., 2022). To ensure adherence to guidelines such as safety and values, a constitution dataset was compiled. This dataset delineates principles to be followed and those to be avoided. It was used to instruct LLMs to produce responses that either are aligned with or deviated from these guidelines, serving as a reference for demonstration and preference data.

## Supervised Fine-tuning [p. 8]

[p. 8] An extensive instruction dataset featuring more than **500,000 examples** was assembled, covering skills such as instruction following, coding, mathematics, logical reasoning, role-playing, multilingualism, and safety. The model was fine-tuned for **two epochs** with a **sequence length of 32,768 tokens**. To optimize learning, the learning rate was gradually decreased from **7 x 10^-6 to 7 x 10^-7**. To address overfitting, a **weight decay of 0.1** was applied and gradients were clipped at a **maximum value of 1.0**.

## Reinforcement Learning from Human Feedback [p. 8]

[p. 8] The training regime for RLHF comprises two sequential stages: **offline and online training**.

In the **offline training stage**, a pre-compiled preference dataset P is used to maximize the difference in likelihood between y_i^+ and y_i^- with Direct Preference Optimization (DPO, Rafailov et al., 2023).

In the **online training stage**, the model iteratively refines its performance in real-time, leveraging reward models for immediate feedback. Specifically, multiple responses are sampled from the current policy model, and the reward model selects the most and the least preferred responses, forming preference pairs that are used for DPO in each episode. Moreover, the Online Merging Optimizer (Lu et al., 2024a) is employed to mitigate the alignment tax, i.e., the performance degradation associated with aligning model generation with human preferences.
