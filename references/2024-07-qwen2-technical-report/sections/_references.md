# References

References cited in the section notes, with bibliographic details from the paper's reference list.

---

**Abdin et al., 2024**
Marah Abdin, Jyoti Aneja, Sebastien Bubeck, et al. Phi-2: The surprising power of small language models. 2024.
- Cited in 05_evaluation.md as baseline for smaller model comparison (Table 5).

**AI@Meta, 2024**
AI@Meta. Llama 3 model card, 2024.
- Cited in 01_introduction.md as state-of-the-art open-weight model series; in 05_evaluation.md as baseline for 72B and 7B comparisons.

**Ainslie et al., 2023**
Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: Training generalized multi-query Transformer models from multi-head checkpoints. In *EMNLP*, pp. 4895–4901. Association for Computational Linguistics, 2023.
- Cited in 02_tokenizer-and-model.md as the source of Grouped Query Attention (GQA) adopted in Qwen2.

**An et al., 2024**
Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, and Lingpeng Kong. Training-free long-context scaling of large language models. *CoRR*, abs/2402.17463, 2024.
- Cited in 02_tokenizer-and-model.md for Dual Chunk Attention (DCA); in 03_pre-training.md for long-context training; in 05a_instruction-tuned-model.md for improving long-context abilities with YARN+DCA.

**Anthropic, 2024**
Anthropic. The Claude 3 model family: Opus, Sonnet, Haiku. Technical report, Anthropic, AI, 2024.
- Cited in 01_introduction.md as a pinnacle proprietary model on Chatbot Arena.

**Austin et al., 2021**
Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. *CoRR*, abs/2108.07732, 2021.
- Cited in 05_evaluation.md as the MBPP benchmark.

**Bai et al., 2022**
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, et al. Constitutional AI: Harmlessness from AI feedback. *CoRR*, abs/2212.08073, 2022.
- Cited in 04_post-training.md as the basis for constitutional feedback data synthesis.

**Bai et al., 2023a**
Jinze Bai, Shuai Bai, Yunfei Chu, et al. Qwen technical report. *CoRR*, abs/2309.16609, 2023a.
- Cited in 01_introduction.md, 02_tokenizer-and-model.md, 03_pre-training.md as the predecessor Qwen model.

**Bai et al., 2023b**
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: A frontier large vision-language model with versatile abilities. *CoRR*, abs/2308.12966, 2023b.
- Cited in 01_introduction.md as the vision-language model in the Qwen family.

**Bandarkar et al., 2023**
Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. The Belebele benchmark: A parallel reading comprehension dataset in 122 language variants. *CoRR*, abs/2308.16884, 2023.
- Cited in 05_evaluation.md as a multilingual understanding benchmark (BELEBELE).

**Cao et al., 2024**
Boxi Cao, Keming Lu, Xinyu Lu, Jiawei Chen, Mengjie Ren, Hao Xiang, Peilin Liu, Yaojie Lu, Ben He, Xianpei Han, Le Sun, Hongyu Lin, and Bowen Yu. Towards scalable automated alignment of LLMs: A survey. *CoRR*, abs/2406.01252, 2024.
- Cited in 04_post-training.md as the reference for scalable alignment with minimal human annotation.

**Cassano et al., 2023**
Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q. Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda. MultiPL-E: A scalable and polyglot approach to benchmarking neural code generation. *IEEE Trans. Software Eng.*, 49(7):3675–3691, 2023.
- Cited in 05_evaluation.md as the MultiPL-E coding benchmark.

**Chen et al., 2021**
Mark Chen, Jerry Tworek, Heewoo Jun, et al. Evaluating large language models trained on code. *CoRR*, abs/2107.03374, 2021.
- Cited in 01_introduction.md and 05_evaluation.md as the HumanEval benchmark.

**Chen et al., 2023a**
Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. TheoremQA: A theorem-driven question answering dataset. In *EMNLP*, pp. 7889–7901. Association for Computational Linguistics, 2023a.
- Cited in 05_evaluation.md as the Theorem QA benchmark.

**Chen et al., 2023b**
Zhihong Chen, Shuo Yan, Juhao Liang, Feng Jiang, Xiangbo Wu, Fei Yu, Guiming Hardy Chen, Junying Chen, Hongbo Zhang, Li Jianquan, Wan Xiang, and Benyou Wang. Multilingual-SIFT: Multilingual supervised instruction fine-tuning, 2023b.
- Cited in 05_evaluation.md for translated MMLU evaluations.

**Chiang et al., 2024**
Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael I. Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating LLMs by human preference. *CoRR*, abs/2403.04132, 2024.
- Cited in 01_introduction.md for Chatbot Arena rankings; in 05_evaluation.md as evaluation context.

**Chu et al., 2023**
Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-Audio: Advancing universal audio understanding via unified large-scale audio-language models. *CoRR*, abs/2311.07919, 2023.
- Cited in 01_introduction.md as the audio-language model in the Qwen family.

**Clark et al., 2018**
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? Try ARC, the AI2 reasoning challenge. *CoRR*, abs/1803.05457, 2018.
- Cited in 05_evaluation.md as the ARC-C benchmark.

**Cobbe et al., 2021**
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. *CoRR*, abs/2110.14168, 2021.
- Cited in 01_introduction.md and 05_evaluation.md as the GSM8K benchmark.

**Dai et al., 2024**
Damai Dai, Chengqi Deng, Chenggang Zhao, et al. DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts language models. *CoRR*, abs/2401.06066, 2024.
- Cited in 02_tokenizer-and-model.md for fine-grained experts and shared/routing-specific experts design.

**Dauphin et al., 2017**
Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In *ICML*, volume 70 of *Proceedings of Machine Learning Research*, pp. 933–941. PMLR, 2017.
- Cited in 02_tokenizer-and-model.md for SwiGLU activation.

**Dong et al., 2023**
Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. How abilities in large language models are affected by supervised fine-tuning data composition. *CoRR*, abs/2310.05492, 2023.
- Cited in 04_post-training.md for instruction selection methodology.

**Dong et al., 2024**
Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren Zhou. Self-play with execution feedback: Improving instruction-following capabilities of large language models. *CoRR*, abs/2406.13542, 2024.
- Cited in 04_post-training.md for execution feedback in coding tasks.

**Fenogenova et al., 2024**
Alena Fenogenova, Artem Chervyakov, Nikita Martynov, et al. MERA: A comprehensive LLM evaluation in Russian. *CoRR*, abs/2401.04531, 2024.
- Cited in 05_evaluation.md as the ruMMLU benchmark.

**Golchin & Surdeanu, 2024**
Shahriar Golchin and Mihai Surdeanu. Time travel in llms: Tracing data contamination in large language models. In *ICLR*. OpenReview.net, 2024.
- Cited in 05a_instruction-tuned-model.md as a reference on contamination analysis methodology.

**Goyal et al., 2022**
Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc'Aurelio Ranzato, Francisco Guzman, and Angela Fan. The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. *Trans. Assoc. Comput. Linguistics*, 10:522–538, 2022.
- Cited in 05_evaluation.md for MGSM and Flores-101 multilingual benchmarks.

**Hendrycks et al., 2021a**
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In *ICLR*. OpenReview.net, 2021a.
- Cited in 01_introduction.md and 05_evaluation.md as the MMLU benchmark.

**Hendrycks et al., 2021b**
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In *NeurIPS Datasets and Benchmarks*, 2021b.
- Cited in 05_evaluation.md as the MATH benchmark.

**Huang et al., 2023**
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. In *NeurIPS*, 2023.
- Cited in 05_evaluation.md as the C-Eval Chinese benchmark.

**Jain et al., 2024**
Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. LiveCodeBench: Holistic and contamination free evaluation of large language models for code. *CoRR*, abs/2403.07974, 2024.
- Cited in 01_introduction.md and 05a_instruction-tuned-model.md as the LiveCodeBench benchmark.

**Jiang et al., 2023a**
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7B. *CoRR*, abs/2310.06825, 2023a.
- Cited in 01_introduction.md and 02_tokenizer-and-model.md as Mistral baseline; in 05_evaluation.md as 7B comparison.

**Jiang et al., 2023b**
Zixuan Jiang, Jiaqi Gu, Hanqing Zhu, and David Z. Pan. Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and efficient pre-LN Transformers. *CoRR*, abs/2305.14858, 2023b.
- Cited in 02_tokenizer-and-model.md for RMSNorm and pre-normalization.

**Jiang et al., 2024**
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, et al. Mixtral of experts. *CoRR*, abs/2401.04088, 2024.
- Cited in 02_tokenizer-and-model.md for Mixtral architecture comparison; in 05_evaluation.md as MoE baseline.

**Kamradt, 2023**
Gregory Kamradt. Needle in a haystack - pressure testing LLMs, 2023. URL https://github.com/gkamradt/LLMTest_NeedleInAHaystack.
- Cited in 05a_instruction-tuned-model.md as the Needle in a Haystack evaluation method.

**Komatsuzaki et al., 2023**
Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. Sparse upcycling: Training mixture-of-experts from dense checkpoints. In *ICLR*. OpenReview.net, 2023.
- Cited in 02_tokenizer-and-model.md for expert initialization via upcycling.

**Koto et al., 2023**
Fajri Koto, Nurul Aisyah, Haonan Li, and Timothy Baldwin. Large language models only pass primary school exams in Indonesia: A comprehensive test on IndoMMLU. In *EMNLP*, pp. 12359–12374. Association for Computational Linguistics, 2023.
- Cited in 05_evaluation.md as the IndoMMLU benchmark.

**Li et al., 2023**
Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. *CoRR*, abs/2306.09212, 2023.
- Cited in 05_evaluation.md as the CMMLU Chinese benchmark.

**Li et al., 2024**
Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-Hard and BenchBuilder pipeline. *CoRR*, abs/2406.11939, 2024.
- Cited in 05a_instruction-tuned-model.md as the Arena-Hard benchmark.

**Lieber et al., 2024**
Opher Lieber, Barak Lenz, Hofit Bata, et al. Jamba: A hybrid Transformer-Mamba language model. *CoRR*, abs/2403.19887, 2024.
- Cited in 05_evaluation.md as a baseline MoE model in Table 3.

**Lin et al., 2022a**
Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In *ACL (1)*, pp. 3214–3252. Association for Computational Linguistics, 2022a.
- Cited in 05_evaluation.md as the TruthfulQA benchmark.

**Lin et al., 2022b**
Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, et al. Few-shot learning with multilingual generative language models. In *EMNLP*, pp. 9019–9052. Association for Computational Linguistics, 2022b.
- Cited in 05_evaluation.md for XStoryCloze multilingual benchmark.

**Liu et al., 2023a**
Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by ChatGPT really correct? Rigorous evaluation of large language models for code generation. In *NeurIPS*, 2023a.
- Cited in 05_evaluation.md as the EvalPlus benchmark.

**Liu et al., 2023b**
Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun, Hongning Wang, Jing Zhang, Minlie Huang, Yuxiao Dong, and Jie Tang. AlignBench: Benchmarking Chinese alignment of large language models. *CoRR*, abs/2311.18743, 2023b.
- Cited in 05a_instruction-tuned-model.md as the AlignBench benchmark.

**Lu et al., 2024a**
Keming Lu, Bowen Yu, Fei Huang, Yang Fan, Runji Lin, and Chang Zhou. Online merging optimizers for boosting rewards and mitigating tax in alignment. *CoRR*, abs/2405.17931, 2024a.
- Cited in 04_post-training.md for the Online Merging Optimizer to mitigate alignment tax.

**Lu et al., 2024b**
Keming Lu, Bowen Yu, Chang Zhou, and Jingren Zhou. Large language models are superpositions of all characters: Attaining arbitrary role-play via self-alignment. *CoRR*, abs/2401.12474, 2024b.
- Cited in 04_post-training.md for role-play data repurposing.

**Lu et al., 2024c**
Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. #InsTag: Instruction tagging for analyzing supervised fine-tuning of large language models. In *ICLR*. OpenReview.net, 2024c.
- Cited in 04_post-training.md for InsTag, the open-set fine-grained tagger for ontology extraction.

**Mesnard et al., 2024**
Thomas Mesnard, Cassidy Hardin, Robert Dadashi, et al. Gemma: Open models based on Gemini research and technology. *CoRR*, abs/2403.08295, 2024.
- Cited in 01_introduction.md as a competitive open-weight LLM; in 05_evaluation.md as 7B and 2B baselines.

**Muennighoff et al., 2023**
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, et al. Crosslingual generalization through multitask finetuning. In *ACL (1)*, pp. 15991–16111. Association for Computational Linguistics, 2023.
- Cited in 05_evaluation.md for XWinograd multilingual benchmark.

**Ni et al., 2024**
Jinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng, Mahir Shah, Kabir Jain, Graham Neubig, and Yang You. MixEval: Deriving wisdom of the crowd from LLM benchmark mixtures. *CoRR*, abs/2406.06565, 2024.
- Cited in 05a_instruction-tuned-model.md as the MixEval benchmark.

**OpenAI, 2022**
OpenAI. Introducing ChatGPT, 2022. URL https://openai.com/index/chatgpt/.
- Cited in 01_introduction.md as the catalyst for global enthusiasm for LLMs.

**OpenAI, 2023**
OpenAI. GPT4 technical report. *arXiv preprint arXiv:2303.08774*, 2023.
- Cited in 05a_instruction-tuned-model.md for the contamination analysis methodology (strict non-contaminated test set).

**OpenAI, 2024**
OpenAI. Hello GPT-4o, 2024. URL https://openai.com/index/hello-gpt-4o/.
- Cited in 01_introduction.md as a pinnacle proprietary model.

**OpenCompass Contributors, 2023**
OpenCompass Contributors. OpenCompass: A universal evaluation platform for foundation models, 2023. URL https://github.com/open-compass/opencompass.
- Cited in 05a_instruction-tuned-model.md for the NeedleBench evaluation.

**Peng et al., 2023**
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window extension of large language models. *CoRR*, abs/2309.00071, 2023.
- Cited in 02_tokenizer-and-model.md and 03_pre-training.md for YARN context extension; in 05a_instruction-tuned-model.md for NIAH evaluation.

**Ponti et al., 2020**
Edoardo Maria Ponti, Goran Glavas, Olga Majewska, Qianchu Liu, Ivan Vulic, and Anna Korhonen. XCOPA: A multilingual dataset for causal commonsense reasoning. In *EMNLP (1)*, pp. 2362–2376. Association for Computational Linguistics, 2020.
- Cited in 05_evaluation.md as the XCOPA multilingual benchmark.

**Qwen Team, 2024a**
Qwen Team. Introducing Qwen1.5, 2024a. URL https://qwenlm.github.io/blog/qwen1.5/.
- Cited in 01_introduction.md, 03_pre-training.md, 05_evaluation.md as the predecessor Qwen1.5 model.

**Qwen Team, 2024b**
Qwen Team. Qwen1.5-110B: The first 100B+ model of the Qwen1.5 series, 2024b. URL https://qwenlm.github.io/blog/qwen1.5-110b/.
- Cited in 05_evaluation.md as the Qwen1.5-110B baseline.

**Qwen Team, 2024c**
Qwen Team. Qwen1.5-MoE: Matching 7B model performance with 1/3 activated parameters, 2024c. URL https://qwenlm.github.io/blog/qwen-moe/.
- Cited in 02_tokenizer-and-model.md as the predecessor MoE architecture.

**Rafailov et al., 2023**
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In *NeurIPS*, 2023.
- Cited in 01_introduction.md and 04_post-training.md for DPO alignment method.

**Rajbhandari et al., 2022**
Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. DeepSpeed-MoE: Advancing mixture-of-experts inference and training to power next-generation AI scale. In *ICML*, volume 162 of *Proceedings of Machine Learning Research*, pp. 18332–18346. PMLR, 2022.
- Cited in 02_tokenizer-and-model.md for shared and routing-specific experts design.

**Ravaut et al., 2024**
Mathieu Ravaut, Bosheng Ding, Fangkai Jiao, Hailin Chen, Xingxuan Li, Ruochen Zhao, Chengwei Qin, Caiming Xiong, and Shafiq Joty. How much are LLMs contaminated? A comprehensive survey and the llmsanitize library. *CoRR*, abs/2404.00699, 2024.
- Cited in 05a_instruction-tuned-model.md as a reference on contamination analysis.

**Rein et al., 2023**
David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: A graduate-level Google-proof Q&A benchmark. *CoRR*, abs/2311.12022, 2023.
- Cited in 01_introduction.md and 05_evaluation.md as the GPQA benchmark.

**Sainz et al., 2023**
Oscar Sainz, Jon Ander Campos, Iker Garcia-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre. NLP evaluation in trouble: On the need to measure LLM data contamination for each benchmark. In *EMNLP (Findings)*, pp. 10776–10787. Association for Computational Linguistics, 2023.
- Cited in 05a_instruction-tuned-model.md as a reference on contamination analysis.

**Sakaguchi et al., 2021**
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An adversarial winograd schema challenge at scale. *Commun. ACM*, 64(9):99–106, 2021.
- Cited in 05_evaluation.md as the Winogrande benchmark.

**Su, 2023**
Jianlin Su. The magical effect of the Bias term: RoPE + Bias = better length extrapolation, 2023. URL https://spaces.ac.cn/archives/9577.
- Cited in 02_tokenizer-and-model.md for QKV bias in attention.

**Su et al., 2024**
Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced Transformer with rotary position embedding. *Neurocomputing*, 568:127063, 2024.
- Cited in 02_tokenizer-and-model.md for Rotary Positional Embeddings (RoPE).

**Suzgun et al., 2023**
Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging BIG-Bench tasks and whether chain-of-thought can solve them. In *ACL (Findings)*, pp. 13003–13051. Association for Computational Linguistics, 2023.
- Cited in 01_introduction.md and 05_evaluation.md as the BBH benchmark.

**Touvron et al., 2023**
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models. *CoRR*, abs/2302.13971, 2023.
- Cited in 01_introduction.md as the Llama series that ignited the open-source community.

**Vaswani et al., 2017**
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In *NIPS*, pp. 5998–6008, 2017.
- Cited in 01_introduction.md and 02_tokenizer-and-model.md as the foundational Transformer architecture.

**Wang et al., 2024**
Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. MMLU-Pro: A more robust and challenging multi-task language understanding benchmark. *CoRR*, abs/2406.01574, 2024.
- Cited in 05_evaluation.md as the MMLU-Pro benchmark.

**Xiong et al., 2023**
Wenhan Xiong, Jingyu Liu, Igor Molybog, et al. Effective long-context scaling of foundation models. *CoRR*, abs/2309.16039, 2023.
- Cited in 03_pre-training.md for modifying the base frequency of RoPE for long-context.

**Yang et al., 2019**
Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. PAWS-X: A cross-lingual adversarial dataset for paraphrase identification. In *EMNLP/IJCNLP (1)*, pp. 3685–3690. Association for Computational Linguistics, 2019.
- Cited in 05_evaluation.md as the PAWS-X multilingual benchmark.

**Young et al., 2024**
Alex Young, Bei Chen, Chao Li, et al. Yi: Open foundation models by 01.AI. *CoRR*, abs/2403.04652, 2024.
- Cited in 05_evaluation.md as baseline in Table 3 (Yi-1.5-34B).

**Yuan et al., 2023**
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling relationship on learning mathematical reasoning with large language models. *CoRR*, abs/2308.01825, 2023.
- Cited in 04_post-training.md for rejection sampling methodology.

**Yuan et al., 2024**
Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang. LV-Eval: A balanced long-context benchmark with 5 length levels up to 256K. *CoRR*, abs/2402.05136, 2024.
- Cited in 05a_instruction-tuned-model.md as the LV-Eval long-context benchmark.

**Zellers et al., 2019**
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? In *ACL (1)*, pp. 4791–4800. Association for Computational Linguistics, 2019.
- Cited in 05_evaluation.md as the HellaSwag benchmark.

**Zeng et al., 2024**
Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, et al. ChatGLM: A family of large language models from GLM-130B to GLM-4 all tools. *CoRR*, abs/2406.12793, 2024.
- Cited in 05a_instruction-tuned-model.md as the ChatGLM4-9B-1M baseline for long-context evaluation.

**Zhao et al., 2024**
Yingxiu Zhao, Bowen Yu, Binyuan Hui, Haiyang Yu, Minghao Li, Fei Huang, Nevin L. Zhang, and Yongbin Li. Tree-Instruct: A preliminary study of the intrinsic relationship between complexity and alignment. In *LREC/COLING*, pp. 16776–16789. ELRA and ICCL, 2024.
- Cited in 04_post-training.md for the self-evolution strategy for instruction evolution.

**Zheng et al., 2023**
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. In *NeurIPS*, 2023.
- Cited in 01_introduction.md and 05a_instruction-tuned-model.md as the MT-Bench benchmark.

**Zhou et al., 2023**
Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. *CoRR*, abs/2311.07911, 2023.
- Cited in 05a_instruction-tuned-model.md as the IFEval benchmark.
