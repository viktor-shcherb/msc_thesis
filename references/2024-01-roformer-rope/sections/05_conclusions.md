# Conclusions [p. 12]

[p. 12] The authors proposed a new position embedding method that incorporates explicit relative position dependency in self-attention to enhance the performance of transformer architectures. The theoretical analysis indicates that relative position can be naturally formulated using vector production in self-attention, with absolute position information being encoded through a rotation matrix. In addition, the authors mathematically illustrated the advantageous properties of the proposed method when applied to the Transformer. Experiments on both English and Chinese benchmark datasets demonstrate that the method encourages faster convergence in pre-training. The experimental results also show that the proposed RoFormer can achieve better performance on long texts task.
