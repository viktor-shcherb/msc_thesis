# RoFormer: Enhanced Transformer with Rotary Position Embedding

**Authors:** Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu
**Affiliation:** Zhuiyi Technology Co., Ltd., Shenzhen, China

## Publication Status

- **arXiv preprint:** April 2021, arXiv:2104.09864 (v5 November 2023)
- **Peer-reviewed:** Yes
- **Journal:** Neurocomputing, Volume 568, Article 127063, January 2024
- **Status:** Published journal paper

## Preferred Citation

Cite the Neurocomputing 2024 version:

> Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., & Liu, Y. (2024). RoFormer: Enhanced Transformer with Rotary Position Embedding. Neurocomputing, 568, 127063.

## Notes

The arXiv preprint went through five revisions between April 2021 and November 2023. The paper is widely cited under varying years (2021, 2022, 2023, 2024) depending on which version or the journal publication authors reference. Despite modest experimental results in the original paper, RoPE became the dominant positional encoding in modern LLMs (LLaMA, Falcon, Pythia, Qwen, Mistral, Gemma) and has over 3,800 citations as of early 2026.

## Links

- arXiv: https://arxiv.org/abs/2104.09864
- Journal: https://doi.org/10.1016/j.neucom.2023.127063
- Code: https://github.com/ZhuiyiTechnology/roformer
- Hugging Face: https://huggingface.co/docs/transformers/model_doc/roformer
