# Appendix E: GSM-8K in GPT-4 Training [p. 29]

[p. 29] To improve GPT-4's ability to do mathematical reasoning, data from the training set of MATH and GSM-8K -- two commonly studied benchmarks for mathematical reasoning in language models -- was mixed into the training data. The total number of tokens drawn from these math benchmarks was a tiny fraction of the overall GPT-4 training budget. When mixing in data from these math benchmarks, a portion of the training data was held back, so each individual training example may or may not have been seen by GPT-4 during training. [p. 29]

Contamination checking was conducted to verify the test set for GSM-8K is not included in the training set (see Appendix D). The authors recommend interpreting the performance results reported for GPT-4 GSM-8K in Table 2 as something in-between true few-shot transfer and full benchmark-specific tuning. [p. 29]
