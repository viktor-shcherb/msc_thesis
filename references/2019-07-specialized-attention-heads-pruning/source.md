# Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned

**Authors:** Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov
**Affiliations:** Elena Voita (Yandex, Russia; University of Amsterdam, Netherlands), David Talbot (Yandex, Russia), Fedor Moiseev (Yandex, Russia; Moscow Institute of Physics and Technology, Russia), Rico Sennrich (University of Edinburgh, Scotland; University of Zurich, Switzerland), Ivan Titov (University of Edinburgh, Scotland; University of Amsterdam, Netherlands)

## Publication Status

- **arXiv preprint:** May 2019, arXiv:1905.09418
- **Peer-reviewed:** Yes
- **Conference:** Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019), pages 5797--5808, Florence, Italy, July 28 -- August 2, 2019
- **Status:** Published conference paper

## Preferred Citation

Cite the ACL 2019 version:

> Voita, E., Talbot, D., Moiseev, F., Sennrich, R., & Titov, I. (2019). Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5797--5808, Florence, Italy. Association for Computational Linguistics.

## Links

- arXiv: https://arxiv.org/abs/1905.09418
- ACL Anthology: https://aclanthology.org/P19-1580/
- Code: https://github.com/lena-voita/the-story-of-heads
