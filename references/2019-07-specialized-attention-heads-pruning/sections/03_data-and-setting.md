# 3 Data and Setting [p. 3]

The paper focuses on English as a source language with three target languages: Russian, German and French. [p. 3]

For each language pair, the same number of sentence pairs from WMT data is used to control for the amount of training data. Transformer models are trained with the same numbers of parameters. They use 2.5m sentence pairs, corresponding to the amount of English-Russian parallel training data (excluding UN and Paracrawl). [p. 3]

In Section 5.2, the same held-out data is used for all language pairs: 50k English sentences taken from the WMT EN-FR data not used in training. [p. 3]

For English-Russian, additional experiments are performed using the publicly available OpenSubtitles2018 corpus (Lison et al., 2018) to evaluate the impact of domains on results. [p. 3]

In Section 6, the paper concentrates on English-Russian and two domains: WMT and OpenSubtitles. [p. 3]

Model hyperparameters, preprocessing and training details are provided in appendix B. [p. 3]
