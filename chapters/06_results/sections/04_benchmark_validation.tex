\section{Benchmark Validation}
\label{sec:benchmark-validation}

This section addresses RQ3: \emph{do plasticity profiles correspond to
behavioral long-context performance?} We correlate plasticity metrics with LongBench-Pro
aggregate scores across 7 matched models and with per-length data where
available.

\subsection{Plasticity Drop Predicts LongBench-Pro Ordering}

Table~\ref{tab:lbp-mechanistic} presents the 7 models with both mechanistic
metrics and LongBench-Pro scores. The key finding is that
$\text{AP}_{\text{drop}}$---the plasticity degradation from early to late
context---separates model families in the same order as benchmark performance.

\begin{table}[t]
\centering
\small
\caption{Mechanistic metrics and LongBench-Pro scores for 7 matched models,
sorted by LBP score. $\text{AP}_{\text{drop}}$ separates families: Ministral
($\sim$0.07), Qwen ($\sim$0.17), Llama (0.23), matching the LBP ordering.
Non-thinking mode scores used throughout.}
\label{tab:lbp-mechanistic}
\begin{tabular}{@{}llrrrrr@{}}
\toprule
Model & Family & LBP & AP$_{\text{overall}}$ & AP$_{\text{first\,20\%}}$ &
  AP$_{\text{last\,20\%}}$ & AP$_{\text{drop}}$ \\
\midrule
Ministral-3-14B & Ministral & 40.14 & 0.615 & 0.655 & 0.587 & 0.068 \\
Ministral-3-8B  & Ministral & 37.80 & 0.608 & 0.654 & 0.571 & 0.083 \\
Qwen-3-14B      & Qwen      & 37.11 & 0.590 & 0.680 & 0.519 & 0.161 \\
Qwen-3-8B       & Qwen      & 33.41 & 0.597 & 0.694 & 0.517 & 0.177 \\
Qwen-3-4B       & Qwen      & 31.26 & 0.594 & 0.699 & 0.512 & 0.187 \\
Ministral-3-3B  & Ministral & 30.18 & 0.622 & 0.664 & 0.592 & 0.072 \\
Llama-3.2-3B    & Llama     & 15.71 & 0.565 & 0.686 & 0.456 & 0.230 \\
\bottomrule
\end{tabular}
\end{table}

Across families, $\text{AP}_{\text{drop}}$ separates Ministral ($\sim$0.07) from
Qwen ($\sim$0.17) from Llama (0.23), matching the LBP ordering. Within the Qwen
family, the relationship is monotonic: 14B ($\text{AP}_{\text{drop}} = 0.161$,
LBP 37.1) outperforms 8B (0.177, 33.4) outperforms 4B (0.187, 31.3)---larger
models degrade less and score higher.

\begin{figure}[t]
\centering
\includegraphics[width=0.75\textwidth]{figures/attention-plasticity/10_ap_drop_vs_lbp/figure.pdf}
\caption{$\text{AP}_{\text{drop}}$ vs.\ LongBench-Pro overall score for 7
matched models. Lower plasticity drop (flatter profiles) associates with
higher benchmark scores. Ministral-3-3B (circled) sits off the trend:
low $\text{AP}_{\text{drop}}$ but low LBP, suggesting its bottleneck is
base capability rather than context preservation.}
\label{fig:apdrop-vs-lbp}
\end{figure}

$\text{AP}_{\text{last\,20\%}}$ also separates models: Llama-3.2-3B at 0.456
(last) maps to LBP 15.71 (last), while Ministral models maintain 0.57--0.59 at
distant positions, corresponding to LBP scores of 30--40.

\subsection{Base Capability vs.\ Context Preservation}

Ministral-3-3B is the diagnostic outlier in Figure~\ref{fig:apdrop-vs-lbp}: it
has the lowest $\text{AP}_{\text{drop}}$ (0.072, comparable to the 14B variant)
and the highest aggregate plasticity (0.622) among all 7 models, yet scores
only 30.18 on LBP. Its flat plasticity profile indicates strong context
preservation, but the low benchmark score reflects limited base capability at 3B
scale.

This dissociation clarifies what plasticity measures and what it does not.
Benchmark performance depends on both base model capability (knowledge,
reasoning ability) and context preservation (maintaining attention flexibility
at distance). Plasticity metrics capture the second factor.
$\text{AP}_{\text{drop}}$ isolates context preservation by measuring the
\emph{slope} of plasticity decline, which is robust to the absolute level of
model capability. Within a family (controlled base architecture),
$\text{AP}_{\text{drop}}$ and LBP move together. Across families, the base
capability confound must be accounted for.

\subsection{Per-Length Correspondence}

For Ministral-3-14B---the one model with published per-length LBP
scores~\cite{2026-01-longbench-pro}---plasticity at each length bin tracks
benchmark performance (Table~\ref{tab:ministral-per-length}).

\begin{table}[t]
\centering
\small
\caption{Ministral-3-14B per-length LongBench-Pro scores and matched
plasticity values. Both decline with length, though plasticity decline
(12.9\%) is shallower than LBP decline (18.4\%), reflecting
length-independent task difficulty.}
\label{tab:ministral-per-length}
\begin{tabular}{@{}lrrrrrr@{}}
\toprule
 & 8K & 16K & 32K & 64K & 128K & 256K \\
\midrule
LBP score   & 51.88 & 48.52 & 48.75 & 45.70 & 42.36 & 37.59 \\
Plasticity  & 0.684 & 0.660 & 0.639 & 0.615 & 0.596 & --- \\
\bottomrule
\end{tabular}
\end{table}

Both metrics decline with length, but plasticity decline (12.9\% from 8K to
128K) is shallower than LBP decline (18.4\%). The gap reflects
length-independent task difficulty: some benchmark tasks are harder at longer
lengths for reasons unrelated to attention flexibility (e.g., more candidate
answers, higher reasoning complexity).

SmolLM3-3B at the end of long-context training
($\text{AP}_{\text{drop}} \approx 0.16$) matches Qwen-3 models
(0.16--0.19)---both are standard LC-trained models of comparable scale.
Ministral-3 achieves $\text{AP}_{\text{drop}} \approx 0.07$ at comparable bias
levels over a longer context window, suggesting that its training recipe or
architecture may address factors beyond positional bias reduction.
