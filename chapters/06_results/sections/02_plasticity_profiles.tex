\section{Plasticity Profiles}
\label{sec:plasticity-profiles}

This section addresses RQ2: \emph{does positional bias functionally constrain
which keys the model attends to?} Attention plasticity
(Section~\ref{sec:attention-plasticity}) measures whether the query content
determines key selection or whether position dominates. All models show
plasticity decline with context position, but the \emph{rate} of decline
separates model families.

\subsection{Plasticity Declines with Position}

Table~\ref{tab:quintile-plasticity} reports the quintile plasticity profile for
all 11 primary models. Every model shows declining plasticity across context,
but the profiles differ qualitatively.

\begin{table}[t]
\centering
\small
\caption{Quintile plasticity profiles for 11 primary models. Each column shows
mean attention plasticity for query positions in that quintile of the context
window. All models decline from left to right; the rate of decline varies by
family.}
\label{tab:quintile-plasticity}
\begin{tabular}{@{}lcccccc@{}}
\toprule
Model & 0--20\% & 20--40\% & 40--60\% & 60--80\% & 80--100\% & AP$_{\text{drop}}$ \\
\midrule
\multicolumn{7}{@{}l}{\emph{Ministral-3}} \\
\quad 3B   & 0.664 & 0.633 & 0.615 & 0.606 & 0.592 & 0.072 \\
\quad 8B   & 0.654 & 0.624 & 0.602 & 0.588 & 0.571 & 0.083 \\
\quad 14B  & 0.655 & 0.620 & 0.607 & 0.602 & 0.587 & 0.068 \\
\addlinespace
\multicolumn{7}{@{}l}{\emph{Qwen-3}} \\
\quad 0.6B & 0.695 & 0.628 & 0.592 & 0.567 & 0.526 & 0.169 \\
\quad 1.7B & 0.702 & 0.640 & 0.603 & 0.579 & 0.540 & 0.162 \\
\quad 4B   & 0.699 & 0.628 & 0.581 & 0.540 & 0.512 & 0.187 \\
\quad 8B   & 0.694 & 0.630 & 0.585 & 0.545 & 0.517 & 0.177 \\
\quad 14B  & 0.680 & 0.615 & 0.576 & 0.548 & 0.519 & 0.161 \\
\addlinespace
\multicolumn{7}{@{}l}{\emph{Llama-3.2}} \\
\quad 1B   & 0.703 & 0.653 & 0.631 & 0.527 & 0.487 & 0.216 \\
\quad 3B   & 0.686 & 0.621 & 0.561 & 0.489 & 0.456 & 0.230 \\
\quad 11B  & 0.658 & 0.587 & 0.534 & 0.557 & 0.489 & 0.169 \\
\bottomrule
\end{tabular}
\end{table}

Three patterns emerge. Ministral-3 models show gradual, near-linear decline
with the flattest profiles of any family
($\text{AP}_{\text{drop}} \approx 0.07$--$0.08$). Qwen-3 models decline more
steeply ($\text{AP}_{\text{drop}} \approx 0.16$--$0.19$), with acceleration in
the second half of context; larger models are slightly flatter. Llama-3.2
models show the steepest decline overall
($\text{AP}_{\text{drop}} \approx 0.17$--$0.23$), with Llama-3.2-3B dropping
from 0.686 to 0.456---the largest degradation of any model.

Llama-3.2-11B exhibits an anomalous non-monotone profile: plasticity dips at
40--60\%, partially recovers at 60--80\%, then collapses at 80--100\%.
Llama-3.1-8B (a predecessor model) shows the same pattern, consistent with a
Llama-family trait at 32+ layers rather than an artifact of the vision
architecture.

Aggregate plasticity ($\text{AP}_{\text{overall}}$) does \emph{not} predict
performance across families. Ministral-3B has the highest aggregate plasticity
(0.622) among models with LongBench-Pro scores, yet scores only 30.18. The
aggregate confounds attention mechanics with base model capability. The
informative metrics are positional: $\text{AP}_{\text{drop}}$ and
$\text{AP}_{\text{last\,20\%}}$.

\subsection{2D Geometry: Plasticity Depends on Two Distances}

The 1D position profiles in Table~\ref{tab:quintile-plasticity} collapse
essential structure. The 2D bucket heatmaps (Figure~\ref{fig:bucket-heatmaps})
reveal that plasticity is a function of \emph{both} inter-key distance (how far
apart the two competing keys are) and key-to-query distance (how far the keys
are from the query).

\begin{figure}[t]
\centering
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{figures/attention-plasticity/01_bucket_heatmap_ministral3_14b/figure.png}
\caption{Ministral-3-14B}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{figures/attention-plasticity/02_bucket_heatmap_qwen3_14b/figure.png}
\caption{Qwen-3-14B}
\end{subfigure}

\vspace{0.5em}
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{figures/attention-plasticity/03_bucket_heatmap_llama3.2_3b/figure.png}
\caption{Llama-3.2-3B}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\textwidth}
\includegraphics[width=\textwidth]{figures/attention-plasticity/04_bucket_heatmap_llama3.2_11b/figure.png}
\caption{Llama-3.2-11B}
\end{subfigure}
\caption{2D plasticity heatmaps for four representative models. Horizontal
axis: inter-key distance $|t_{k_i} - t_{k_j}|$. Vertical axis:
key-to-query distance. Warm colors indicate high plasticity
(content-driven key selection); dark colors indicate low plasticity
(position-dominated). Ministral maintains high plasticity nearly everywhere;
Qwen shows a diagonal gradient; Llama-3.2-3B shows the strongest contrast;
Llama-3.2-11B shows non-smooth patches suggesting NoPE layer interference.}
\label{fig:bucket-heatmaps}
\end{figure}

The geometry of the competition follows a consistent pattern: keys close
together but far from the query yield high plasticity (content determines the
winner), while keys far apart with one near the query yield low plasticity
(position dominates). The transition between these regimes is where families
diverge.

Ministral-3 is strikingly uniform: the warm region ($> 0.55$) fills nearly the
entire triangle, with the dark corner confined to extreme inter-key distances
($> 90$K). The 3B, 8B, and 14B heatmaps are nearly identical in structure.
Qwen-3 shows a clear diagonal gradient from warm top-left to dark bottom-right,
with the dark region starting at $\sim$50K inter-key distance. Remarkably, the
pattern is nearly identical across scales from 0.6B to 14B---the architecture
determines the geometry; scale adjusts the level. Llama-3.2-3B shows the most
extreme contrast of any model, with the dark zone starting earlier ($\sim$40K)
and reaching below 0.2. Llama-3.2-11B is uniquely non-smooth, with dark patches
at specific distance combinations and a bright recovery zone around 57K--65K
query-to-key distance.

\subsection{Head Heterogeneity}

Per-head plasticity profiles (Figure~\ref{fig:head-profiles}) reveal different
architectural strategies for distributing content-based and position-based
attention across heads.

\begin{figure}[t]
\centering
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\textwidth]{figures/attention-plasticity/05_profile_ministral3_3b/figure.png}
\caption{Ministral-3-3B}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\textwidth]{figures/attention-plasticity/06_profile_qwen3_4b/figure.png}
\caption{Qwen-3-4B}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.32\textwidth}
\includegraphics[width=\textwidth]{figures/attention-plasticity/07_profile_llama3.2_3b/figure.png}
\caption{Llama-3.2-3B}
\end{subfigure}
\caption{Per-head plasticity profiles (gray) with model mean (red). Three
qualitatively different organizations: Ministral produces a tight, homogeneous
bundle; Qwen shows bimodal specialization with content-focused heads near 1.0
and position-locked heads near 0.2; Llama shows a wide spread that converges at
distant positions---position eventually dominates every head.}
\label{fig:head-profiles}
\end{figure}

Ministral-3-3B produces a tight bundle with low inter-head variance: nearly all
heads decline gently and uniformly. The architecture produces homogeneous
attention behavior where every head contributes similarly to content-based
retrieval. Qwen-3-4B shows a bimodal distribution: content-specialized heads
near 0.8--1.0 coexist with position-locked heads near 0.2--0.3. The model
achieves content retrieval through heterogeneous specialization. Llama-3.2-3B
shows a wide spread that \emph{converges at distance}: beyond 100K tokens, all
heads cluster near 0.45, erasing individual differences. Position bias
eventually dominates every head, even initially plastic ones.
