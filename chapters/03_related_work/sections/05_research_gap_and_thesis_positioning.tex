\section{Research Gap and Thesis Positioning}

Across the literature, behavioral benchmark work and mechanistic analysis are
often developed separately: benchmark papers provide strong evidence of
claimed-versus-effective context gaps, while mechanistic papers isolate specific
failure pathways such as positional bias or fragile retrieval circuits
\cite{2024-10-ruler-context-size,2025-07-nolima-long-context-evaluation,2026-01-longbench-pro,2025-04-attention-sink-emerges,2025-04-retrieval-head-long-context-factuality}.
% Verification (split literature): confirm benchmark-centric framing in RULER,
% NoLiMa, LongBench Pro (2404.06654; 2502.05167; 2601.02872) and mechanism-
% centric framing in Attention Sink and retrieval-head papers (2410.10781;
% 2502.12435).

As a result, no existing work provides a quantitative framework that traces
behavioral long-context degradation to specific geometric properties of
attention head representations and validates this connection across model
families. Benchmark papers measure that models fail; mechanistic papers
identify candidate failure pathways; but the link between internal
geometry and behavioral ECL remains uncharacterized.

This thesis addresses this gap by developing a geometric framework that
extracts positional bias structure and attention plasticity from post-RoPE
query and key vectors, then correlates these mechanistic metrics with
benchmark performance across 13 models from three families. The framework
is observational---it measures associations between internal geometry and
behavioral performance rather than establishing causality through
intervention---but to our knowledge it provides the first systematic connection between
per-head attention geometry and cross-model long-context capability.
