\section{Thesis Outline}

The remainder of this thesis is organized as follows:

\begin{description}
    \item[Chapter~\ref{chap:background}] introduces the foundational concepts:
    effective context length, the transformer attention mechanism, rotary
    positional encoding, and long-context architectural variants.

    \item[Chapter~\ref{chap:related_work}] reviews behavioral evaluation of
    long-context models, mechanistic interpretability of attention patterns,
    context-extension methods, and positions this thesis within the literature.

    \item[Chapter~\ref{chap:methodology}] develops the three-analysis
    framework: Q/K capture protocol, PCA decomposition, the planar rotation
    model, attention plasticity with its formal decay theorem, and the
    connections between analyses.

    \item[Chapter~\ref{chap:experiments}] describes the experimental setup:
    the 13-model cross-model study, the SmolLM3 training dynamics study,
    benchmark selection, and implementation details.

    \item[Chapter~\ref{chap:results}] presents the experimental results across
    models, training checkpoints, and benchmark correlations.

    \item[Chapter~\ref{chap:discussion}] interprets the findings, discusses
    limitations, and suggests directions for future work.

    \item[Chapter~\ref{chap:conclusion}] concludes the thesis.
\end{description}
