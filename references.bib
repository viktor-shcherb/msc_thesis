@string{ICML="International Conference on Machine Learning"}
@string{NeurIPS="Advances in Neural Information Processing Systems"}
@string{ACL="Annual Meeting of the Association for Computational Linguistics"}
@string{EMNLP="Conference on Empirical Methods in Natural Language Processing"}
@string{FindingsEMNLP="Findings of the Association for Computational Linguistics: EMNLP"}
@string{FindingsACL="Findings of the Association for Computational Linguistics: ACL"}
@string{NAACL="Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"}
@string{TMLR="Transactions on Machine Learning Research"}
@string{ICLR="International Conference on Learning Representations"}
@string{COLM="Conference on Language Modeling"}
@string{TACL="Transactions of the Association for Computational Linguistics"}
@string{JMLR="Journal of Machine Learning Research"}
@string{arXiv="arXiv Technical Report"}
@article{2014-10-neural-turing-machines,
  author = {Alex Graves and Greg Wayne and Ivo Danihelka},
  title = {{Neural Turing Machines}},
  journal = arXiv,
  year = {2014},
  month = oct,
  eprint = {1410.5401},
  archivePrefix = {arXiv},
  primaryClass = {cs.NE},
}
@inproceedings{2017-05-moe-sparsely-gated-mixture-experts,
  author = {Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc V. Le and Geoffrey E. Hinton and Jeff Dean},
  title = {{Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}},
  year = {2017},
  booktitle = ICLR,
  keywords = {conference},
}
@inproceedings{2017-12-attention-is-all-you-need,
  author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and {\L}ukasz Kaiser and Illia Polosukhin},
  title = {{Attention Is All You Need}},
  year = {2017},
  booktitle = NeurIPS,
  pages = {5998--6008},
  volume = {30},
  keywords = {conference},
}
@inproceedings{2018-06-prediction-short-memory,
  title     = {Prediction with a Short Memory},
  author    = {Sharan, Vatsal and Kakade, Sham and Liang, Percy and Valiant, Gregory},
  booktitle = {Proceedings of the 50th Annual {ACM} {SIGACT} Symposium on Theory of Computing ({STOC})},
  pages     = {1074--1087},
  year      = {2018},
  doi       = {10.1145/3188745.3188954},
}
@inproceedings{2018-07-sharp-nearby-fuzzy-far-away,
  author = {Urvashi Khandelwal and He He and Peng Qi and Dan Jurafsky},
  title = {{Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context}},
  booktitle = ACL,
  year = {2018},
  pages = {284--294},
  address = {Melbourne, Australia},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/P18-1027},
  url = {https://aclanthology.org/P18-1027/},
  keywords = {published},
}
@techreport{2019-02-gpt-2-language-models-unsupervised,
  author       = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  title        = {Language Models are Unsupervised Multitask Learners},
  institution  = {OpenAI},
  year         = {2019},
  month        = {2},
  url          = {https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
}
@inproceedings{2019-06-bert-pretraining-language-understanding,
  author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
  year = {2019},
  booktitle = NAACL,
  pages = {4171--4186},
  keywords = {conference},
}
@inproceedings{2019-07-specialized-attention-heads-pruning,
  author = {Elena Voita and David Talbot and Fedor Moiseev and Rico Sennrich and Ivan Titov},
  title = {{Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned}},
  year = {2019},
  booktitle = ACL,
  pages = {5797--5808},
  keywords = {conference},
}
@inproceedings{2019-07-transformer-xl,
  author = {Zihang Dai and Zhilin Yang and Yiming Yang and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
  title = {{Transformer-XL}: Attentive Language Models beyond a Fixed-Length Context},
  year = {2019},
  booktitle = ACL,
  pages = {2978--2988},
  keywords = {conference},
}
@inproceedings{2019-08-bert-attention-analysis,
  author = {Kevin Clark and Urvashi Khandelwal and Omer Levy and Christopher D. Manning},
  title = {{What Does BERT Look At? An Analysis of BERT's Attention}},
  year = {2019},
  month = aug,
  booktitle = {Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  pages = {276--286},
  address = {Florence, Italy},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/W19-4828},
  url = {https://aclanthology.org/W19-4828/},
  keywords = {workshop},
}
@inproceedings{2019-11-dark-secrets-of-bert,
  author = {Olga Kovaleva and Alexey Romanov and Anna Rogers and Anna Rumshisky},
  title = {{Revealing the Dark Secrets of BERT}},
  year = {2019},
  booktitle = EMNLP,
  pages = {4365--4374},
  keywords = {conference},
}
@inproceedings{2019-12-sixteen-heads-better-than-one,
  author = {Paul Michel and Omer Levy and Graham Neubig},
  title = {{Are Sixteen Heads Really Better than One?}},
  year = {2019},
  booktitle = NeurIPS,
  volume = {32},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf},
  keywords = {conference},
}
@inproceedings{2020-04-compressive-transformer-pg19,
  author = {Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},
  title = {{Compressive Transformers for Long-Range Sequence Modelling}},
  booktitle = ICLR,
  year = {2020},
  url = {https://openreview.net/forum?id=SylKikSYDH},
}
@misc{2020-04-longformer-long-document-transformer,
  author = {Iz Beltagy and Matthew E. Peters and Arman Cohan},
  title = {{Longformer: The Long-Document Transformer}},
  year = {2020},
  eprint = {2004.05150},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  keywords = {preprint},
}
@inproceedings{2020-07-quantifying-attention-flow,
  author = {Samira Abnar and Willem Zuidema},
  title = {{Quantifying Attention Flow in Transformers}},
  year = {2020},
  booktitle = ACL,
  pages = {4190--4197},
  keywords = {conference},
}
@article{2020-07-theoretical-limitations-self-attention,
  author = {Michael Hahn},
  title = {{Theoretical Limitations of Self-Attention in Neural Sequence Models}},
  year = {2020},
  journal = TACL,
  volume = {8},
  pages = {156--171},
  doi = {10.1162/tacl_a_00306},
  keywords = {journal},
}
@inproceedings{2020-12-bigbird-sparse-attention,
  author = {Manzil Zaheer and Guru Guruganesh and Avinava Dubey and Joshua Ainslie and Chris Alberti and Santiago Ontanon and Philip Pham and Anirudh Ravula and Qifan Wang and Li Yang and Amr Ahmed},
  title = {{Big Bird: Transformers for Longer Sequences}},
  booktitle = NeurIPS,
  year = {2020},
  volume = {33},
  pages = {17283--17297},
  address = {Vancouver, Canada},
}
@inproceedings{2020-12-gpt-3-few-shot-learners,
  author = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  title = {{Language Models are Few-Shot Learners}},
  booktitle = NeurIPS,
  year = {2020},
  pages = {1877--1901},
  volume = {33},
}
@inproceedings{2021-05-gnn-bottleneck-over-squashing,
  author = {Uri Alon and Eran Yahav},
  title = {{On the Bottleneck of Graph Neural Networks and its Practical Implications}},
  year = {2021},
  booktitle = ICLR,
  keywords = {conference},
}
@inproceedings{2021-05-long-range-arena,
  author = {Yi Tay and Mostafa Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and Philip Pham and Jinfeng Rao and Liu Yang and Sebastian Ruder and Donald Metzler},
  title = {{Long Range Arena: A Benchmark for Efficient Transformers}},
  year = {2021},
  booktitle = ICLR,
  keywords = {conference},
}
@inproceedings{2021-08-context-features-transformer-lm,
  author = {Joe O'Connor and Jacob Andreas},
  title = {{What Context Features Can Transformer Language Models Use?}},
  booktitle = ACL,
  year = {2021},
  month = aug,
  pages = {851--864},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.acl-long.70},
  url = {https://aclanthology.org/2021.acl-long.70/},
  eprint = {2106.08367},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
}
@inproceedings{2021-11-ff-layers-key-value-memories,
  author = {Mor Geva and Roei Schuster and Jonathan Berant and Omer Levy},
  title = {{Transformer Feed-Forward Layers Are Key-Value Memories}},
  year = {2021},
  month = nov,
  booktitle = EMNLP,
  pages = {5484--5495},
  publisher = {Association for Computational Linguistics},
  address = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.446},
  url = {https://aclanthology.org/2021.emnlp-main.446/},
  eprint = {2012.14913},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  keywords = {feed-forward layers, key-value memories, transformer, interpretability},
}
@inproceedings{2021-11-long-range-models-use-context,
  author = {Simeng Sun and Kalpesh Krishna and Andrew Mattarella-Micke and Mohit Iyyer},
  title = {{Do Long-Range Language Models Actually Use Long-Range Context?}},
  year = {2021},
  month = nov,
  booktitle = EMNLP,
  pages = {807--822},
  publisher = {Association for Computational Linguistics},
  address = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.62},
  url = {https://aclanthology.org/2021.emnlp-main.62/},
  eprint = {2109.09115},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  keywords = {long-range context, transformer, language models, attention analysis, PG-19},
}
@misc{2021-12-transformer-circuits-framework,
  author = {Nelson Elhage and Neel Nanda and Catherine Olsson and Tom Henighan and Nicholas Joseph and Ben Mann and Amanda Askell and Yuntao Bai and Anna Chen and Tom Conerly and Nova DasSarma and Dawn Drain and Deep Ganguli and Zac Hatfield-Dodds and Danny Hernandez and Andy Jones and Jackson Kernion and Liane Lovitt and Kamal Ndousse and Dario Amodei and Tom Brown and Jack Clark and Jared Kaplan and Sam McCandlish and Chris Olah},
  title = {{A Mathematical Framework for Transformer Circuits}},
  year = {2021},
  month = dec,
  howpublished = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2021/framework/index.html},
  keywords = {informal},
}
@misc{2022-03-in-context-learning-induction-heads,
  author = {Catherine Olsson and Nelson Elhage and Neel Nanda and Nicholas Joseph and Nova DasSarma and Tom Henighan and Ben Mann and Amanda Askell and Yuntao Bai and Anna Chen and Tom Conerly and Dawn Drain and Deep Ganguli and Zac Hatfield-Dodds and Danny Hernandez and Scott Johnston and Andy Jones and Jackson Kernion and Liane Lovitt and Kamal Ndousse and Dario Amodei and Tom Brown and Jack Clark and Jared Kaplan and Sam McCandlish and Chris Olah},
  title = {{In-context Learning and Induction Heads}},
  year = {2022},
  month = mar,
  howpublished = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html},
  eprint = {2209.11895},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  keywords = {induction heads, in-context learning, mechanistic interpretability, attention heads, transformers},
}
@inproceedings{2022-04-alibi-train-short-test-long,
  author = {Ofir Press and Noah A. Smith and Mike Lewis},
  title = {{Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation}},
  year = {2022},
  month = apr,
  booktitle = ICLR,
  url = {https://openreview.net/forum?id=R8sQPpGCv0},
  eprint = {2108.12409},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  keywords = {positional encoding, length extrapolation, attention, transformer},
}
@inproceedings{2022-04-s4-structured-state-spaces,
  author       = {Albert Gu and Karan Goel and Christopher R{\'{e}}},
  title        = {{Efficiently Modeling Long Sequences with Structured State Spaces}},
  year         = {2022},
  month        = apr,
  booktitle    = ICLR,
  url          = {https://openreview.net/forum?id=uYLFoz1vlAC},
  eprint       = {2111.00396},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  keywords     = {state space models, sequence modeling, long-range dependencies, S4},
}
@article{2022-06-switch-transformers-moe,
  author       = {William Fedus and Barret Zoph and Noam Shazeer},
  title        = {{Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity}},
  journal      = JMLR,
  year         = {2022},
  month        = apr,
  volume       = {23},
  number       = {120},
  pages        = {1--39},
  url          = {http://jmlr.org/papers/v23/21-0998.html},
  eprint       = {2101.03961},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  keywords     = {mixture-of-experts, natural language processing, sparsity, large-scale machine learning, distributed computing},
}
@inproceedings{2022-12-chain-of-thought-prompting,
  author = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed H. Chi and Quoc V. Le and Denny Zhou},
  title = {{Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}},
  year = {2022},
  month = dec,
  booktitle = NeurIPS,
  volume = {35},
  pages = {24824--24837},
  publisher = {Curran Associates, Inc.},
  address = {New Orleans, LA, USA},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html},
  eprint = {2201.11903},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  keywords = {chain-of-thought, prompting, reasoning, large language models},
}
@inproceedings{2022-12-chinchilla-scaling-laws,
  author = {Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
  title = {{Training Compute-Optimal Large Language Models}},
  year = {2022},
  month = dec,
  booktitle = NeurIPS,
  volume = {35},
  pages = {30016--30030},
  publisher = {Curran Associates, Inc.},
  address = {New Orleans, LA, USA},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/c1e2faff6f588870935f114ebe04a3e5-Abstract-Conference.html},
  eprint = {2203.15556},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  keywords = {scaling laws, compute-optimal training, language models, chinchilla},
}
@inproceedings{2022-12-flashattention,
  author = {Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher R\'{e}},
  title = {{FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}},
  year = {2022},
  booktitle = NeurIPS,
  keywords = {conference},
}
@inproceedings{2022-12-nope-transformers-learn-positions,
    title = "Transformer Language Models without Positional Encodings Still Learn Positional Information",
    author = "Haviv, Adi and Ram, Ori and Press, Ofir and Izsak, Peter and Levy, Omer",
    booktitle = FindingsEMNLP,
    year = "2022",
    pages = "1382--1390",
    url = "https://aclanthology.org/2022.findings-emnlp.99/",
    doi = "10.18653/v1/2022.findings-emnlp.99",
}
@inproceedings{2022-12-scrolls-long-language-sequences,
  author = {Uri Shaham and Elad Segal and Maor Ivgi and Avia Efrat and Ori Yoran and Adi Haviv and Ankit Gupta and Wenhan Xiong and Mor Geva and Jonathan Berant and Omer Levy},
  title = {{SCROLLS: Standardized CompaRison Over Long Language Sequences}},
  year = {2022},
  booktitle = EMNLP,
  pages = {12007--12021},
  keywords = {conference},
}
@misc{2023-02-llama-open-efficient-foundation,
  author = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth\'{e}e Lacroix and Baptiste Rozi\`{e}re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  title = {{LLaMA: Open and Efficient Foundation Language Models}},
  year = {2023},
  eprint = {2302.13971},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  keywords = {preprint},
}
@misc{2023-03-gpt-4-technical-report,
  author = {OpenAI},
  title = {{GPT-4 Technical Report}},
  year = {2023},
  eprint = {2303.08774},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  keywords = {preprint},
}
@misc{2023-06-pi-positional-interpolation,
  author = {Shouyuan Chen and Sherman Wong and Liangjian Chen and Yuandong Tian},
  title = {{Extending Context Window of Large Language Models via Position Interpolation}},
  year = {2023},
  eprint = {2306.15595},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  keywords = {preprint},
}
@misc{2023-06-rope-ntk,
  author = {bloc97},
  title = {{NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation}},
  year = {2023},
  howpublished = {Reddit post, r/LocalLLaMA},
  url = {https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/},
  keywords = {informal},
}
@inproceedings{2023-07-gsm-ic-irrelevant-context,
  author = {Freda Shi and Xinyun Chen and Kanishka Misra and Nathan Scales and David Dohan and Ed H. Chi and Nathanael Sch\"{a}rli and Denny Zhou},
  title = {{Large Language Models Can Be Easily Distracted by Irrelevant Context}},
  year = {2023},
  booktitle = ICML,
  pages = {31210--31227},
  volume = {202},
  series = {Proceedings of Machine Learning Research},
  keywords = {conference},
}
@inproceedings{2023-07-hyena-hierarchy-long-convolutions,
  author = {Michael Poli and Stefano Massaroli and Eric Nguyen and Daniel Y. Fu and Tri Dao and Stephen Baccus and Yoshua Bengio and Stefano Ermon and Christopher Ré},
  title = {{Hyena Hierarchy: Towards Larger Convolutional Language Models}},
  booktitle = ICML,
  year = {2023},
  volume = {202},
  publisher = {PMLR},
  eprint = {2302.10866},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG},
}
@misc{2023-07-llama-2-open-foundation-chat,
  author = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  title = {{Llama 2: Open Foundation and Fine-Tuned Chat Models}},
  year = {2023},
  eprint = {2307.09288},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  keywords = {preprint},
}
@misc{2023-07-retnet-retentive-network,
  author = {Yutao Sun and Li Dong and Shaohan Huang and Shuming Ma and Yuqing Xia and Jilong Xue and Jianyong Wang and Furu Wei},
  title = {{Retentive Network: A Successor to Transformer for Large Language Models}},
  year = {2023},
  eprint = {2307.08621},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  keywords = {preprint},
}
@misc{2023-09-qwen-technical-report,
  author = {Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
  title = {{Qwen Technical Report}},
  year = {2023},
  eprint = {2309.16609},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  keywords = {preprint},
}
@misc{2023-10-mistral-7b,
  author = {Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and L\'{e}lio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timoth\'{e}e Lacroix and William El Sayed},
  title = {{Mistral 7B}},
  year = {2023},
  eprint = {2310.06825},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  keywords = {preprint},
}
@misc{2023-11-needle-in-a-haystack,
  author = {Greg Kamradt},
  title = {{Needle In A Haystack -- Pressure Testing LLMs}},
  year = {2023},
  howpublished = {GitHub repository},
  url = {https://github.com/gkamradt/LLMTest_NeedleInAHaystack},
  keywords = {informal},
}
@inproceedings{2023-12-gqa-grouped-query-attention,
  author = {Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebr{\'o}n and Sumit Sanghai},
  title = {{GQA}: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  booktitle = EMNLP,
  year = {2023},
  url = {https://arxiv.org/abs/2305.13245},
  eprint = {2305.13245},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
}
@inproceedings{2023-12-landmark-attention-infinite-context,
  author = {Amirkeivan Mohtashami and Martin Jaggi},
  title = {{Random-Access Infinite Context Length for Transformers}},
  year = {2023},
  booktitle = NeurIPS,
  keywords = {conference},
}
@inproceedings{2023-12-positional-encoding-length-generalization,
    author = {Amirhossein Kazemnejad and Inkit Padhi and Karthikeyan Natesan Ramamurthy and Payel Das and Siva Reddy},
    title = {{The Impact of Positional Encoding on Length Generalization in Transformers}},
    year = {2023},
    booktitle = NeurIPS,
    url = {https://arxiv.org/abs/2305.19466},
}
@inproceedings{2023-12-quantizable-transformers-attention-do-nothing,
  author = {Yelysei Bondarenko and Markus Nagel and Tijmen Blankevoort},
  title = {{Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing}},
  year = {2023},
  booktitle = NeurIPS,
  keywords = {conference},
}
@inproceedings{2023-12-rwkv-reinventing-rnns-transformer,
  author = {Bo Peng and Eric Alcaide and Quentin Anthony and Alon Albalak and Samuel Arcadinho and Stella Biderman and Huanqi Cao and Xin Cheng and Michael Chung and Xingjian Du and Matteo Grella and Kranthi Kiran GV and Xuzheng He and Haowen Hou and Jiaju Lin and Przemys{\l}aw Kazienko and Jan Koco{\'n} and Jiaming Kong and Bart{\l}omiej Koptyra and Hayden Lau and Krishna Sri Ipsit Mantri and Ferdinand Mom and Atsushi Saito and Guangyu Song and Xiangru Tang and Bolun Wang and Johan S. Wind and Stanis{\l}aw Wo{\'z}niak and Ruichong Zhang and Zhenyuan Zhang and Qihang Zhao and Peng Zhou and Qinghua Zhou and Jian Zhu and Rui-Jie Zhu},
  title = {{RWKV: Reinventing RNNs for the Transformer Era}},
  booktitle = FindingsEMNLP,
  year = {2023},
  eprint = {2305.13048},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
}
@inproceedings{2023-12-zeroscrolls-zero-shot-long-text,
  author = {Uri Shaham and Maor Ivgi and Avia Efrat and Jonathan Berant and Omer Levy},
  title = {{ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding}},
  year = {2023},
  booktitle = FindingsEMNLP,
  pages = {7977--7989},
  keywords = {conference},
}
@article{2024-01-roformer-rope,
  author = {Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
  title = {{RoFormer: Enhanced Transformer with Rotary Position Embedding}},
  year = {2024},
  journal = {Neurocomputing},
  volume = {568},
  pages = {127063},
  keywords = {journal},
}
@article{2024-02-lost-in-the-middle,
  author = {Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang},
  title = {{Lost in the Middle: How Language Models Use Long Contexts}},
  year = {2024},
  journal = TACL,
  volume = {12},
  pages = {157--173},
  keywords = {journal},
}
@article{2024-03-gemini-1.5-long-context,
  title     = {Gemini 1.5: Unlocking Multimodal Understanding Across Millions of Tokens of Context},
  author    = {Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal   = arXiv,
  year      = {2024},
  eprint    = {2403.05530},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
}
@article{2024-03-gemma-open-models,
  title   = {Gemma: Open Models Based on Gemini Research and Technology},
  author  = {{Gemma Team} and {Google DeepMind}},
  journal = arXiv,
  year    = {2024},
  eprint  = {2403.08295},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL}
}
@misc{2024-03-yi-open-foundation-models,
  author = {Alex Young and Bei Chen and Chao Li and Chengen Huang and Ge Zhang and Guanwei Zhang and Guoyin Wang and Heng Li and Jiangcheng Zhu and Jianqun Chen and Jing Chang and Kaidong Yu and Peng Liu and Qiang Liu and Shawn Yue and Senbin Yang and Shiming Yang and Wen Xie and Wenhao Huang and Xiaohui Hu and Xiaoyi Ren and Xinyao Niu and Pengcheng Nie and Yanpeng Li and Yuchi Xu and Yudong Liu and Yue Wang and Yuxuan Cai and Zhenyu Gu and Zhiyuan Liu and Zonghong Dai},
  title = {{Yi: Open Foundation Models by 01.AI}},
  year = {2024},
  eprint = {2403.04652},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  keywords = {preprint},
}
@inproceedings{2024-05-attention-sinks-streaming,
  author = {Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
  title = {{Efficient Streaming Language Models with Attention Sinks}},
  year = {2024},
  booktitle = ICLR,
  keywords = {conference},
}
@article{2024-05-deepseek-v2-moe,
  title={{DeepSeek-V2}: A Strong, Economical, and Efficient Mixture-of-Experts Language Model},
  author={{DeepSeek-AI}},
  journal={arXiv preprint arXiv:2405.04434},
  year={2024},
  url={https://arxiv.org/abs/2405.04434}
}
@inproceedings{2024-05-flashattention-2,
  author = {Tri Dao},
  title = {{FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning}},
  year = {2024},
  booktitle = ICLR,
  keywords = {conference},
}
@inproceedings{2024-05-mamba-selective-state-spaces,
  author = {Albert Gu and Tri Dao},
  title = {{Mamba: Linear-Time Sequence Modeling with Selective State Spaces}},
  booktitle = ICLR,
  year = {2024},
  eprint = {2312.00752},
  archiveprefix = {arXiv},
  primaryclass = {cs.LG},
}
@inproceedings{2024-05-ring-attention-near-infinite-context,
  title     = {Ring Attention with Blockwise Transformers for Near-Infinite Context},
  author    = {Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
  booktitle = ICLR,
  year      = {2024},
  url       = {https://openreview.net/forum?id=WsRHpHH4s0}
}
@inproceedings{2024-05-yarn-context-extension,
  author = {Bowen Peng and Jeffrey Quesnelle and Honglu Fan and Enrico Shippole},
  title = {{YaRN: Efficient Context Window Extension of Large Language Models}},
  year = {2024},
  booktitle = ICLR,
  keywords = {conference},
}
@inproceedings{2024-06-ada-leval-length-adaptable-benchmark,
  author = {Chonghua Wang and Haodong Duan and Songyang Zhang and Dahua Lin and Kai Chen},
  title = {{Ada-LEval: Evaluating Long-Context LLMs with Length-Adaptable Benchmarks}},
  year = {2024},
  booktitle = NAACL,
  pages = {3712--3724},
  keywords = {conference},
}
@inproceedings{2024-06-effective-long-context-scaling,
  title     = {Effective Long-Context Scaling of Foundation Models},
  author    = {Xiong, Wenhan and Liu, Jingyu and Molybog, Igor and Zhang, Hejia and Bhargava, Prajjwal and Hou, Rui and Martin, Louis and Rungta, Rashi and Sankararaman, Karthik Abinav and Oguz, Barlas and Khabsa, Madian and Fang, Han and Mehdad, Yashar and Narang, Sharan and Malik, Kshitiz and Fan, Angela and Bhosale, Shruti and Edunov, Sergey and Lewis, Mike and Wang, Sinong and Ma, Hao},
  booktitle = NAACL,
  pages     = {4643--4663},
  year      = {2024},
  address   = {Mexico City, Mexico},
  url       = {https://aclanthology.org/2024.naacl-long.260/}
}
@misc{2024-07-llama-3-herd-of-models,
  author = {Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and others},
  title = {{The Llama 3 Herd of Models}},
  year = {2024},
  eprint = {2407.21783},
  archiveprefix = {arXiv},
  primaryclass = {cs.AI},
  keywords = {preprint},
}
@inproceedings{2024-07-longrope-context-extension,
  author = {Yiran Ding and Li Lyna Zhang and Chengruidong Zhang and Yuanyuan Xu and Ning Shang and Jiahang Xu and Fan Yang and Mao Yang},
  title = {{LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens}},
  year = {2024},
  booktitle = ICML,
  keywords = {conference},
}
@inproceedings{2024-07-mamba-2-transformers-ssms,
  title     = {Transformers are {SSMs}: Generalized Models and Efficient Algorithms Through Structured State Space Duality},
  author    = {Dao, Tri and Gu, Albert},
  booktitle = ICML,
  year      = {2024},
  url       = {https://arxiv.org/abs/2405.21060},
}
@misc{2024-07-qwen2-technical-report,
  author = {An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jianxin Yang and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Xuejing Liu and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhifang Guo and Zhihao Fan},
  title = {{Qwen2 Technical Report}},
  year = {2024},
  eprint = {2407.10671},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  keywords = {preprint},
}
@inproceedings{2024-08-deepseek-moe,
  title     = {{DeepSeekMoE}: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models},
  author    = {Dai, Damai and Deng, Chengqi and Zhao, Chenggang and Xu, R.X. and Gao, Huazuo and Chen, Deli and Li, Jiashi and Zeng, Wangding and Yu, Xingkai and Wu, Y. and Xie, Zhenda and Li, Y.K. and Huang, Panpan and Luo, Fuli and Ruan, Chong and Sui, Zhifang and Liang, Wenfeng},
  booktitle = ACL,
  year      = {2024},
  pages     = {1280--1297},
  doi       = {10.18653/v1/2024.acl-long.70},
  url       = {https://aclanthology.org/2024.acl-long.70/},
}
@inproceedings{2024-08-flenqa-input-length-reasoning,
  author = {Mosh Levy and Alon Jacoby and Yoav Goldberg},
  title = {{Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models}},
  year = {2024},
  booktitle = ACL,
  eprint = {2402.14848},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  keywords = {conference},
}
@inproceedings{2024-08-found-in-the-middle,
  author = {Cheng-Yu Hsieh and Yung-Sung Chuang and Chun-Liang Li and Zifeng Wang and Long T. Le and Abhishek Kumar and James Glass and Alexander Ratner and Chen-Yu Lee and Ranjay Krishna and Tomas Pfister},
  title = {{Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization}},
  year = {2024},
  booktitle = FindingsACL,
  pages = {14982--14995},
  keywords = {conference},
}
@article{2024-08-gemma-2-technical-report,
  title   = {Gemma 2: Improving Open Language Models at a Practical Size},
  author  = {{Gemma Team}},
  journal = {arXiv preprint arXiv:2408.00118},
  year    = {2024},
  url     = {https://arxiv.org/abs/2408.00118}
}
@inproceedings{2024-08-infinitebench-long-context-evaluation,
  author = {Xinrong Zhang and Yingfa Chen and Shengding Hu and Zihang Xu and Junhao Chen and Moo Khai Hao and Xu Han and Zhen Leng Thai and Shuo Wang and Zhiyuan Liu and Maosong Sun},
  title = {{{$\infty$}Bench: Extending Long Context Evaluation Beyond 100K Tokens}},
  year = {2024},
  booktitle = ACL,
  pages = {15262--15277},
  keywords = {conference},
}
@inproceedings{2024-08-l-eval-standardized-evaluation,
  author = {Chenxin An and Shansan Gong and Ming Zhong and Xingjian Zhao and Mukai Li and Jun Zhang and Lingpeng Kong and Xipeng Qiu},
  title = {{L-Eval: Instituting Standardized Evaluation for Long Context Language Models}},
  year = {2024},
  booktitle = ACL,
  pages = {14388--14411},
  keywords = {conference},
}
@inproceedings{2024-08-longbench-bilingual-benchmark,
  author = {Yushi Bai and Xin Lv and Jiajie Zhang and Hongchang Lyu and Jiankai Tang and Zhidian Huang and Zhengxiao Du and Xiao Liu and Aohan Zeng and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li},
  title = {{LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding}},
  year = {2024},
  booktitle = ACL,
  pages = {3119--3137},
  keywords = {conference},
}
@inproceedings{2024-08-scaling-llm-test-time-compute,
  author = {Charlie Snell and Jaehoon Lee and Kelvin Xu and Aviral Kumar},
  title = {{Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters}},
  year = {2025},
  booktitle = ICLR,
  keywords = {conference},
}
@inproceedings{2024-10-ruler-context-size,
  author = {Cheng-Ping Hsieh and Simeng Sun and Samuel Kriman and Shantanu Acharya and Dima Rekesh and Fei Jia and Yang Zhang and Boris Ginsburg},
  title = {{RULER: What's the Real Context Size of Your Long-Context Language Models?}},
  year = {2024},
  booktitle = COLM,
  keywords = {conference},
}
@inproceedings{2024-10-rwkv-eagle-finch-matrix-states,
  author = {Bo Peng and Daniel Goldstein and Quentin Anthony and Alon Albalak and Eric Alcaide and Stella Biderman and Eugene Cheah and Xingjian Du and Teddy Ferdinan and Haowen Hou and Przemys{\l}aw Kazienko and Kranthi Kiran GV and Jan Koco{\'n} and Bart{\l}omiej Koptyra and Satyapriya Krishna and Ronald McClelland Jr. and Jiaju Lin and Niklas Muennighoff and Fares Obeid and Atsushi Saito and Guangyu Song and Haoqin Tu and Cahya Wirawan and Stanis{\l}aw Wo{\'z}niak and Ruichong Zhang and Bingchen Zhao and Qihang Zhao and Peng Zhou and Jian Zhu and Rui-Jie Zhu},
  title = {{Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence}},
  year = {2024},
  booktitle = COLM,
  keywords = {conference},
}
@inproceedings{2024-11-genuinely-difficult-long-context,
  author = {Omer Goldman and Alon Jacovi and Aviv Slobodkin and Aviya Maimon and Ido Dagan and Reut Tsarfaty},
  title = {{Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP}},
  year = {2024},
  booktitle = EMNLP,
  pages = {16576--16586},
  keywords = {conference},
}
@inproceedings{2024-12-babilong-long-context-reasoning,
  author = {Yuri Kuratov and Aydar Bulatov and Petr Anokhin and Ivan Rodkin and Dmitry Sorokin and Artyom Sorokin and Mikhail Burtsev},
  title = {{BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack}},
  year = {2024},
  booktitle = NeurIPS,
  keywords = {conference},
}
@misc{2024-12-deepseek-v3-technical-report,
  title         = {{DeepSeek-V3} Technical Report},
  author        = {{DeepSeek-AI}},
  year          = {2024},
  eprint        = {2412.19437},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2412.19437}
}
@inproceedings{2024-12-flashattention-3,
  author    = {Jay Shah and Ganesh Bikshandi and Ying Zhang and Vijay Thakkar and Pradeep Ramani and Tri Dao},
  title     = {{FlashAttention-3}: Fast and Accurate Attention with Asynchrony and Low-precision},
  booktitle = NeurIPS,
  year      = {2024},
  url       = {https://arxiv.org/abs/2407.08608},
}
@misc{2024-12-lost-in-the-middle-in-between,
  author = {George Arthur Baker and Ankush Raut and Sagi Shaier and Lawrence E Hunter and Katharina von der Wense},
  title = {{Lost in the Middle, and In-Between: Enhancing Language Models' Ability to Reason Over Long Contexts in Multi-Hop QA}},
  year = {2024},
  eprint = {2412.10079},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  keywords = {preprint},
}
@inproceedings{2024-12-transformers-need-glasses-over-squashing,
  author = {Federico Barbero and Andrea Banino and Steven Kapturowski and Dharshan Kumaran and João G.M. Araújo and Alex Vitvitskyi and Razvan Pascanu and Petar Veličković},
  title = {{Transformers Need Glasses! Information Over-squashing in Language Tasks}},
  year = {2024},
  booktitle = NeurIPS,
  keywords = {conference},
}
@article{2025-01-kimi-k1.5-scaling-rl,
  title     = {Kimi k1.5: Scaling Reinforcement Learning with LLMs},
  author    = {{Kimi Team}},
  journal   = arXiv,
  year      = {2025},
  eprint    = {2501.12599},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI},
  url       = {https://arxiv.org/abs/2501.12599}
}
@misc{2025-03-gemma-3-technical-report,
  title         = {Gemma 3 Technical Report},
  author        = {{Gemma Team}},
  year          = {2025},
  eprint        = {2503.19786},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{2025-03-longiclbench-long-in-context-learning,
  author = {Tianle Li and Ge Zhang and Quy Duc Do and Xiang Yue and Wenhu Chen},
  title = {{Long-context {LLM}s Struggle with Long In-context Learning}},
  year = {2025},
  journal = TMLR,
  issn = {2835-8856},
  keywords = {journal},
}
@misc{2025-03-survey-transformer-context-extension,
  author = {Yijun Liu and Jinzheng Yu and Yang Xu and Zhongyang Li and Qingfu Zhu},
  title = {{A Survey on Transformer Context Extension: Approaches and Evaluation}},
  year = {2025},
  eprint = {2503.13299},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  keywords = {preprint},
}
@inproceedings{2025-04-attention-sink-emerges,
  author = {Xiangming Gu and Tianyu Pang and Chao Du and Qian Liu and Fengzhuo Zhang and Cunxiao Du and Ye Wang and Min Lin},
  title = {{When Attention Sink Emerges in Language Models: An Empirical View}},
  year = {2025},
  booktitle = ICLR,
  keywords = {conference},
}
@inproceedings{2025-04-differential-transformer,
  author = {Tianzhu Ye and Li Dong and Yuqing Xia and Yutao Sun and Yi Zhu and Gao Huang and Furu Wei},
  title = {{Differential Transformer}},
  booktitle = ICLR,
  year = {2025},
  keywords = {conference-paper},
}
@inproceedings{2025-04-effective-context-length-falls-short,
  author = {Chenxin An and Jun Zhang and Ming Zhong and Lei Li and Shansan Gong and Yao Luo and Jingjing Xu and Lingpeng Kong},
  title = {{Why Does the Effective Context Length of LLMs Fall Short?}},
  year = {2025},
  booktitle = ICLR,
  keywords = {conference},
}
@inproceedings{2025-04-gated-delta-networks,
  author = {Songlin Yang and Jan Kautz and Ali Hatamizadeh},
  title = {{Gated Delta Networks: Improving Mamba2 with Delta Rule}},
  booktitle = ICLR,
  year = {2025},
  eprint = {2412.06464},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
}
@inproceedings{2025-04-helmet-long-context-evaluation,
  author = {Howard Yen and Tianyu Gao and Minmin Hou and Ke Ding and Daniel Fleischer and Peter Izsak and Moshe Wasserblat and Danqi Chen},
  title = {{HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly}},
  year = {2025},
  booktitle = ICLR,
  keywords = {conference},
}
@article{2025-04-kimi-vl-technical-report,
  title   = {{Kimi-VL} Technical Report},
  author  = {{Kimi Team}},
  journal = arXiv,
  year    = {2025},
  eprint  = {2504.07491},
  archiveprefix = {arXiv},
  primaryclass = {cs.CV}
}
@inproceedings{2025-04-longgenbench-long-form-generation,
  author = {Yuhao Wu and Ming Shan Hee and Zhiqing Hu and Roy Ka-Wei Lee},
  title = {{LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs}},
  year = {2025},
  booktitle = ICLR,
  keywords = {conference},
}
@inproceedings{2025-04-pine-eliminating-position-bias,
  author = {Ziqi Wang and Hanlin Zhang and Xiner Li and Kuan-Hao Huang and Chi Han and Shuiwang Ji and Sham M. Kakade and Hao Peng and Heng Ji},
  title = {{Eliminating Position Bias of Language Models: A Mechanistic Approach}},
  year = {2025},
  booktitle = ICLR,
  keywords = {conference},
}
@inproceedings{2025-04-retrieval-head-long-context-factuality,
  author = {Wenhao Wu and Yizhong Wang and Guangxuan Xiao and Hao Peng and Yao Fu},
  title = {{Retrieval Head Mechanistically Explains Long-Context Factuality}},
  year = {2025},
  booktitle = ICLR,
  keywords = {conference},
}
@inproceedings{2025-04-round-and-round-rope,
  author = {Federico Barbero and Alex Vitvitskyi and Christos Perivolaropoulos and Razvan Pascanu and Petar Veli\v{c}kovi\'{c}},
  title = {{Round and Round We Go! What Makes Rotary Positional Encodings Useful?}},
  year = {2025},
  booktitle = ICLR,
  keywords = {conference},
}
@misc{2025-05-100-longbench-long-context-benchmarks,
  author = {Wang Yang and Hongye Jin and Shaochen Zhong and Song Jiang and Qifan Wang and Vipin Chaudhary and Xiaotian Han},
  title = {{100-LongBench: Are \textit{de facto} Long-Context Benchmarks Literally Evaluating Long-Context Ability?}},
  year = {2025},
  eprint = {2505.19293},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  keywords = {preprint},
}
@article{2025-05-qwen3-technical-report,
  title   = {{Qwen3 Technical Report}},
  author  = {{Qwen Team}},
  journal = {arXiv preprint arXiv:2505.09388},
  year    = {2025},
  url     = {https://arxiv.org/abs/2505.09388}
}
@misc{2025-07-kimi-k2-open-agentic-intelligence,
  title         = {Kimi K2: Open Agentic Intelligence},
  author        = {{Kimi Team}},
  year          = {2025},
  eprint        = {2507.20534},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2507.20534}
}
@inproceedings{2025-07-longbench-v2,
  author = {Yushi Bai and Shangqing Tu and Jiajie Zhang and Hao Peng and Xiaozhi Wang and Xin Lv and Shulin Cao and Jiazheng Xu and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li},
  title = {{LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks}},
  year = {2025},
  booktitle = ACL,
  pages = {3639--3664},
  keywords = {conference},
}
@inproceedings{2025-07-lv-eval-long-context-benchmark,
  author = {Tao Yuan and Xuefei Ning and Dong Zhou and Zhijie Yang and Shiyao Li and Minghui Zhuang and Zheyue Tan and Zhuyu Yao and Dahua Lin and Boxun Li and Guohao Dai and Shengen Yan and Yu Wang},
  title = {{LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K}},
  year = {2025},
  booktitle = COLM,
  keywords = {conference},
}
@inproceedings{2025-07-nolima-long-context-evaluation,
  author = {Ali Modarressi and Hanieh Deilamsalehy and Franck Dernoncourt and Trung Bui and Ryan A. Rossi and Seunghyun Yoon and Hinrich Sch\"{u}tze},
  title = {{NoLiMa: Long-Context Evaluation Beyond Literal Matching}},
  year = {2025},
  booktitle = ICML,
  keywords = {conference},
}
@inproceedings{2025-07-position-bias-single-dimension-scaling,
  author = {Yijiong Yu and Huiqiang Jiang and Xufang Luo and Qianhui Wu and Chin-Yew Lin and Dongsheng Li and Yuqing Yang and Yongfeng Huang and Lili Qiu},
  title = {{Mitigate Position Bias in LLMs via Scaling a Single Hidden States Channel}},
  year = {2025},
  booktitle = FindingsACL,
  keywords = {conference},
}
@inproceedings{2025-07-position-bias-transformers,
  author = {Xinyi Wu and Yifei Wang and Stefanie Jegelka and Ali Jadbabaie},
  title = {{On the Emergence of Position Bias in Transformers}},
  year = {2025},
  booktitle = ICML,
  keywords = {conference},
}
@article{2025-09-apertus-open-compliant-llms,
  author       = {{Project Apertus} and Alejandro Hernández-Cano and Alexander Hägele and Allen Hao Huang and Angelika Romanou and Antoni-Joan Solergibert and Barna Pasztor and Bettina Messmer and Dhia Garbaya and Eduard Frank Ďurech and Ido Hakimi and Juan García Giraldo and Mete Ismayilzada and Negar Foroutan and Skander Moalla and Tiancheng Chen and Vinko Sabolčec and Yixuan Xu and Antoine Bosselut and Martin Jaggi and Imanol Schlag},
  title        = {Democratizing Open and Compliant {LLMs} for Global Language Environments},
  year         = {2025},
  month        = sep,
  journal      = arXiv,
  url          = {https://arxiv.org/abs/2509.14233},
  eprint       = {2509.14233},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  keywords     = {large language models, multilingual, data compliance, open source, pretraining}
}
@misc{2025-09-locobench-long-context-code,
  author = {Jielin Qiu and Zuxin Liu and Zhiwei Liu and Rithesh Murthy and Jianguo Zhang and Haolin Chen and Shiyu Wang and Ming Zhu and Liangwei Yang and Juntao Tan and Zhepeng Cen and Cheng Qian and Shelby Heinecke and Weiran Yao and Silvio Savarese and Caiming Xiong and Huan Wang},
  title = {{LoCoBench: A Comprehensive Long-Context Code Benchmark for LLMs}},
  year = {2025},
  eprint = {2509.09614},
  archiveprefix = {arXiv},
  primaryclass = {cs.SE},
  keywords = {preprint},
}
@article{2025-10-kimi-linear-attention,
  title={Kimi Linear: An Expressive, Efficient Attention Architecture},
  author={{Kimi Team}},
  journal=arXiv,
  year={2025},
  eprint={2510.26692},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}
@inproceedings{2025-11-context-length-hurts-performance,
  author = {Yufeng Du and Minyang Tian and Srikanth Ronanki and Subendhu Rongali and Sravan Babu Bodapati and Aram Galstyan and Azton Wells and Roy Schwartz and Eliu A. Huerta and Hao Peng},
  title = {{Context Length Alone Hurts LLM Performance Despite Perfect Retrieval}},
  year = {2025},
  booktitle = FindingsEMNLP,
  keywords = {conference},
}
@inproceedings{2025-11-pos2distill-position-bias-distillation,
  author = {Yifei Wang and Feng Xiong and Yong Wang and Linjing Li and Xiangxiang Chu and Daniel Dajun Zeng},
  title = {{Position Bias Mitigates Position Bias: Mitigate Position Bias Through Inter-Position Knowledge Distillation}},
  year = {2025},
  booktitle = EMNLP,
  pages = {1495--1512},
  keywords = {conference},
}
@misc{2025-12-deepseek-v3.2-frontier-open-llm,
  title         = {{DeepSeek-V3.2}: Pushing the Frontier of Open Large Language Models},
  author        = {{DeepSeek-AI}},
  year          = {2025},
  eprint        = {2512.02556},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2512.02556},
  journal       = arXiv
}
@misc{2025-12-drope-dropping-positional-embeddings,
  author = {Yoav Gelberg and Koshi Eguchi and Takuya Akiba and Edoardo Cetin},
  title = {{DroPE: Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings}},
  year = {2025},
  eprint = {2512.12167},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  keywords = {preprint},
}
@article{2025-12-ttt-e2e-long-context,
  title     = {End-to-End Test-Time Training for Long Context},
  author    = {Tandon, Arnuv and Dalal, Karan and Li, Xinhao and Koceja, Daniel and R{\o}d, Marcel and Buchanan, Sam and Wang, Xiaolong and Leskovec, Jure and Koyejo, Sanmi and Hashimoto, Tatsunori and Guestrin, Carlos and McCaleb, Jed and Choi, Yejin and Sun, Yu},
  journal   = arXiv,
  year      = {2025},
  eprint    = {2512.23675},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@misc{2026-01-longbench-pro,
  author = {Ziyang Chen and others},
  title = {{LongBench Pro: A More Realistic and Comprehensive Bilingual Long-Context Evaluation Benchmark}},
  year = {2026},
  eprint = {2601.02872},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  keywords = {preprint},
}
@misc{2026-01-ministral-3-cascade-distillation,
  author = {Liu, Alexander H. and Khandelwal, Kartik and Subramanian, Sandeep and Jouault, Victor and others},
  title = {{Ministral 3}},
  year = {2026},
  eprint = {2601.08584},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  keywords = {preprint},
}
