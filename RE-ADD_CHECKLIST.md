# Reference Re-Add Checklist

Track progress of re-adding all references through the full pipeline:
1. **init** — `source.md`, `cite.bib`, PDF (or sources manifest for non-standard)
2. **read** — `sections/` (standard) or `sources/` (non-standard)
3. **analysis** — `analysis.md`

Legend:
- `S` = standard (has PDF), `N` = non-standard (blog/repo/Reddit)
- `[x]` = done, `[~]` = in progress, `[ ]` = not started, `[>]` = queued for processing

| # | Slug | Type | init | read | analysis | Status |
|---|------|------|------|------|----------|
| 1 | `2014-10-neural-turing-machines` | S | [x] | [x] | [x] | ✓ |
| 2 | `2017-05-moe-sparsely-gated-mixture-experts` | S | [x] | [x] | [x] | ✓ |
| 3 | `2017-12-attention-is-all-you-need` | S | [x] | [x] | [x] | ✓ |
| 4 | `2018-06-prediction-short-memory` | S | [x] | [x] | [x] | ✓ |
| 5 | `2018-07-sharp-nearby-fuzzy-far-away` | S | [x] | [x] | [x] | ✓ |
| 6 | `2019-02-gpt-2-language-models-unsupervised` | S | [x] | [x] | [x] | ✓ |
| 7 | `2019-06-bert-pretraining-language-understanding` | S | [x] | [x] | [x] | ✓ |
| 8 | `2019-07-specialized-attention-heads-pruning` | S | [x] | [x] | [x] | ✓ |
| 9 | `2019-07-transformer-xl` | S | [x] | [x] | [x] | ✓ |
| 10 | `2019-08-bert-attention-analysis` | S | [x] | [x] | [x] | ✓ |
| 11 | `2019-11-dark-secrets-of-bert` | S | [x] | [x] | [x] | ✓ |
| 12 | `2019-12-sixteen-heads-better-than-one` | S | [x] | [x] | [x] | ✓ |
| 13 | `2020-04-compressive-transformer-pg19` | S | [x] | [x] | [x] | ✓ |
| 14 | `2020-04-longformer-long-document-transformer` | S | [x] | [x] | [x] | ✓ |
| 15 | `2020-07-quantifying-attention-flow` | S | [x] | [x] | [x] | ✓ |
| 16 | `2020-07-theoretical-limitations-self-attention` | S | [x] | [x] | [x] | ✓ |
| 17 | `2020-12-bigbird-sparse-attention` | S | [x] | [x] | [x] | ✓ |
| 18 | `2020-12-gpt-3-few-shot-learners` | S | [x] | [x] | [x] | ✓ |
| 19 | `2021-05-gnn-bottleneck-over-squashing` | S | [x] | [x] | [x] | ✓ |
| 20 | `2021-05-long-range-arena` | S | [x] | [x] | [x] | ✓ |
| 21 | `2021-08-context-features-transformer-lm` | S | [x] | [x] | [x] | ✓ |
| 22 | `2021-11-ff-layers-key-value-memories` | S | [x] | [x] | [x] | ✓ |
| 23 | `2021-11-long-range-models-use-context` | S | [x] | [x] | [x] | ✓ |
| 24 | `2021-12-transformer-circuits-framework` | N | [x] | [x] | [x] | ✓ |
| 25 | `2022-03-in-context-learning-induction-heads` | S | [x] | [x] | [x] | ✓ |
| 26 | `2022-04-alibi-train-short-test-long` | S | [x] | [x] | [x] | ✓ |
| 27 | `2022-04-s4-structured-state-spaces` | S | [x] | [x] | [x] | ✓ |
| 28 | `2022-06-switch-transformers-moe` | S | [x] | [x] | [x] | ✓ |
| 29 | `2022-12-chain-of-thought-prompting` | S | [x] | [x] | [x] | ✓ |
| 30 | `2022-12-chinchilla-scaling-laws` | S | [x] | [x] | [x] | ✓ |
| 31 | `2022-12-flashattention` | S | [x] | [x] | [x] | ✓ |
| 32 | `2022-12-nope-transformers-learn-positions` | S | [x] | [x] | [x] | ✓ |
| 33 | `2022-12-scrolls-long-language-sequences` | S | [x] | [x] | [x] | ✓ |
| 34 | `2023-02-llama-open-efficient-foundation` | S | [x] | [x] | [x] | ✓ |
| 35 | `2023-03-gpt-4-technical-report` | S | [x] | [x] | [x] | ✓ |
| 36 | `2023-06-pi-positional-interpolation` | S | [x] | [x] | [x] | ✓ |
| 37 | `2023-06-rope-ntk` | N | [x] | [x] | [x] | ✓ |
| 38 | `2023-07-gsm-ic-irrelevant-context` | S | [x] | [x] | [x] | ✓ |
| 39 | `2023-07-hyena-hierarchy-long-convolutions` | S | [x] | [x] | [x] | ✓ |
| 40 | `2023-07-llama-2-open-foundation-chat` | S | [x] | [x] | [x] | ✓ |
| 41 | `2023-07-retnet-retentive-network` | S | [x] | [x] | [x] | ✓ |
| 42 | `2023-09-qwen-technical-report` | S | [x] | [x] | [x] | ✓ |
| 43 | `2023-10-mistral-7b` | S | [x] | [x] | [x] | ✓ |
| 44 | `2023-11-needle-in-a-haystack` | N | [x] | [x] | [x] | ✓ |
| 45 | `2023-12-gqa-grouped-query-attention` | S | [x] | [x] | [x] | ✓ |
| 46 | `2023-12-landmark-attention-infinite-context` | S | [x] | [x] | [x] | ✓ |
| 47 | `2023-12-positional-encoding-length-generalization` | S | [x] | [x] | [x] | ✓ |
| 48 | `2023-12-quantizable-transformers-attention-do-nothing` | S | [x] | [x] | [x] | ✓ |
| 49 | `2023-12-rwkv-reinventing-rnns-transformer` | S | [x] | [x] | [x] | ✓ |
| 50 | `2023-12-zeroscrolls-zero-shot-long-text` | S | [x] | [x] | [x] | ✓ |
| 51 | `2024-01-roformer-rope` | S | [x] | [x] | [x] | ✓ |
| 52 | `2024-02-lost-in-the-middle` | S | [x] | [x] | [x] | ✓ |
| 53 | `2024-03-gemini-1.5-long-context` | S | [x] | [x] | [x] | ✓ |
| 54 | `2024-03-gemma-open-models` | S | [x] | [x] | [x] | ✓ |
| 55 | `2024-03-yi-open-foundation-models` | S | [x] | [x] | [x] | ✓ |
| 56 | `2024-05-attention-sinks-streaming` | S | [x] | [x] | [x] | ✓ |
| 57 | `2024-05-deepseek-v2-moe` | S | [x] | [x] | [x] | ✓ |
| 58 | `2024-05-flashattention-2` | S | [x] | [x] | [x] | ✓ |
| 59 | `2024-05-mamba-selective-state-spaces` | S | [x] | [x] | [x] | ✓ |
| 60 | `2024-05-ring-attention-near-infinite-context` | S | [x] | [x] | [x] | ✓ |
| 61 | `2024-05-yarn-context-extension` | S | [x] | [x] | [x] | ✓ |
| 62 | `2024-06-ada-leval-length-adaptable-benchmark` | S | [x] | [x] | [x] | ✓ |
| 63 | `2024-06-effective-long-context-scaling` | S | [x] | [x] | [x] | ✓ |
| 64 | `2024-07-llama-3-herd-of-models` | S | [x] | [x] | [x] | ✓ |
| 65 | `2024-07-longrope-context-extension` | S | [x] | [x] | [x] | ✓ |
| 66 | `2024-07-mamba-2-transformers-ssms` | S | [x] | [x] | [x] | ✓ |
| 67 | `2024-07-qwen2-technical-report` | S | [x] | [x] | [x] | ✓ |
| 68 | `2024-08-deepseek-moe` | S | [x] | [x] | [x] | ✓ |
| 69 | `2024-08-flenqa-input-length-reasoning` | S | [x] | [x] | [x] | ✓ |
| 70 | `2024-08-found-in-the-middle` | S | [x] | [x] | [x] | ✓ |
| 71 | `2024-08-gemma-2-technical-report` | S | [x] | [x] | [x] | ✓ |
| 72 | `2024-08-infinitebench-long-context-evaluation` | S | [x] | [x] | [x] | ✓ |
| 73 | `2024-08-l-eval-standardized-evaluation` | S | [x] | [x] | [x] | ✓ |
| 74 | `2024-08-longbench-bilingual-benchmark` | S | [x] | [x] | [x] | ✓ |
| 75 | `2024-08-scaling-llm-test-time-compute` | S | [x] | [x] | [x] | ✓ |
| 76 | `2024-10-ruler-context-size` | S | [x] | [x] | [x] | ✓ |
| 77 | `2024-10-rwkv-eagle-finch-matrix-states` | S | [x] | [x] | [x] | ✓ |
| 78 | `2024-11-genuinely-difficult-long-context` | S | [x] | [x] | [x] | ✓ |
| 79 | `2024-12-babilong-long-context-reasoning` | S | [x] | [x] | [x] | ✓ |
| 80 | `2024-12-deepseek-v3-technical-report` | S | [x] | [x] | [x] | ✓ |
| 81 | `2024-12-flashattention-3` | S | [x] | [x] | [x] | ✓ |
| 82 | `2024-12-lost-in-the-middle-in-between` | S | [x] | [x] | [x] | ✓ |
| 83 | `2024-12-transformers-need-glasses-over-squashing` | S | [x] | [x] | [x] | ✓ |
| 84 | `2025-01-kimi-k1.5-scaling-rl` | S | [x] | [x] | [x] | ✓ |
| 85 | `2025-03-gemma-3-technical-report` | S | [x] | [x] | [x] | ✓ |
| 86 | `2025-03-longiclbench-long-in-context-learning` | S | [x] | [x] | [x] | ✓ |
| 87 | `2025-03-survey-transformer-context-extension` | S | [x] | [x] | [x] | ✓ |
| 88 | `2025-04-attention-sink-emerges` | S | [x] | [x] | [x] | ✓ |
| 89 | `2025-04-differential-transformer` | S | [x] | [x] | [x] | ✓ |
| 90 | `2025-04-effective-context-length-falls-short` | S | [x] | [x] | [x] | ✓ |
| 91 | `2025-04-gated-delta-networks` | S | [x] | [x] | [x] | ✓ |
| 92 | `2025-04-helmet-long-context-evaluation` | S | [x] | [x] | [x] | ✓ |
| 93 | `2025-04-kimi-vl-technical-report` | S | [x] | [x] | [x] | ✓ |
| 94 | `2025-04-longgenbench-long-form-generation` | S | [x] | [x] | [x] | ✓ |
| 95 | `2025-04-pine-eliminating-position-bias` | S | [x] | [x] | [x] | ✓ |
| 96 | `2025-04-retrieval-head-long-context-factuality` | S | [x] | [x] | [x] | ✓ |
| 97 | `2025-04-round-and-round-rope` | S | [ ] | [ ] | [ ] | queued |
| 98 | `2025-05-100-longbench-long-context-benchmarks` | S | [ ] | [ ] | [ ] | queued |
| 99 | `2025-05-qwen3-technical-report` | S | [ ] | [ ] | [ ] | queued |
| 100 | `2025-07-kimi-k2-open-agentic-intelligence` | S | [ ] | [ ] | [ ] | queued |
| 101 | `2025-07-longbench-v2` | S | [ ] | [ ] | [ ] | queued |
| 102 | `2025-07-lv-eval-long-context-benchmark` | S | [ ] | [ ] | [ ] | queued |
| 103 | `2025-07-nolima-long-context-evaluation` | S | [ ] | [ ] | [ ] | queued |
| 104 | `2025-07-position-bias-single-dimension-scaling` | S | [ ] | [ ] | [ ] | queued |
| 105 | `2025-07-position-bias-transformers` | S | [ ] | [ ] | [ ] | queued |
| 106 | `2025-09-locobench-long-context-code` | S | [ ] | [ ] | [ ] | queued |
| 107 | `2025-10-kimi-linear-attention` | S | [ ] | [ ] | [ ] | queued |
| 108 | `2025-11-context-length-hurts-performance` | S | [ ] | [ ] | [ ] | queued |
| 109 | `2025-11-pos2distill-position-bias-distillation` | S | [ ] | [ ] | [ ] | queued |
| 110 | `2025-12-deepseek-v3.2-frontier-open-llm` | S | [ ] | [ ] | [ ] | queued |
| 111 | `2025-12-drope-dropping-positional-embeddings` | S | [ ] | [ ] | [ ] | queued |
| 112 | `2025-12-ttt-e2e-long-context` | S | [ ] | [ ] | [ ] | queued |
| 113 | `2026-01-longbench-pro` | S | [ ] | [ ] | [ ] | queued |
| 114 | `2026-01-ministral-3-cascade-distillation` | S | [ ] | [ ] | [ ] | queued |

**Progress: 98 / 114 complete**

**Processing (2026-02-08):**
- Queue: #37, #75-114 (41 papers), full pipeline from init
- **Note:** #37 `rope-ntk` is non-standard (N) — uses read_sources path
